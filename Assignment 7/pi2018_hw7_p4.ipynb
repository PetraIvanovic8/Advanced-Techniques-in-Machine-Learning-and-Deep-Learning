{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMAdi9qgC-B9"
      },
      "source": [
        "To copy this template: File -> Save a Copy in Drive\n",
        "\n",
        "***DISCLAIMER**: In case of any discrepancy in the assignment instruction, please refer to the `PDF` document.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 4 - Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbEBvSo3ClZT"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "- **Episodic Tasks**: Episodic tasks are usually used when we have a defined - one time goal. Due to this, the steps in episodic tasks are separated in states (initial and terminal), actions (subtasks/actions that are done) and rewards. Looking at the real world example we can think of computer playing games such as chess or tic-tac-toe. For instance in the game of tic-tac-toe we have initial state where the screen is clear - empty board, than we have moves we complete (choosing where to put x or o) which are called actions, and then in the end we have terminal state which is result of our game - win/lose/draw. Each episodic task as a whole is separate from all others and it learns from its own experiences working towards the given goal.\n",
        "\n",
        "- **Continuous Tasks**: Main difference between Episodic Tasks and Continous Tasks is that Continuous Tasks do not have 'end game'. In other words, the task can have different states and actions and potentially rewards but it does not have any end at which it will stop working. For instance we can think of a self driving car that makes real-time decisions (actions) based on the outside inputs - road state and signs (states) and adapts by doing different actions - drive, stop, slow down, speed up, etc. The goal of such task is to keep the car safely on the road but there is no ending to the task so it has to be a continous task.\n",
        "\n",
        "Looking at both of these tasks main difference is that episodic tasks are defined by discrete episodes with beginnings and endings which are good for tasks with distinct goals or end states while continuous tasks involve ongoing interactions without decided endpoints, so they are best for ongoing processes that require constant adjustment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 4.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SDulg8cCnRA"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "In Reinforcement Learning (RL), exploration and exploitation are both used to help a model learn about it's enviroment.\n",
        "\n",
        "- **Exploration:** Exploration includes trying new things to get more information about the environment around it - it explores different approaches and places. It is very important because it allows the model to learn new strategies or actions that might result in higher rewards in the future. Without exploration (or with too little of it) the model might miss out on these opportunities that could have led to best result.\n",
        "\n",
        "- **Exploitation:** Exploitation is using the current knoledge the model has to make decisions in a way to maximize reward. It can be more usefull if exploration has alredy been done or if the best startegy was already learned (but we can never relly know if this is true). Exploitation leverages what model aready knows to get the best outcome based on input but does not involve discovering any new strategies (Exploration).\n",
        "\n",
        "It is very important to have best possible combination of Exploration and Exploitation in order to get best results.\n",
        "\n",
        "- **ε-greedy policy:** ε-greedy policy is a method to balance exploration and exploitation where ε is a probability value (between 0 and 1). At each step model takes with probability ε, model performs exploration by selecting a random actions (with a goal of finding the best one). This encourages the model to try out new actions that it hasn't yet evaluated entirely. Then the model performs exploitation, with probability 1-ε, by choosing the best-known action (the one that maximizes rewards). This utilizes the model's existing/learned knowledge to maximize possible rewards.\n",
        "\n",
        "- **fixed or scheduled ε:** Generally, we would not want a fixed ε as in the begining we want to do more exploration and less exploitation to learn more about the environment (high ε) and once we get diverse information about the environment we would want more exploitation and less exploration - lower ε. Due to this the best approach would be to have ε schedule so that as model learns more about the environment, ε is gradually decreased, shifting the balance from more exploration towards more exploitation.This scheduled approach makes sure that the model does not remain stuck in the exploration phase and starts leveraging its lerned knowledge to make more informed decisions.\n",
        "\n",
        "- **ε & balance of eploration and exploitation:** ε is crucial to balance our trade-off between exploration (trying new actions to find potentially better strategies) and exploitation (using known/learned strategies to maximize rewards). Thus, we can say that ε is crucial to balance exploration and exploitation during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eXCL_NyGy2_"
      },
      "source": [
        "## 4.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjo-oTDxCole"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "- The main diffeerence between Deep Q-Learning and Q-Learning is that Deep Q-Learning introduces deep neural networks to approximate the Q-value function while in general (non deep) Q-Learning we generally deal with problems that do not require deep neural networks. Due to this Deep Q-Learning might be more appropriate when dealing with high dimensional inputs like images or videos while general Q-Learning is more comonly used with tables or other low dimensional problems.\n",
        "\n",
        "\n",
        "\n",
        "**Steps in Deep Q-Learning**\n",
        "- Step 1: We initialize replay memory and the action-value function. In other words, we set up a memory buffer to store tuples - state, action, reward, and next state and then initialize the neural network for Q-value approximation.\n",
        "- Then for each episode we initialize the environment's state.\n",
        "- Then for each time step we first select an action (using ε-greedy policy) - either random action - exploration or best already known action - exploitation based on the current Q-value estimates. After that we execute the action and observe the reward and the next state and store the touple in the replay memory.\n",
        "\n",
        "- Finaly, we preiodically sample a minibatch of experiences from the replay memory and perform a gradient descent step on the loss function to update the network parameters.\n",
        "\n",
        "\n",
        "This process allows our network to learn in optimal way over time while handling exploitation - exploration tradeoff while handiling complex input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BuGX6EVG4uI"
      },
      "source": [
        "## 4.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyQOE5AhHBxX"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Target Q-network has been introduces in Deep Q-Learning methods because of the issues with instability and divergence. Generally, when updating Q-values there can be significants shifts in policy in each step which leads to instability, but when we use target Q-network it provides a stable target for updates. In other words, it stabilized our network as it is a separate network with the same architecture as the main Q-network but with its weights frozen for a given number of training steps. Additionally, target Q-network with fixed Q-values also provides consistent objective to the main network which helps mitigate the issue of feedback loops. Finally, by periodically updating target network's weights - slower than primary network - the learning process is more gradual which allows the model to lean from more stable and reliable set of Q-values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bKbdum_NYpc"
      },
      "source": [
        "## 4.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N943kCNcNYpc"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Experience replay is a crucial technique in Q-Learning (and especially if it is Deep Q-Learning) because it allows the learning algorithm to reuse past experiences with state, action, reward and next state touples multiple times for training. This means that we are utilizing the learned expeiences more efficiently as they can be used multiple times. Further, as we know, sequential expeiences can get very highly overated which can then in turn lead to instability of the entire training / learning. Experience replay breaks these correlations because it randomly samples from a group of past experiences which means it provides a more varied and representative sample for training at each step. Finally, by using a more diverse set of learned experiences for training, experience replay helps us by reducing the variance in the updates made to the Q-network. This then helps us avoid overfittitng to recent/common experiences which leads to a more generalizable model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1LATb1uNY0r"
      },
      "source": [
        "## 4.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuvDJvDwNY0r"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The prioritized experience replay is a method of prioriting some experiences for learning rather than uniformly sampling from all. This is done by assigning priorities based on how much certain experince can inform the model better to make more precise decisions. This is commonly measued using the size of the tempral difference - TD - error which tells us how \"unexpected\" the epxeience was. In other words, it tells us how much new things model can learn from the certain experience. Experiences with higher TD errors are considered more informative since they represent areas/topics in which model's predictions were not as accurate as we would want them. This is useful as it allows us to focus on high-priority expeiences which makes the learning more efficient. Using this method model learns faster and more as it first learns crucial experiences that have the most potential to improve its understanding and performance/prediction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyFsFl-YNZFO"
      },
      "source": [
        "## 4.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y4ioC-oNZFO"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The GORILA (General Reinforcement Learning Architecture) and Ape-X architectures are 2 approaches to conduct distributed deep reinforcement learning.\n",
        "\n",
        "- **Similarities:**\n",
        "  - Both architectures use distributed computing so that they can harness multiple machines/processors to handle complex reinforcement learning tasks.\n",
        "  - Both architectures distinctly separate the roles of actors which interact with the environment to generate experiences and learners which update the policy based on these experiences they have gained.\n",
        "  - Both architectures use experience replay memory, so that they can store and reuse past experiences for learning.\n",
        "\n",
        "\n",
        "- **Differences:**\n",
        "  - GORILA involves a more detailed setup with multiple parallel components such as actors, learners, parameter servers which all work at the same time while Ape-X has a simpler and more streamlined approach.\n",
        "  - Ape-X uses prioritized experience replay to focus on learning from the most important experiences with a goal of making its learning more efficient.  \n",
        "  - Ape-X uses different exploration policies across its actors which can be beneficial in more complicated environments where a variety of different experiences can help in solving challenging and complicated exploration problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
