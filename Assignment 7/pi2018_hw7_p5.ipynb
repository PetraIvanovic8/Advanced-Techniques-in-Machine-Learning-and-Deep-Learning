{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 5 - ML Cloud Platforms (BONUS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 5.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbEBvSo3ClZT"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "\n",
        "\n",
        "*   **IBM:**\n",
        "  - IBM Cloud Pak for Data supports many different frameworks. These include PyTorch 1.10.2, TensorFlow\t2.7.1, scikit-learn\t1.0.2, XGBoost\t1.5.2, ONNX\t1.10.2, Python\t3.9.7, OpenCV\t4.5.5 and more.\n",
        "  - Source: https://www.ibm.com/docs/en/wmla/2.3?topic=included-deep-learning-frameworks\n",
        "\n",
        "*   **Google:**\n",
        "  - Google Vertex AI supports many differnt frameworks just like IBM Cloud Pak for Data including TensorFlow 2.x (stable and nightly builds), PyTorch 1.x (stable and nightly builds), XGBoost 1.6, Scikit-learn 1.1, Apache Beam 3.x, Kubeflow Pipelines, TensorFlow Extended (TFX) 1.x\n",
        "  - Source: https://cloud.google.com/vertex-ai/docs/tutorials/jupyter-notebooks\n",
        "\n",
        "*   **Microsoft:**\n",
        "  - Azure also supports many different frameworks including TensorFlow 2.x (stable and nightly builds), PyTorch 1.x (stable and nightly builds), XGBoost 1.6, Scikit-learn 1.1, ONNX Runtime, CNTK, and Python.\n",
        "  - Source: https://learn.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/dsvm-tools-deep-learning-frameworks?view=azureml-api-2\n",
        "*   **Amazon:**\n",
        "  - Amazon SageMaker, like others, supports many different frameworks including TensorFlow 2.x (stable and nightly builds, PyTorch 1.x (stable and nightly builds), XGBoost 1.x, Scikit-learn 1.x, Apache MXNet 1.x, Chainer 7.x, CNTK, R, and Python.\n",
        "  - Source: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-common-data-formats.html\n",
        "\n",
        "\n",
        "\n",
        "While all four major ML cloud platforms offer many different Deep Learning frameworks, each has slight variations in versions and additional tools. At the end of the day which one we choose is going to depend on a specific project (all can handle simple DL projecta) requirements and bugets as costs of these platforms can vary significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 5.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SDulg8cCnRA"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "*   **IBM:**\n",
        "  - IBM offers many different GPU options such as bare metal servers with several different GPU options including NVIDIA Tesla P100 and V100 GPUs.\n",
        "  - Sources: https://www.ibm.com/cloud/gpu, https://www.principledtechnologies.com/IBM/Cloud-vs-Amazon-AWS-research-0419-v2.pdf\n",
        "\n",
        "*   **Google:**\n",
        "  - Google Vertex AI supports Google Cloud TPUs (v4, v4-32, v4 Pod), Nvidia GPUs (A100, P100, T4), and CPUs (high-performance and standard).\n",
        "  - Source: https://cloud.google.com/vertex-ai/pricing\n",
        "\n",
        "*   **Microsoft:**\n",
        "  - Azure Machine Learning uses Azure VMs with various configurations, including High-performance CPUs (Standard, Low Priority, Burstable), GPUs (Nvidia A100, P40, V100), and Specialized instances for FPGAs and HPC.\n",
        "  - Source: https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target?view=azureml-api-2\n",
        "*   **Amazon:**\n",
        "  - SageMaker offers a wide range of compute instances, including: CPUs (high-performance, standard, and burstable), GPUs (Nvidia A100, P40, T4, V100, and others, ML accelerators (Amazon ML instances with custom hardware), and Local compute (run on your own laptop or workstation).\n",
        "  - Source: https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html\n",
        "\n",
        "\n",
        "We can see that similarly to frameworks, all 4 have very broad offering of compute units. We can see that IBM stands ut with NVIDIA Tesla P100 GPU while other 3 extend their offerings to include a broader range of Nvidia GPUs, alongside CPUs and specialized instances to serve their customers for all potential training needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eXCL_NyGy2_"
      },
      "source": [
        "## 5.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjo-oTDxCole"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "*   **IBM:**\n",
        "  - Watson Studio is primarly used for building and training ML models, Watson Machine Learning that focuses on the deployment and scaling of ML models, and Watson OpenScale that can be used for monitoring and managing deployed ML models.\n",
        "  - Sources: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=analyzing-data-building-models, https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=deploying-managing-models-functions\n",
        "\n",
        "*   **Google:**\n",
        "  - Google Vertex AI has multiple services for Model lifecycle management. Vertex AI Pipelines can be used to streamline and automate the entire ML pipeline from data ingest to model deployment. For Model Exporting & Serving, models can be deployed to diverse environments (cloud, on-prem, edge) with flexible options like Cloud TPU, Kubernetes, or Cloud Run. For addressing Versioning & Rollback there are differnt versions of a saved model and it is easy to roll back to previous versions if needed. Finally, it is fairly simple to monitor model performance and understand why your model makes certain predictions (for those that have interpretability options).\n",
        "  - Source: https://cloud.google.com/vertex-ai/docs/pipelines\n",
        "*   **Microsoft:**\n",
        "  - Azure Machine Learning Workbench is a web-based environment for model development, training, and deployment with built-in tools for versioning, collaboration, and automation. Azure ML AML pipelines can be used to define and manage ML pipelines through code or visual interface, including data prep, training, deployment, and monitoring. Finally, Azure DevOps integration cna be used to manage ML code and deployments along with software development lifecycle.\n",
        "  - Source: https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2 https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2\n",
        "*   **Amazon:**\n",
        "  - SageMaker Studio is a web-based IDE for building, training, and deploying models with versioning, collaboration, and automation tools. Further, SageMaker Pipelines can be used to define and automate ML workflows with visual or code-based pipelines for data preparation, training, deployment, and monitoring. After that, we can use SageMaker Model Registry for a centralized repository for managing model versions, approvals, and rollbacks. Finally we can use SageMaker to deploy models to various environments (cloud, on-premises, edge) with flexible options like container hosting, serverless inference, and batch prediction.\n",
        "\n",
        "  - Source: https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html https://aws.amazon.com/sagemaker/pipelines/\n",
        "\n",
        "\n",
        "Looking at the model lifecycle management IBM, Google, Microsoft, and Amazon each provide a comprehensive suite of tools that can be tailored to various stages of model development, deployment, and management. While IBM offers a combination of Watson Studio, Watson Machine Learning, and Watson OpenScale, Google, Microsoft, and Amazon present similar functionalities through Vertex AI, Azure Machine Learning, and SageMaker respectively, each integrating features like automated pipelines, version control, and deployment options across their own diverse environments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BuGX6EVG4uI"
      },
      "source": [
        "## 5.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyQOE5AhHBxX"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "*   **IBM:**\n",
        "  - Some of the resources that can be used for monitoring are Watson Studio, Watson Machine Learning, and Watson OpenScale. Watson Studio can be used as it provides ogs related to the execution of various tasks within the studio, like model training runs, and notebook executions. It can also allows users to track  usage of computational resources during model training and experimentation including monitoring the utilization of CPU, GPU, and memory resources.\n",
        "  - Source: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=analyzing-data-building-models#ws\n",
        "\n",
        "*   **Google:**\n",
        "  - Google Vertex AI Monitoring provides comprehensive monitoring of training jobs and deployed models. Further, it is possible to access application logs and resource usage (GPU, CPU, memory) through Cloud Monitoring and Cloud Logging. Google Vertex AI also provied functionally for user to define custom metrics and track training progress in real-time through TensorBoard visualization.\n",
        "  - Source: https://cloud.google.com/vertex-ai/docs/model-monitoring/overview\n",
        "*   **Microsoft:**\n",
        "  - Microsoft has a specific platform for vizualization, Azure Machine Learning Insights, which provides metrics and visualizations for model performance, resource utilization, and training job health. Further, Application Insights can be integrated with code to monitor application logs and performance in real-time. Finally, Azure Monitor can be used as a comprehensive monitoring platform for all Azure resources, including ML components.\n",
        "\n",
        "  - Source: https://learn.microsoft.com/en-us/training/modules/manage-compare-models-azure-machine-learning/ https://learn.microsoft.com/en-us/azure/azure-monitor/ https://learn.microsoft.com/en-us/azure/azure-monitor/\n",
        "*   **Amazon:**\n",
        "  - SageMaker Monitoring provides real-time and historical metrics for model performance, resource utilization, and training job health. CloudWatch Logs can be used to collect and analyze application logs for debugging and troubleshooting. Further, CloudWatch Metrics can be used to track resource usage and performance metrics for all SageMaker resources. Just like in previous two, there is ability to create custom dashboards within CloudWatch to visualize desired metrics and insights.\n",
        "  - Source: https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html\n",
        "\n",
        "\n",
        "Once again, all 4 platforms show a strong focus on providing detailed insights into application logs, as well as CPU, GPU, and memory usage. While IBM uses Watson Studio and Watson Machine Learning for task execution logs and resource utilization monitoring, Google, Microsoft, and Amazon offer similar functionalities through Vertex AI Monitoring, Azure Machine Learning Insights, and SageMaker Monitoring, all integrating advanced tools for real-time tracking and custom metrics visualization, ensuring users have comprehensive visibility into their ML operations for model interprtability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bKbdum_NYpc"
      },
      "source": [
        "## 5.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N943kCNcNYpc"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "*   **IBM:**\n",
        "  - Watson Studio organizes and manages training runs but its visualization tools are not very detailed. It includes plots for metrics like accuracy and loss but does not have a good during training dashboard. However, Watson Machine Learning might also have some visualization options for monitoring performance during training but they are not specified in their public documentation.\n",
        "  - Source: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml_dlaas_working_with_new_models.html?context=cpdaas\n",
        "\n",
        "*   **Google:**\n",
        "  - Vertex AI does not offer a specified training visualization platform but can use TensorBoard integration that allow for visualizations of accuracy, loss, and other performance metrics during training. Additionally, Cloud Monitoring provides additional charts and graphs for resource utilization and overall training job health.\n",
        "  - Source: https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction\n",
        "*   **Microsoft:**\n",
        "  - Azure Machine Learning has real-time visualizations of training metrics within the Workbench interface. Like google it also allows for usage of TensorBoard within the Workbench environment to visualize detailed training metrics and graphs. Additionally, it has ability to create custom dashboards with Azure Monitor to track specific metrics and KPIs for models.\n",
        "  - Source: https://learn.microsoft.com/en-us/azure/machine-learning/?view=azureml-api-2 https://learn.microsoft.com/en-us/azure/machine-learning/how-to-monitor-tensorboard?view=azureml-api-1 https://learn.microsoft.com/en-us/azure/azure-monitor/\n",
        "*   **Amazon:**\n",
        "  - SageMaker Studio can be used to visualize training metrics in real-time within the Studio interface. Further, we can also (as in others) use TensorBoard within Studio for detailed visualizations of training progress and loss functions. Additionally, it also has the ability to create custom dashboards with CloudWatch to track specific metrics and KPIs during training.\n",
        "  - Source: https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html https://docs.aws.amazon.com/sagemaker/latest/dg/tensorboard-on-sagemaker.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Looking at the visualisation options, IBM's Watson Studio provides basic visualization tools for accuracy and loss but it lacks detailed training dashboards, Google and Microsoft leverage TensorBoard (external system) integration for in-depth visualizations, and Amazon's SageMaker Studio offers real-time metric visualization within its interface, with all three also providing options for custom dashboard creation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1LATb1uNY0r"
      },
      "source": [
        "## 5.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuvDJvDwNY0r"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "*   **IBM:** Uses YAML-formated file (manifest) - has following fields: model_definition.name, model_definition.framework,\n",
        "model_definition.description,\n",
        "training_data_reference, and\n",
        "compute_configuration.\n",
        "*   **Google:** Uses YAML or JSON format - has following fields: jobId and trainingInput (with runtimeVersion, scaleTier, pythonModule, args).\n",
        "*   **Microsoft:** It is best suited for Azure ML SDK format in Python but it can also use YAML or JSON formats. The file has following fields: Experiment Name, Environment, Compute Target, and Script Parameters.\n",
        "*   **Amazon:** Uses JSON or Boto3 (Python SDK) with fields TrainingJobName, AlgorithmSpecification (with TrainingImage and TrainingInputMode), ResourceConfig, and HyperParameters.\n",
        "\n",
        "\n",
        "Despite different file formats (YAML, JSON Python code), all platforms share core fields like job name, framework, training data, and resource configuration. As mentioned before, at the end of the day we will chose the one whos format we are most familiar with, which is best fit to our project and which fits our buget.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "Lets use a image classification example to compare the training job descriptions across platforms:\n",
        "\n",
        "Lets say we want to train CNN for image classification using a pre-trained ResNet 18 model on the CIFAR dataset.\n",
        "\n",
        "While file formats and specific terminology differ, all platforms share similar core fields defining the training job:\n",
        "\n",
        "  - Job Name: A unique identifier for the training run. This is model_definition.name on IBM, jobId on Google, Experiment Name on Microsoft, TrainingJobName on Amazon. For our example we would set this field to something like\n",
        "  \"cifar_resnet_training\".\n",
        "  - Framework: Specifies the deep learning framework. In our case we can choose to use either TensorFlow or PyTorch.\n",
        "  - Training Script: Identifies the code file implementing the training logic such as  \"train.py\".\n",
        "  - Training Data: Defines the location and format of the training dataset. We can store this data on coresponding platforms such as S3 bucket for Amazon and Azure blob storage for Microsoft.\n",
        "  - Model Configuration: Specifies the pre-trained model and any hyperparameters for fine-tuning for example learning rate and number of epochs.\n",
        "  - Compute Resources: Defines the hardware configuration for training such as GPU type.\n",
        "\n",
        "\n",
        "Beyond these fields that are necessary for all training, most of the platforms have some additional parameters we can assign such as model_definition for IBM for finer control over model structure and hyperparameters. For Google this would be runtimeVersion for specifying framework versions and scaleTier for auto-scaling resources. For Microsoft this could be environment for managing dependencies and Script Parameters for passing arguments to the training script. For Amazon this could be TrainingInputMode for choosing between file or streaming data sources and HyperParameters for various training settings.\n",
        "\n",
        "This will all depend on the problem we are working on and how much we want to fine tune it.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
