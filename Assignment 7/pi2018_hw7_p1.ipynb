{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 1 - Staleness in Asynchronous SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1w0-Qn-CAZl"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**What we know:**\n",
        "- Learner 1 and 2 send gradients to the PS and update weights\n",
        "- Learner 1 is about 2.5x faster than 2\n",
        "- Updates to weights are instantaneous once received from learners\n",
        "- Staleness is defined as the number of updates that have occurred since a learner last read the weights\n",
        "\n",
        "\n",
        "**Gradient computations:**\n",
        "- g[L1, 1] at second 1\n",
        "- g[L1, 2] at second 2\n",
        "- g[L2, 1] at second 2.5\n",
        "- g[L1, 3] at second 3\n",
        "- g[L1, 4] at second 4\n",
        "- g[L2, 2] at second 5\n",
        "\n",
        "\n",
        "\n",
        "We can say that staleness refers to how many updates have been made to the weights after a learner has last read them but before they apply their new gradient.\n",
        "\n",
        "As we are working with 2 learners and weights are updated instantly after receiving a gradient, we can think about staleness as the number of gradients sent by other learners during the time between two consecutive gradient computations of a learner.\n",
        "\n",
        "\n",
        "**Let's break this down for our 6 cases:**\n",
        "- **g[L1, 1]**: For Learner 1's first gradient there is no staleness (it is 0) as it is the first action in the system (for learner 1 and in general since learner 2 has not yet done anything).\n",
        "- **g[L1, 2]**: For Learner 1's second gradient there is also no staleness (it is 0) as Learner 2 has not yet computed any gradient.\n",
        "- **g[L2, 1]**: For Learner 2's first gradient there is no staleness (it is 0) as it is the first action it has taken - it is Learner 2's first gradient.\n",
        "- **g[L1, 3]**: For Learner 1's third gradient there is staleness of 1 as Learner 2 has computed 1 gradient (g[L2, 1]) since the last Learner 1 update (g[L1, 2]). As this (Learner 2's gradient computation) has happened after Learner 1's second gradient computation but before third, we can say that the staleness for Learner 1's third gradient is 1.\n",
        "- **g[L1, 4]**: For Learner 1's fourth gradient there is no staleness (it is 0) as Learner 2 has not computed any gradients after g[L2,1] and before g[L1,4]. In other words, since the last Learner 1's update - third gradient - Learner 2 has not made any computations by Learner 1's forth gradient.\n",
        "- **g[L2, 2]**: For Learner 2's second gradient there is a staleness of 2 as Learner 1 has computed 2 gradients (g[L1,2] and g[L1,3]) after Learner 2 computed g[L2,1] and before g[L2,2] (current one).\n",
        "\n",
        "\n",
        "\n",
        "**Stalness results:**\n",
        "- g[L1, 1] = 0\n",
        "- g[L1, 2] = 0\n",
        "- g[L2, 1] = 0\n",
        "- g[L1, 3] = 1\n",
        "- g[L1, 4] = 0\n",
        "- g[L2, 2] = 2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
