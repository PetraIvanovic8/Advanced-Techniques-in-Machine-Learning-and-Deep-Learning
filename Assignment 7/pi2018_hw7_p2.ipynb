{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 2 - Data Parallelism in Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXw3oFggfpUn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "import pickle\n",
        "from torchvision.models import resnet18\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.nn.parallel import DataParallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPfhSubI9X88",
        "outputId": "f4154b2c-6573-4f02-dadb-69885e3e6709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of GPUs = 4\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Number of GPUs =\", torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiNJZmWufpUq",
        "outputId": "2e69e38d-c446-4ee5-a1ba-464ddcead780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# CIFAR10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48EXDXBtfpUr"
      },
      "outputs": [],
      "source": [
        "def train(epoch, trainloader, net, criterion, optimizer):\n",
        "    net.train()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iRdxi8ZfpUs",
        "outputId": "73a4a99d-189c-471b-990d-7017501cd1d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Batch Size: 32, Training Time: 17.92 seconds\n",
            "Batch Size: 128, Training Time: 8.26 seconds\n",
            "Batch Size: 512, Training Time: 8.07 seconds\n",
            "Batch Size: 2048, Training Time: 8.38 seconds\n",
            "Batch Size: 8192, Training Time: 9.47 seconds\n",
            "Batch Size: 32768, Training Time: 12.77 seconds\n",
            "Batch Size: 131072, Training Time: 16.32 seconds\n",
            "Batch Size: 524288, Training Time: 16.27 seconds\n",
            "Batch Size: 2097152, Training Time: 16.37 seconds\n",
            "Batch Size: 8388608, Training Time: 16.30 seconds\n",
            "Batch Size: 33554432, Training Time: 16.44 seconds\n",
            "Batch Size: 134217728, Training Time: 19.67 seconds\n",
            "Batch Size: 536870912, Training Time: 21.83 seconds\n",
            "Batch Size: 2147483648, Training Time: 30.68 seconds\n",
            "Batch Size: 8589934592, Training Time: 64.33 seconds\n"
          ]
        }
      ],
      "source": [
        "def measure_training_time(batch_size):\n",
        "    try:\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        net = resnet18().to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "        # Warm-up epoch\n",
        "        train(0, trainloader, net, criterion, optimizer)\n",
        "\n",
        "        # Timed epoch\n",
        "        start_time = time.time()\n",
        "        train(1, trainloader, net, criterion, optimizer)\n",
        "        end_time = time.time()\n",
        "\n",
        "        return end_time - start_time\n",
        "    except RuntimeError as e:\n",
        "        if 'out of memory' in str(e):\n",
        "            print(f\"Out of memory for batch size: {batch_size}\")\n",
        "        else:\n",
        "            raise e\n",
        "        return None\n",
        "\n",
        "batch_size = 32\n",
        "times = []\n",
        "while True:\n",
        "    training_time = measure_training_time(batch_size)\n",
        "    if training_time is not None:\n",
        "        times.append((batch_size, training_time))\n",
        "        print(f\"Batch Size: {batch_size}, Training Time: {training_time:.2f} seconds\")\n",
        "        batch_size *= 4  # Increase batch size 4-fold\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Output the times for analysis\n",
        "print(times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1w0-Qn-CAZl"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The above output shows different training times with Batch size going from 32 until 1 GPU cannot handle it anymore (increasing 4-fold) and it dies. We can notice that at first - from 32 to 512 - the training time decreases with increasing batch size but after 512 training time starts increasing with increasing batch size. From this we can conclude that very small batch sizes lead to less efficient GPU usage which does not optimize the training time, while extremely large batch sizes can strain or exceed GPU memory limit which can again lead to slower training times. This just shows us how important it is to choose the right batch size for training while making sure to find a good balance between computational efficiency and memory constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7f6mM_CCFcY",
        "outputId": "c59b59e2-f4f3-4c51-d10d-be9260197d3b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Size: 32, GPUs: 1, Training Time: 16.10 seconds\n",
            "Batch Size: 32, GPUs: 2, Training Time: 41.32 seconds\n",
            "Batch Size: 32, GPUs: 4, Training Time: 24.85 seconds\n",
            "Batch Size: 128, GPUs: 1, Training Time: 8.37 seconds\n",
            "Batch Size: 128, GPUs: 2, Training Time: 10.53 seconds\n",
            "Batch Size: 128, GPUs: 4, Training Time: 9.29 seconds\n",
            "Batch Size: 512, GPUs: 1, Training Time: 9.19 seconds\n",
            "Batch Size: 512, GPUs: 2, Training Time: 8.78 seconds\n",
            "Batch Size: 512, GPUs: 4, Training Time: 8.76 seconds\n"
          ]
        }
      ],
      "source": [
        "def measure_training_time_multi_gpu(batch_size, num_gpus):\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size*num_gpus, shuffle=True, num_workers=2)\n",
        "    net = resnet18().to(device)\n",
        "    if num_gpus > 1:\n",
        "        net = DataParallel(net, device_ids=list(range(num_gpus)))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    # Warm-up epoch\n",
        "    train(0, trainloader, net, criterion, optimizer)\n",
        "\n",
        "    # Timed epoch\n",
        "    start_time = time.time()\n",
        "    train(1, trainloader, net, criterion, optimizer)\n",
        "    end_time = time.time()\n",
        "\n",
        "    return end_time - start_time\n",
        "\n",
        "num_available_gpus = torch.cuda.device_count()\n",
        "\n",
        "# Measure training times for batch sizes 32, 128, and 512 on 1, 2, and 4 GPUs\n",
        "batch_sizes = [32, 128, 512]\n",
        "for batch_size in batch_sizes:\n",
        "    for num_gpus in [1, 2, 4]:\n",
        "        if num_gpus <= num_available_gpus:\n",
        "            training_time = measure_training_time_multi_gpu(batch_size, num_gpus)\n",
        "            print(f\"Batch Size: {batch_size}, GPUs: {num_gpus}, Training Time: {training_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWV5Ee_0CHsk"
      },
      "source": [
        "From above output and the table we can see that for batch sizes of 32 and 128, 1 GPU training time is the fastest, followed by 4 GPU, while 2 GPU training time is the slowest. However, in the case of batch size of 512, training time is the fastest for 4 GPUs, followed by 2 GPU while 1 GPU is the slowest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd2nKyduCFcZ"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "|        | Batch-size 32 per GPU |              | Batch-size 128 per GPU |              | Batch-size 512 per GPU |              |\n",
        "|--------|-----------------------|--------------|------------------------|--------------|------------------------|--------------|\n",
        "|        | Time (sec)            | Speedup      | Time (sec)             | Speedup      | Time (sec)             | Speedup      |\n",
        "| 1-GPU  | 16.10                 | 1            | 8.37                   | 1            | 9.19                   | 1            |\n",
        "| 2-GPU  | 41.32                 | 0.78         | 10.53                  |1.59        | 8.78                   | 2.09         |\n",
        "| 4-GPU  | 24.85                 | 2.59         | 9.29                   | 3.60         | 8.76                   | 4.20        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX31AT7NfpUv"
      },
      "source": [
        "Speedup calculation:\n",
        "\n",
        "For 2 GPUs: Speedup = (2 * Time for 1 GPU) / (Time for 2 GPUs)\n",
        "<br>\n",
        "For 4 GPUs: Speedup = (4 * Time for 1 GPU) / (Time for 4 GPUs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM1Sl3yTD7lP"
      },
      "source": [
        "Looking at the speedup, we can say that for batch size of 32 and 2 GPUs is actually a \"slowdown\" as having 2 GPUs takes more time than 1 GPU does. This shows us that for small batch sizes the overhead of distributing the work across multiple GPUs could outweight the benefits. However, for batch size of 32 and 4GPU we have the expected speedup of around 2.6. Further, we can see that for batch size of 128 and 512 we have a significant speedup for both 2 and 4 GPU setups compared to 1 GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c0adJ_NE3Zq"
      },
      "source": [
        "**Comment on which type of scaling we are measuring: weak-scaling or strong-scaling?**\n",
        "\n",
        "**Comment on if the other type of scaling was used to speed up the number will be better or worse than what you are measuring.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkAGQj4nE2Ls"
      },
      "source": [
        "When using Weak-Scaling, the workload per GPU remains constant while the number of processors increase. On the other hand, in Strong-Scaling the total workload remains constant while increasing the number of GPUs.\n",
        "\n",
        "Our case fits better with Weak-Scaling since the batch size per GPU is constant while we are incresing the number of GPUs. We can say that Strong-Scaling is not being used as total workload (total batch size) increases with more GPUs rather than being constant.\n",
        "\n",
        "As mentioned above, we can see from the results that Weak-Scaling efficiency is not linear, especially for smaller batch sizes (32, 128) where the overhead of parallelization (when using multiple GPUs) leads to worse performance than that of a 1 GPU. This is most likely due to the communication overhead and the inefficiency of smaller workloads (batch-sizes) on multiple GPUs.\n",
        "\n",
        "If we chose to use Strong-Scaling instead of Weak-Scaling, we would need to keep total batch size constant while increasing the number of GPUs. This could have lead to better results for smaller batch sizes as Strong-Scaling can reduce the workloan per GPU  which makes the training process more efficient for smaller batch sizes. However, for larger batch sizes Strong-Scaling could also potentially lead to worse results since it might lead to under-utilization of each GPU's capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMHZDOECjD3"
      },
      "source": [
        "## 2.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_lwd-A3CIbx"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "\n",
        "|        | Batch-size 32 per GPU |             | Batch-size 128 per GPU |             | Batch-size 512 per GPU |             |\n",
        "|--------|-----------------------|-------------|------------------------|-------------|------------------------|-------------|\n",
        "|        | Compute(sec)          | Comm(sec)   | Compute(sec)           | Comm(sec)   | Compute(sec)           | Comm(sec)   |\n",
        "| 2-GPU  | 8.05                 | 33.27       | 4.19                   | 6.34       | 4.59                   | 4.19       |\n",
        "| 4-GPU  | 4.03                 | 20.82        | 2.09                   | 7.2        | 2.29                   | 6.47       |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRd6kumGKsI8"
      },
      "source": [
        "As we have training times for 1 GPU, 2 GPU, and 4 GPU setup we will calculate compute time and communication time for 2 and 4 GPUs the following way:\n",
        "- We will approximate **Compute Time** of multi-GPU setup to be equal to training time for 1 GPU diveded by the number of GPUs used (2 for 2-GPU setup and 4 for 4-GPU setup) since there is no communication overhead (it does not communicate with anyone since it's just 1 GPU).\n",
        "- We will calculate the **Communication Time** as the difference in the total training time of multi-GPUs and their compute time (1 GPU training time / N for corresponding batch size).\n",
        "\n",
        "As this is not a perfect method we have to keep in mind we are making these assumptions:\n",
        "- The computation time scales perfectly with the number of GPUs. (this is often not true due to the inefficiencies and overheads)\n",
        "- The communication time is the only additional time when moving from single GPU to multiple GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycvaDdLsMmPz"
      },
      "source": [
        "**2-GPU Calculations:**\n",
        "- **Batch size 32:** Compute time = 1-GPU time for batch size 32, which is 16.10 seconds / 2 = 8.05. Communication time would be 41.32 (total time for 2-GPU) - 8.05 = 33.27 sec.\n",
        "- **Batch size 128:** Compute time is 4.19 sec. Communication time would be 10.53 - 4.19 = 6.34 sec.\n",
        "- **Batch size 512:** Compute time is 4.59 sec. Communication time would be 8.78 - 4.59 = 4.19 sec.\n",
        "\n",
        "\n",
        "**4-GPU Calculations:**\n",
        "- **Batch size 32:** Compute time = 1-GPU time for batch size 32, which is 16.10 seconds / 4 = 4.03 sec. Communication time would be 24.85 - 4.03 = 20.82 sec.\n",
        "- **Batch size 128:** Compute time is 2.09 sec. Communication time would be 9.29 - 2.09 = 7.2 sec.\n",
        "- **Batch size 512:** Compute time is 2.29 sec. Communication time would be 8.76 - 2.29 = 6.47 sec.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIm7c26UMnN9"
      },
      "source": [
        "We can notice that the compute etime is the smallest for both 2 and 4 GPU setup when using 128 batch size. Further we can also notice that communication time decreases by increasing batch size and thus is the smallest with 512 batch size.\n",
        "\n",
        "We can say that this approach (using 1 GPU training time as benchmark for multi-GPU compute time) gives us a good approximation, we should do more detailed calculations to better (more accuratly) calculate and understand compute and communication times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmrANWyOCRC0"
      },
      "source": [
        "## 2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQBRzWG3G0cw"
      },
      "source": [
        "Formula for Allreduce:  $2(N-1)\\frac{K}{N}$\n",
        "\n",
        "Where\n",
        "  - K is number of model parameters (for resnet18 in our case)\n",
        "  - N is number of GPUs utilized\n",
        "\n",
        "\n",
        "\n",
        "Source: https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgbTk6yrG7Fw"
      },
      "source": [
        "Formula for Bandwidth Utilization = Allreduce communication cost / Communication time (from 2.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8kEEnYSH6QD"
      },
      "source": [
        "Let's first calculate the Allreduce Cost for 2 and 4 GPUs:\n",
        "\n",
        "K = number of parameters in resnet18 = 11,689,512\n",
        "- Source: https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html\n",
        "\n",
        "<br>\n",
        "\n",
        "- 2 GPU Allreduce Cost = $2(2 -1) \\frac{11689512}{2}$ = $11,689,512$\n",
        "\n",
        "- 4 GPU Allreduce Cost = $2(4 -1) \\frac{11689512}{4}$ = $17,534,268$\n",
        "\n",
        "As we want our final results in GB, we will convert these to GB:\n",
        "- 2 GPU Allreduce Cost = $\\frac{11,689,512*4bytes}{2^{30}} = 0.044 GB$\n",
        "\n",
        "- 4 GPU Allreduce Cost = $\\frac{17,534,268*4bytes}{2^{30}} = 0.065 GB$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGpfko_NCIbx"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "For Bandwidth we devide above mentioned Allreduce costs with corresponding communication time from part 2.3 for each batch size:\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "|        | Batch-size-per-GPU 32                  | Batch-size-per-GPU 128                 | Batch-size-per-GPU 512                 |\n",
        "|--------|----------------------------------------|----------------------------------------|----------------------------------------|\n",
        "|        | Bandwidth Utilization (GB/s)           | Bandwidth Utilization (GB/s)           | Bandwidth Utilization (GB/s)           |\n",
        "| 2-GPU  | 0.0013 |0.0069  |0.0105|\n",
        "| 4-GPU  | 0.0031|0.0090   |0.01005 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao13NidbLIlh"
      },
      "source": [
        "We can see that the bandwith utilizations are quite small (when in GB/s) but also that they grow significanly with increase in batch size."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
