{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH-oCacQcg5S"
      },
      "source": [
        "# Problem 1 - Sentiment Analysis using recurrent models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-ir4ZYFcg5T"
      },
      "source": [
        "## 1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaQU3CFicg5T"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xldTkM0qcg5V",
        "outputId": "fcf6ff04-be9d-4d93-bf60-a15194efb905"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  One of the other reviewers has mentioned that ...          1\n",
              "1  A wonderful little production. <br /><br />The...          1\n",
              "2  I thought this was a wonderful way to spend ti...          1\n",
              "3  Basically there's a family where a little boy ...          0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\", usecols=[\"review\", \"sentiment\"], encoding='latin-1')\n",
        "## 1 - positive, 0 - negative\n",
        "df.sentiment = (df.sentiment == \"positive\").astype(\"int\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-36PITnScg5X",
        "outputId": "d283ea0d-5bd9-4c1e-d875-95d559d0f9a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35000 7499 7499\n",
            "35000 35000 7499\n"
          ]
        }
      ],
      "source": [
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "\n",
        "\n",
        "def train_val_test_split(df=None, train_percent=0.7, test_percent=0.15, val_percent=0.15):\n",
        "  df = df.sample(frac=1)\n",
        "  train_df = df[: int(len(df)*train_percent)]\n",
        "  test_df = df[int(len(df)*train_percent)+1 : int(len(df)*(train_percent+test_percent))]\n",
        "  val_df = df[int(len(df)*(train_percent + test_percent))+1 : ]\n",
        "  return train_df, test_df, val_df\n",
        "\n",
        "train_df, test_df, val_df = train_val_test_split(df, 0.7, 0.15, 0.15)\n",
        "train_labels, train_texts = train_df.values[:,1], train_df.values[:,0]\n",
        "val_labels, val_texts = val_df.values[:,1], val_df.values[:,0]\n",
        "test_labels, test_texts = test_df.values[:,1], test_df.values[:,0]\n",
        "print(len(train_df), len(test_df), len(val_df))\n",
        "print(len(train_texts), len(train_labels), len(val_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWrDFX6Vcg5Z",
        "outputId": "c384f410-4bf4-4e89-d4a9-f9878c06f578"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([\"i'm not going to ramble on about it but i'm just going to make it brief. basically for those who don't know how prue actually died........... the first time round the demonic assassin comes hit piper and prue with an energy ball they fly through the wall blood everywhere. phoebe the third sister comes down the stairs, says the spell which send him away but not vanquished.(NEEDS THE POWER OF THREE)leo comes heals them both and so on. they get exposed along the line and the only way the can be saved is for a demon named tempus to turn back time. the only way he can do that is is phoebe stays in the underworld. she agrees, tempus turns back time. it now around 7:00 in the morning again. demon comes strucks piper and prue with energy ball. they fly through wall again. but this time phoebe isn't there to say the spell to fend demon off. demon kills doctor. doctor flies through window. he is dead. demon goes in a whirl wing type thing and glass on the doors shatter which is a great effect bye the way and there is and airy sound. thats where it ends. NOW.......... what the whole world doesn't know if they didn't pay attention to the next episode. although what i'm about to say wasn't shown its what happens trust me................ because this time there was no phoebe to call for leo this time he arrives later. piper survives because her injuries wern't as fatal as prue's and leo heals her first before prue so by that time prue is already dead. there mystery solved. ps calling for prue with a spell should have worked!!! and she should have made a surprise appearance in the last ever episode.OK i did ramble on\",\n",
              "       'Back in the 1960\\'s, those of us who were bad movie aficionados thought that \"Plan Nine From Outer Space\" was the worst movie ever made, and would remain so for all time. To put things in perspective, though, we also thought that $3,000 was a lot to pay for a new car.<br /><br />As we grew older, our innocence was gradually stripped away as we were exposed to movies like \"Hercules in New York\" and \"Overdrawn at the Memory Bank,\" which completely redefined the \"bad movie\" genre. In this context, last night, my son and I saw \"Alien From L.A.,\" which pushed the envelope to an extreme unimaginable just a generation ago. To call this movie \"bad\" (or wretched or execrable) completely fails to do it justice, as does any other label existent in the English language. Even if there were words with which to accurately describe this movie, it would be of no consequence, since they would be banned in civilized society.<br /><br />The Alien referred to in the title is played by Kathy Ireland, who apparently took some time off from modeling swimsuits for Sports Illustrated, to kick off her cinematic career. Her casting might seem some sort of recommendation, until you actually see the movie. The makeup artists earned their money by making Kathy look so drab and unappetizing you would not want to touch her with the far end of a broomstick -- no mean feat. To put it bluntly, in this movie she has a face that would freeze Medusa. Even worse than her look, though, was her voice, which was so raucous that I initially failed to credit it as originating with a human being. Throughout the movie, I found myself longing for a chalkboard to drag my nails across to cover the screechy twang of her dialog. At the end of the movie, Kathy finally gets a makeover and finds herself in her beloved swimsuit. I suggested to my son that the movie would have been better if they had put her in the swimsuit at the beginning of the movie, so at least we would have had something to watch. My son perceptively pointed out that if they had then removed the swimsuit and stuffed it into her mouth, it would have considerably improved the movie on two counts. I defer to the plain brilliance of his observation. If you have any doubts, compare this dreck to \"Barbarella,\" in which a competent filmmaker shows how to exploit the assets of an ethereally beautiful leading lady in the fantasy genre.<br /><br />Of the plot, itself, there is little on which to comment, since there was so little in evidence. It is said that if a million monkeys typed unceasingly for millions of years, eventually one would come up with \"Hamlet.\" By the process of elimination, the rest of the time they would come up with something approximating this screenplay. Imagine, if you will, a modern-day Alice falling into a hole and dropping 500 feet onto a rock slab, following which she gets up, dusts herself off, and starts looking for her long-lost father in the city-kingdom of Atlantis. Once in Atlantis, she spends most of her time running, fighting, or climbing stairs and ladders, and basically trying to keep out of the hands of a general who seems to have no soldiers to do his bidding, and who would make Tiny Tim look macho. This summation, as abbreviated as it appears, is probably longer than the shooting script.<br /><br />On the plus side, as you revel in the production values and take in whatever you can of the sets and costumes through the smoke and haze, you realize that this is one movie in which you can actually see on the screen where all $20 of the budget went.<br /><br />The thought that kept going through my mind was that filmmakers ought not be given access to drugs and alcohol while they are shooting a movie, or perhaps prior, if it leads to results like \"Alien from L.A.,\" though in fairness I have to acknowledge that I don\\'t know whether they were actually involved in substance abuse, or were simply brain dead at the outset of the project.',\n",
              "       'Rarely have I seen a work of literature translated so badly to the screen. The hysterical cast of b-movie and sitcom extras simply make the characters seem like bad Jewish stereotypes. The worst of all is Melissa Gilbert, who you hate from scene one and never develop any sympathy for. Performances like this should be noted and used against actors who wish to work again. All in all, a seedy, low-budget made-for-TV film of the sort that gives made-for-TV films a bad name.',\n",
              "       'L\\'OSSESSA, also known as THE TORMENTED or THE SEXORCIST, or the ridiculously titled THE EERIE MIDNIGHT HORROR SHOW, is a forgotten EXORCIST rip-off which contains one of the best horror moments I\\'ve ever seen. The scene is when the crucifix comes alive. This great spooky scene is unforgettable and totally effective (great FXs). It\\'s a shame the rest of the movie doesn\\'t maintain the level of creepiness exemplified during that scene.<br /><br />This is one of the most frustrating movies ever. Imagine the producers deciding to to make an EXORCIST copy but while making it, they actually succeed in creating something truly original (a possessed sculpture of crucified man, which is shocking when you think about it) but then completely forgets their original idea in order to make a boring and uninspired EXORCIST rip-off. Had the film continued with the possessed sculpture concept (with the characters trying to destroy it, etc), this film would have rocked. But once the girl becomes possessed by the spirit of the sculpture, she never tells anyone from where the demon came from. She, and the script, completely forgets the haunted crucifix, which is STUPID!!!<br /><br />If you like so-called \"Euro-cult\" movies, the first 45 minutes deliver unlike any other Euro-cult movies. But after the scene when the girl has a vision of being crucified and she gets stigmatas, the remaining 45 minutes SUCK. Boring. It goes nowhere fast as it tries to emulate (badly) THE EXORCIST. So, watch the first 45 minutes of L\\'OSSESSA and enjoy the 1970s fashions, the sleaze and the amazing statue-comes-alive-to-ravish-the-girl scene but after the first 45 minutes, press stop and eject, and might as well go clip your toenails.',\n",
              "       \"OK, I overrated it just a bit to offset at least one of those grumpy reviews. But I did enjoy it. I didn't laugh out loud, but it held my interest and pulled me along without dropping me at any point. The story built. Yeah, you knew it would have an happy ending--this genre always does. Meantime, it was quirky with sight gags you could miss, so pay attention when you watch. Stiller and Black delivered expertly yet again. Good team. They should work together more. Don't know that it will be a cult classic, but it was certainly a fun ride. Not as good as WHAT ABOUT BOB, or DIRTY ROTTEN SCOUNDRELS but what is? It is still worth going out of your way a little to get and watch this movie.\"],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_texts[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23DG6j7gcg5Z"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fe92B4dcg5a"
      },
      "outputs": [],
      "source": [
        "def process_tokens(text):\n",
        "    \"\"\"\n",
        "    function to process tokens, replace any unwanted chars\n",
        "    \"\"\"\n",
        "    preprocessed_text = text.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\":\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"(\", \"\")\n",
        "    preprocessed_text = ''.join([i for i in preprocessed_text if not preprocessed_text.isdigit()])\n",
        "    return preprocessed_text\n",
        "\n",
        "def preprocessing(data):\n",
        "    \"\"\"\n",
        "    preprocessing data to list of tokens\n",
        "    \"\"\"\n",
        "    nlp = English()\n",
        "    tokenizer = Tokenizer(nlp.vocab)\n",
        "    preprocessed_data = []\n",
        "    for sentence in data:\n",
        "        sentence = process_tokens(sentence)\n",
        "        tokens = tokenizer(sentence)\n",
        "        tlist = []\n",
        "        for token in tokens:\n",
        "            tlist.append(str(token))\n",
        "        preprocessed_data.append(tlist)\n",
        "    return preprocessed_data\n",
        "\n",
        "train_data = preprocessing(train_texts)\n",
        "val_data = preprocessing(val_texts)\n",
        "test_data = preprocessing(test_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_SGU914cg5b",
        "outputId": "20fc5efa-da5e-4cf7-f559-018ed533c244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"i'm\", 'not', 'going', 'to', 'ramble', 'on', 'about', 'it', 'but', \"i'm\", 'just', 'going', 'to', 'make', 'it', 'brief', 'basically', 'for', 'those', 'who', \"don't\", 'know', 'how', 'prue', 'actually', 'died', 'the', 'first', 'time', 'round', 'the', 'demonic', 'assassin', 'comes', 'hit', 'piper', 'and', 'prue', 'with', 'an', 'energy', 'ball', 'they', 'fly', 'through', 'the', 'wall', 'blood', 'everywhere', 'phoebe', 'the', 'third', 'sister', 'comes', 'down', 'the', 'stairs', 'says', 'the', 'spell', 'which', 'send', 'him', 'away', 'but', 'not', 'vanquishedneeds', 'the', 'power', 'of', 'threeleo', 'comes', 'heals', 'them', 'both', 'and', 'so', 'on', 'they', 'get', 'exposed', 'along', 'the', 'line', 'and', 'the', 'only', 'way', 'the', 'can', 'be', 'saved', 'is', 'for', 'a', 'demon', 'named', 'tempus', 'to', 'turn', 'back', 'time', 'the', 'only', 'way', 'he', 'can', 'do', 'that', 'is', 'is', 'phoebe', 'stays', 'in', 'the', 'underworld', 'she', 'agrees', 'tempus', 'turns', 'back', 'time', 'it', 'now', 'around', '700', 'in', 'the', 'morning', 'again', 'demon', 'comes', 'strucks', 'piper', 'and', 'prue', 'with', 'energy', 'ball', 'they', 'fly', 'through', 'wall', 'again', 'but', 'this', 'time', 'phoebe', \"isn't\", 'there', 'to', 'say', 'the', 'spell', 'to', 'fend', 'demon', 'off', 'demon', 'kills', 'doctor', 'doctor', 'flies', 'through', 'window', 'he', 'is', 'dead', 'demon', 'goes', 'in', 'a', 'whirl', 'wing', 'type', 'thing', 'and', 'glass', 'on', 'the', 'doors', 'shatter', 'which', 'is', 'a', 'great', 'effect', 'bye', 'the', 'way', 'and', 'there', 'is', 'and', 'airy', 'sound', 'thats', 'where', 'it', 'ends', 'now', 'what', 'the', 'whole', 'world', \"doesn't\", 'know', 'if', 'they', \"didn't\", 'pay', 'attention', 'to', 'the', 'next', 'episode', 'although', 'what', \"i'm\", 'about', 'to', 'say', \"wasn't\", 'shown', 'its', 'what', 'happens', 'trust', 'me', 'because', 'this', 'time', 'there', 'was', 'no', 'phoebe', 'to', 'call', 'for', 'leo', 'this', 'time', 'he', 'arrives', 'later', 'piper', 'survives', 'because', 'her', 'injuries', \"wern't\", 'as', 'fatal', 'as', \"prue's\", 'and', 'leo', 'heals', 'her', 'first', 'before', 'prue', 'so', 'by', 'that', 'time', 'prue', 'is', 'already', 'dead', 'there', 'mystery', 'solved', 'ps', 'calling', 'for', 'prue', 'with', 'a', 'spell', 'should', 'have', 'worked!!!', 'and', 'she', 'should', 'have', 'made', 'a', 'surprise', 'appearance', 'in', 'the', 'last', 'ever', 'episodeok', 'i', 'did', 'ramble', 'on']\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2hiP4micg5b"
      },
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ9Ir53bcg5c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "## Creating a vectorizer to vectorize text and create matrix of features\n",
        "## Bag of words technique\n",
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = None\n",
        "        self.token_to_index = None\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        word_dict = {}\n",
        "        for sentence in dataset:\n",
        "            for token in sentence:\n",
        "                if token not in word_dict:\n",
        "                    word_dict[token] = 1\n",
        "                else:\n",
        "                    word_dict[token] += 1\n",
        "        word_dict = dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "        end_to_slice = min(len(word_dict), self.max_features)\n",
        "        word_dict = dict(itertools.islice(word_dict.items(), end_to_slice))\n",
        "        self.vocab_list = list(word_dict.keys())\n",
        "        self.token_to_index = {}\n",
        "        counter = 0\n",
        "        for token in self.vocab_list:\n",
        "            self.token_to_index[token] = counter\n",
        "            counter += 1\n",
        "\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
        "        for i, sentence in enumerate(dataset):\n",
        "            for token in sentence:\n",
        "                if token in self.token_to_index:\n",
        "                    data_matrix[i, self.token_to_index[token]] += 1\n",
        "        return data_matrix\n",
        "\n",
        "## max features - top k words to consider only\n",
        "max_features = 2000\n",
        "\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "\n",
        "## Checking if the len of vocab = k\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgmBK0xqcg5d",
        "outputId": "3f0125bd-671a-4b50-e752-64aad5d766a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[19.,  9.,  5., ...,  0.,  0.,  0.],\n",
              "       [38., 15., 15., ...,  0.,  0.,  0.],\n",
              "       [ 5.,  3.,  3., ...,  0.,  0.,  0.],\n",
              "       [31.,  8.,  5., ...,  0.,  0.,  0.],\n",
              "       [ 1.,  3.,  4., ...,  0.,  0.,  0.]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## each sequence of token is a vector of\n",
        "## token indices (with the count of those words)\n",
        "X_train[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNdWQJnMcg5e"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.astype('int')\n",
        "y_val = y_val.astype('int')\n",
        "y_test = y_test.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqkW6h7Bcg5e"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train, 2)\n",
        "y_test = to_categorical(y_test, 2)\n",
        "y_val = to_categorical(y_val, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4pIl9Uncg5f",
        "outputId": "8a47af5d-51bd-4768-cbf3-b6c23b83d784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train.shape: (35000, 1, 2000), y_train.shape: (35000, 2)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
        "X_val = X_val.reshape(-1, 1, X_val.shape[1])\n",
        "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
        "\n",
        "y_train = y_train.reshape(-1, 2)\n",
        "y_val = y_val.reshape(-1, 2)\n",
        "y_test = y_test.reshape(-1, 2)\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTsQlGgcg5g"
      },
      "source": [
        "## 1.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek5YUxR4cg5g"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import SimpleRNN, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wq5cEL7cg5h",
        "outputId": "7a3c7a05-a3c8-4f6b-8205-f64cbad09999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 256)               577792    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 578306 (2.21 MB)\n",
            "Trainable params: 578306 (2.21 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 2s 5ms/step - loss: 0.5293 - accuracy: 0.8131 - val_loss: 0.3085 - val_accuracy: 0.8693\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2650 - accuracy: 0.8889 - val_loss: 0.3124 - val_accuracy: 0.8714\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9096 - val_loss: 0.3323 - val_accuracy: 0.8690\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9345 - val_loss: 0.3578 - val_accuracy: 0.8692\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.9622 - val_loss: 0.4087 - val_accuracy: 0.8630\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9847 - val_loss: 0.4596 - val_accuracy: 0.8674\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.9933 - val_loss: 0.4913 - val_accuracy: 0.8617\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.9968 - val_loss: 0.5437 - val_accuracy: 0.8668\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.5660 - val_accuracy: 0.8657\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 0.9987 - val_loss: 0.5920 - val_accuracy: 0.8644\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "model = None\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(256, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdUpAc2Vcg5i",
        "outputId": "fd299dc4-9bb7-4505-f43c-08d03b794739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.556309163570404\n",
            "Test accuracy: 0.868382453918457\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmisrTi4cg5i"
      },
      "source": [
        "We can see that through training epochs for RNN model, our training loss decreases significantly, almost reaching 0, but our validation loss increases which most likely means that the model is most likely overfitting. Further, the training accuracy significantly increases to up to almost 100% while the validation accuracy does not increase through training which again indicates there could be overfitting. Finally, we can see that our test loss is about 0.56 and test accuracy is about 0.87 which is relatively high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN6VmyDtcg5j"
      },
      "source": [
        "## 1.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap7KZlCfcg5j"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGW1SMbDcg5k",
        "outputId": "0cc85e64-f1e0-4ff4-e10c-37f78c9474b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 256)               2311168   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2311682 (8.82 MB)\n",
            "Trainable params: 2311682 (8.82 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 2s 6ms/step - loss: 0.3751 - accuracy: 0.8253 - val_loss: 0.3081 - val_accuracy: 0.8700\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2587 - accuracy: 0.8932 - val_loss: 0.3062 - val_accuracy: 0.8713\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9107 - val_loss: 0.3147 - val_accuracy: 0.8694\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9342 - val_loss: 0.3547 - val_accuracy: 0.8634\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9592 - val_loss: 0.3911 - val_accuracy: 0.8617\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0758 - accuracy: 0.9767 - val_loss: 0.4306 - val_accuracy: 0.8600\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9874 - val_loss: 0.4828 - val_accuracy: 0.8584\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.9942 - val_loss: 0.5175 - val_accuracy: 0.8601\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0151 - accuracy: 0.9973 - val_loss: 0.5746 - val_accuracy: 0.8629\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.9981 - val_loss: 0.5926 - val_accuracy: 0.8606\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "model = None\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-kT7a4ycg5k",
        "outputId": "e02edb53-6b7c-452c-c12a-f27bb5fa98c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5641476511955261\n",
            "Test accuracy: 0.8642485737800598\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-uKanRncg5l"
      },
      "source": [
        "We can see that through training epochs for LSTM model, our training loss decreases significantly, almost reaching 0, but our validation loss increases which most likely means that the model is most likely overfitting. Further, the training accuracy significantly increases to up to almost 100% while the validation accuracy actually decreases by 1 percentage point which again indicates there could be overfitting. Finally, we can see that our test loss is about 0.56 and test accuracy is about 0.86 which is relatively high and very similar to that of RNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVyohchNcg5l"
      },
      "source": [
        "## 1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55S1jF1fcg5l"
      },
      "outputs": [],
      "source": [
        "# getting data ready\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caKEmtmScg5m"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.astype('int')\n",
        "y_val = y_val.astype('int')\n",
        "y_test = y_test.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgKepbsBcg5m"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train, 2)\n",
        "y_test = to_categorical(y_test, 2)\n",
        "y_val = to_categorical(y_val, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OJL8qEbcg5n",
        "outputId": "62a91adf-e335-41a0-9027-9384ea98c5f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train.shape: (35000, 1, 2000), y_train.shape: (35000, 2)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1, 1, X_train.shape[1])\n",
        "X_val = X_val.reshape(-1, 1, X_val.shape[1])\n",
        "X_test = X_test.reshape(-1, 1, X_test.shape[1])\n",
        "\n",
        "y_train = y_train.reshape(-1, 2)\n",
        "y_val = y_val.reshape(-1, 2)\n",
        "y_test = y_test.reshape(-1, 2)\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}, y_train.shape: {y_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMvpbkaCcg5o",
        "outputId": "11cf6d67-fcb7-4236-ba4d-06571f315398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_1 (GRU)                 (None, 256)               1734144   \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1734658 (6.62 MB)\n",
            "Trainable params: 1734658 (6.62 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 3s 6ms/step - loss: 0.3878 - accuracy: 0.8363 - val_loss: 0.3113 - val_accuracy: 0.8669\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2622 - accuracy: 0.8911 - val_loss: 0.3015 - val_accuracy: 0.8705\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.9070 - val_loss: 0.3151 - val_accuracy: 0.8690\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9334 - val_loss: 0.3433 - val_accuracy: 0.8622\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.1169 - accuracy: 0.9569 - val_loss: 0.3973 - val_accuracy: 0.8628\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0666 - accuracy: 0.9791 - val_loss: 0.4649 - val_accuracy: 0.8596\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9908 - val_loss: 0.5075 - val_accuracy: 0.8585\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.9955 - val_loss: 0.5858 - val_accuracy: 0.8590\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9979 - val_loss: 0.6453 - val_accuracy: 0.8621\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.6860 - val_accuracy: 0.8624\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
            "Test loss: 0.6334048509597778\n",
            "Test accuracy: 0.8703827261924744\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "model.add(GRU(256, input_shape=(1, max_features)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=256,\n",
        "          validation_data=(X_val, y_val),\n",
        "          epochs=10)\n",
        "print(history.history.keys())\n",
        "\n",
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0666avGcg5p"
      },
      "source": [
        "We can see that through training epochs for GRU model, our training loss decreases significantly, almost reaching 0, but our validation loss increases which most likely means that the model is most likely overfitting. Further, the training accuracy significantly increases to up to almost 100% while the validation accuracy actually decreases slightly (only around 0.04) which again indicates there could be overfitting. Finally, we can see that our test loss is about 0.63 and test accuracy is about 0.87 which is relatively high and slighty higher (both loss and acuracy are slightly higher) than in our previous 2 models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mlwXpoXcg5q",
        "outputId": "d27c4a2e-7a93-4c25-89e1-07ebfb430aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 0s 975us/step\n",
            "Label predicted: 0, Actual label: 0\n",
            "text: Its a truly awful movie with a laughable storyline.some awful acting.and a script that Ed Wood might be ashamed of.Wagner is laughable in this. He plays his role like number two in Austin Powers.Easily the worst of the Airport movies.1 out of 10\n",
            "Label predicted: 1, Actual label: 0\n",
            "text: The fight scenes were great. Loved the old and newer cylons and how they painted the ones on their side. It was the ending that I hated. I was disappointed that it was earth but 150k years back. But to travel all that way just to start over? Are you kidding me? 38k people that fought for their very existence and once they get to paradise, they abandon technology? No way. Sure they were eating paper and rationing food, but that is over. They can live like humans again. They only have one good doctor. What are they going to do when someone has a tooth ache never mind giving birth... yea right. No one would have made that choice.\n",
            "Label predicted: 0, Actual label: 0\n",
            "text: I really wanted to love this show. I truly, honestly did.<br /><br />For the first time, gay viewers get their own version of the \"The Bachelor\". With the help of his obligatory \"hag\" Andra, James, a good looking, well-to-do thirty-something has the chance of love with 15 suitors (or \"mates\" as they are referred to in the show). The only problem is half of them are straight and James doesn't know this. If James picks a gay one, they get a trip to New Zealand, and If he picks a straight one, straight guy gets $25,000. How can this not be fun?! Take my hand, lets stroll: <br /><br />The most glaring problem with this show is the bachelor himself. James is your typical young and successful gay guy with a nice smile and body, the one you'd probably give two glances towards at your local bar before grazing for greener pastures. Why they chose to cast James as the leading man is beyond me. God knows there's so many other hotter and vivacious homosexual men out there dying to be on TV.<br /><br />Aside from his rather average physical appearance, James is about as interesting and exciting as a piece of chalk. Even as such, he has this arrogant, smugly condescending aura about him. However, if James were standing up against a blank, white wall he'd meld right into in it. I honestly can't recall a single interesting or noteworthy thing James said during the course of the show. He is THAT boring and forgettable. In fact, one of the mates flat out advised him he wasn't feeling a connection. I thought that was the best part of the show. Also, James speaks with an excruciatingly annoying lilt. Sound feminine or sound masculine, but don't ****ing segue tones in the middle of sentences...so painful to sit through. I hated him so much all throughout the show I kept thinking, \"Please choose a straight guy and humiliate yourself and your unfortunate looking hag\"<br /><br />Then we have the suitors. A remarkably bland bunch of men who don't seem to care either way what is happening. Equally vapid, they seem to be indistinguishable from one guy to the next except, \"Hey that guy has blond highlights or oh that one has curly hair\" Again, astoundingly inept casting decisions seem to be the aim of this show. While it may be hackneyed to type cast roles, it would've been a lot more entertaining to watch than these amorphous drones. However, in all their banality they still manage to upstage James (which isn't all that hard to do anyway), slightly that is. You know you have a problem when some of the suitors are actually hotter and more interesting than the leading man. And the fact that the suitors seem to have more fun around EACH OTHER than with the leading man? Very sad.<br /><br />Also, I just thought that Id point something mentioned on the message boards which I felt was actually true: the straight men are all hotter than the gay guys. <br /><br />Don't get me wrong, Im not saying all the gay guys were ugly and boring, as a matter of fact I found some of them very cute. It's just that overall they were just BLAH compared to the men you'd see on shows like A Shot At Love with Tila Tequila or The Bachelorette.<br /><br />I don't know how many times I hit fast forward during this show. I can accept a lead character as interesting as a cardboard box, I can accept the mundane, apathetic suitors but PLEASE for the love of God entertain me just a little. No such luck.<br /><br />If you're expecting drama, intrigue, sexiness, or excitement you will be SEVERELY disappointed. The biggest \"drama\" comes from the fact that one of the suitors still may have a boyfriend in New York (How scandalous!). As titillating as that may be I guarantee you, that is the ONLY thing that remotely resembles any conflict on this show.<br /><br />Sure there is the twist, but if you have any semblance of Gaydar in you, you'll easily discern who's who (it wasn't hard at all, I was only wrong once.) This show is stacking so much of its chips on the twist that it fails to deliver anywhere else.<br /><br />We get to watch as James & Co plod along such exciting activities such as learning how to Western step dance, shopping for gifts, visiting a petting zoo, and gay karaoke. YAWN. Sure you have the occasional topless dancing but who cares when everyone is boring anyway. That's one of main problems with the show: NO ONE seems to be enjoying themselves--they are there just going through the motion trying mightily hard to appear to have a good time. And you really cant blame them since the events are all wildly unimaginative and lame.<br /><br />Finally, the physical aspect is not there. There's no cuddling, no caressing, no kissing (!), no endearment of any sort. It's just \"Ok that was a boring date, Im gonna go back to my ugly, tacky wanna-be Sydney Operahouse dwelling (quick peck on the lips) CYA.\" This show is so ****ing prudish it's ridiculous. I can understand them not wanting to play up the perceived indiscretionary nature of homosexual men, but come the **** on. People who watch reality TV shows are gonna want more than standoffish hugs and curt kisses. This show refuses to compromise.<br /><br />Sorry if this was long winded but I felt these were issues that needed to be addressed. I do commend Bravo for first putting up a show of this nature, but the staggeringly incompetent manner in which this show was handled is mind boggling. To summarize my three points: Boring + Boring + Boring = go do something else. You'll have more fun waiting at a doctor's office for an appointment, at least they have interesting magazines there.\n",
            "Label predicted: 0, Actual label: 0\n",
            "text: When I caught a glimpse of the title I thought are we going to get another try-hard hip slasher, but actually I found \"7eventy 5ive\" to be a mildly passable, and almost 80s throwback after a tediously slow mid-section it picks up momentum for the final half-hour leading to it's outrageously tacky climax and downright cop out ending. It won't win awards for originality, because it's as systematic as you can get and steals its thunder in the way of thrills (usual cheap jump scares), location (secluded mansion) and motivation from other films. The gleaming direction is by-the-book and the material is quite hackneyed with poorly realised red herrings within its elaborate plotting and flimsy script. Sometimes laughable, but nonetheless I was entertained mainly due to its brutal and grisly acts of pulpy violence towards some rather obnoxiously annoying college students by a psychotic killer with a battle axe. The performances weren't bad in the shape of a spunky young cast, however the characters they were portraying weren't particularly enticing. An always presentable Rutger Hauer shows up in a short supportive role as a grizzled detective. A slickly made, but a shallow and forgettable addition to the fold.\n",
            "Label predicted: 1, Actual label: 1\n",
            "text: Somehow, CHANGI lost out in the AFI Awards to MY BROTHER JACK. The latter, a high-quality adaptation of George Johnston's immortal novel, was outstanding - but, in my opinion, not as good as CHANGI. I have heard that many critics dismissed CHANGI as being irrelevant, unimportant, historically inaccurate or even disrespectful. Who and where are these critics? CHANGI is outstanding. More than that. Brilliant. It's not supposed to be a documentary - certainly I can forgive the actual Changi survivors (or indeed any survivors of a POW camp) for being disappointed with the production - but to the rest of us, CHANGI represents the remarkable power of mateship in times of extreme adversity. It contains a part of the Australian culture that appears to be diminishing as times become easier and less challenging, but which we should never forget: Australians were respected worldwide after Gallipoli and WW2 for their comradery and sense of humour. Rating: 96/100. See also: GALLIPOLI; PARADISE ROAD; THE LAST BULLET; THE SUGAR FACTORY.\n"
          ]
        }
      ],
      "source": [
        "# check predictions\n",
        "from tensorflow.keras.backend import argmax\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "for i in range(5):\n",
        "  print(f'Label predicted: {argmax(y_pred[i]).numpy()}, Actual label: {argmax(y_test[i]).numpy()}')\n",
        "  print(f'text: {test_texts[i]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAc5m3kRcg5r"
      },
      "source": [
        "## 1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNoFUWFPcg5r",
        "outputId": "5cfdf8ea-04b2-4c01-d89c-3203accc58bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_3 (Bidirecti  (None, 256)               2180096   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2180610 (8.32 MB)\n",
            "Trainable params: 2180610 (8.32 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "137/137 [==============================] - 4s 9ms/step - loss: 0.3720 - accuracy: 0.8303 - val_loss: 0.3095 - val_accuracy: 0.8701\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.2665 - accuracy: 0.8882 - val_loss: 0.3123 - val_accuracy: 0.8629\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.2147 - accuracy: 0.9118 - val_loss: 0.3212 - val_accuracy: 0.8678\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9397 - val_loss: 0.3614 - val_accuracy: 0.8640\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.9682 - val_loss: 0.4090 - val_accuracy: 0.8633\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9850 - val_loss: 0.4672 - val_accuracy: 0.8609\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.9925 - val_loss: 0.5319 - val_accuracy: 0.8666\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.0161 - accuracy: 0.9969 - val_loss: 0.5740 - val_accuracy: 0.8601\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9983 - val_loss: 0.6186 - val_accuracy: 0.8632\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 0.9989 - val_loss: 0.6254 - val_accuracy: 0.8645\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "# Part 5: Define a BiLSTM model and train it on the dataset\n",
        "\n",
        "model = None\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(256), input_shape=(1, max_features)))\n",
        "\n",
        "# Add the output layer\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, batch_size=256,\n",
        "                    validation_data=(X_val, y_val), epochs=10)\n",
        "\n",
        "# Print history keys to verify training\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypJ2Lkpncg5s",
        "outputId": "6fb853d6-54a3-4def-9f92-7d0410c09349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5914520621299744\n",
            "Test accuracy: 0.8702493906021118\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "score, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpHT3wSfcg5t"
      },
      "source": [
        "We can see that through training epochs for GRU model, our training loss decreases significantly, almost reaching 0, but our validation loss increases which most likely means that the model is most likely overfitting. Further, the training accuracy significantly increases to up to almost 100% while the validation accuracy actually decreases slightly (only around 0.05) which again indicates there could be overfitting. Finally, we can see that our test loss is about 0.59 and test accuracy is about 0.87 which is relatively high and slighty higher (both loss and acuracy are slightly higher) than in our first 2 models, RNN and LSTM and very similar to our previous model (GRU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbIAP-g-cg5t"
      },
      "source": [
        "## 1.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxnRi0a2cg5u"
      },
      "source": [
        "**Model 1, RNN model:**  <br>\n",
        "Test loss: 0.556309163570404 <br>\n",
        "Test accuracy: 0.868382453918457\n",
        "<br><br>\n",
        "**Model 2, LSTM model:**<br>\n",
        "Test loss: 0.5641476511955261 <br>\n",
        "Test accuracy: 0.8642485737800598\n",
        "<br><br>\n",
        "**Model 3, GRU model:**<br>\n",
        "Test loss: 0.6334048509597778<br>\n",
        "Test accuracy: 0.8703827261924744\n",
        "<br><br>\n",
        "**Model 4, BiLSTM model:** <br>\n",
        "Test loss: 0.5914520621299744 <br>\n",
        "Test accuracy: 0.8702493906021118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly2tOLpAcg5v"
      },
      "source": [
        "Looking solely at accuracy we can see that the GRU model has the highest accuracy with a value of 0.8704. The BiLSTM model is very close, with an accuracy of 0.8702, followed by the RNN model (0.8684), and finally the LSTM model (0.8642). It is important to note that while GRU might be the best model looking solely at accuracy, BiLSTM is extremy close (0.0001 less) it does have sligly lower loss (about 0.04) so we might want to consider it rather than GRU. Furher, we can see that while RNN does not have highest accuracy (0.002 less than the highest) it does have the lowest loss. Finally, as we can see all the models are very close in performance - looking at both loss and accuracy so we might want to also consider which model might be simples / cheapest (least computationally expensive). We might want to consider if RNN might be the best model for us to use as it is the simplest, it has lowest loss (which might mean it has best generalization compared to others) and not significantly lower accuracy compared to BiLSTM and GRU.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "my_env",
      "language": "python",
      "name": "my_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
