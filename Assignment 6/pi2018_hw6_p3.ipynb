{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 3 - Ray Tune for Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RheLCyYeNXI"
      },
      "source": [
        "Sources: https://docs.ray.io/en/latest/tune/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UrFhzD1vTF0h"
      },
      "outputs": [],
      "source": [
        "#!pip install -U \"ray[tune]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P37-Wz0Pdv0i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.integration.keras import TuneReportCallback\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray.air import session\n",
        "from ray.tune.search.bayesopt import BayesOptSearch\n",
        "from ray.tune.schedulers import HyperBandScheduler\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "\n",
        "import time\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "ibZiOfQMjC6F",
        "outputId": "4a028352-c71a-4e05-d256-f32ba610c271"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-05 01:02:30,639\tINFO worker.py:1673 -- Started a local Ray instance.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
              "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
              "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
              "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
              "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
              "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
              "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
              "    </g>\n",
              "    <defs>\n",
              "        <clipPath id=\"clip0_4338_178347\">\n",
              "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
              "        </clipPath>\n",
              "    </defs>\n",
              "  </svg>\n",
              "</div>\n",
              "\n",
              "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
              "    <tr>\n",
              "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
              "        <td style=\"text-align: left\"><b>3.8.6</b></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
              "        <td style=\"text-align: left\"><b>2.8.1</b></td>\n",
              "    </tr>\n",
              "    \n",
              "</table>\n",
              "\n",
              "    </div>\n",
              "</div>\n"
            ],
            "text/plain": [
              "RayContext(dashboard_url='', python_version='3.8.6', ray_version='2.8.1', ray_commit='82a8df138fe7fcc5c42536ebf26e8c3665704fee', protocol_version=None)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y29cIL8Zjif0"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ2oil66TAZs"
      },
      "outputs": [],
      "source": [
        "def train_mnist(config):\n",
        "    num_classes = 10\n",
        "    epochs = 12\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_test = x_train.reshape(-1, 28, 28, 1) / 255.0, x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(filters=config[\"conv_filters\"], kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(config[\"dropout\"]),\n",
        "        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"lr\"]),\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.fit(\n",
        "            x_train,\n",
        "            y_train,\n",
        "            batch_size=config[\"batch_size\"],\n",
        "            epochs=1,\n",
        "            verbose=0,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "        # Evaluate the model\n",
        "        _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "        session.report({\"mean_accuracy\": accuracy})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CJ2XFO_gCIbx",
        "outputId": "c02867e6-6402-4c68-aa05-89b7df86d334"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-24 16:00:22,466\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-11-24 17:27:57</td></tr>\n",
              "<tr><td>Running for: </td><td>01:27:27.60        </td></tr>\n",
              "<tr><td>Memory:      </td><td>40.1/377.3 GiB     </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/48 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:V100)\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_filters</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">   acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_7b9c5_00000</td><td>TERMINATED</td><td>10.32.35.60:3069754</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9864</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         37.562 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00001</td><td>TERMINATED</td><td>10.32.35.60:3070976</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9864</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.3354</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00002</td><td>TERMINATED</td><td>10.32.35.60:3071872</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9873</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.6695</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00003</td><td>TERMINATED</td><td>10.32.35.60:3072733</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9876</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.132 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00004</td><td>TERMINATED</td><td>10.32.35.60:3073722</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9846</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.1848</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00005</td><td>TERMINATED</td><td>10.32.35.60:3074964</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9872</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.5316</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00006</td><td>TERMINATED</td><td>10.32.35.60:3076055</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9855</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         42.8917</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00007</td><td>TERMINATED</td><td>10.32.35.60:3077305</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9859</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         39.9443</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00008</td><td>TERMINATED</td><td>10.32.35.60:3078805</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9844</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         79.7661</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00009</td><td>TERMINATED</td><td>10.32.35.60:3080465</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9894</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        113.503 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00010</td><td>TERMINATED</td><td>10.32.35.60:3082482</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9881</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         48.9788</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00011</td><td>TERMINATED</td><td>10.32.35.60:3083529</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9884</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         92.6275</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00012</td><td>TERMINATED</td><td>10.32.35.60:3086299</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9876</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        136.485 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00013</td><td>TERMINATED</td><td>10.32.35.60:3088282</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9903</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         67.1727</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00014</td><td>TERMINATED</td><td>10.32.35.60:3089514</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.988 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         58.8631</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00015</td><td>TERMINATED</td><td>10.32.35.60:3090635</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9904</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        175.516 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00016</td><td>TERMINATED</td><td>10.32.35.60:3093221</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9888</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        183.547 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00017</td><td>TERMINATED</td><td>10.32.35.60:3095460</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9873</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        128.335 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00018</td><td>TERMINATED</td><td>10.32.35.60:3096973</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9895</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         64.5089</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00019</td><td>TERMINATED</td><td>10.32.35.60:3098087</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9871</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         53.3214</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00020</td><td>TERMINATED</td><td>10.32.35.60:3099218</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9873</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.8603</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00021</td><td>TERMINATED</td><td>10.32.35.60:3100150</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9898</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.4233</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00022</td><td>TERMINATED</td><td>10.32.35.60:3101117</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9888</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.4036</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00023</td><td>TERMINATED</td><td>10.32.35.60:3101993</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9887</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.5901</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00024</td><td>TERMINATED</td><td>10.32.35.60:3102887</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.989 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         41.3606</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00025</td><td>TERMINATED</td><td>10.32.35.60:3104084</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9894</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.7328</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00026</td><td>TERMINATED</td><td>10.32.35.60:3105018</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9888</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.4996</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00027</td><td>TERMINATED</td><td>10.32.35.60:3105949</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.987 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         30.7928</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00028</td><td>TERMINATED</td><td>10.32.35.60:3106873</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9881</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.4456</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00029</td><td>TERMINATED</td><td>10.32.35.60:3107769</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9862</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7906</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00030</td><td>TERMINATED</td><td>10.32.35.60:3108670</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9876</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.4079</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00031</td><td>TERMINATED</td><td>10.32.35.60:3109566</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9867</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.2517</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00032</td><td>TERMINATED</td><td>10.32.35.60:3110561</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9875</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.4794</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00033</td><td>TERMINATED</td><td>10.32.35.60:3111453</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9878</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.5292</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00034</td><td>TERMINATED</td><td>10.32.35.60:3112407</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9876</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.5639</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00035</td><td>TERMINATED</td><td>10.32.35.60:3113376</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9863</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.4986</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00036</td><td>TERMINATED</td><td>10.32.35.60:3114268</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9842</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         31.2422</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00037</td><td>TERMINATED</td><td>10.32.35.60:3115199</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9813</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.4238</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00038</td><td>TERMINATED</td><td>10.32.35.60:3116108</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9806</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7137</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00039</td><td>TERMINATED</td><td>10.32.35.60:3116973</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9844</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.3727</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00040</td><td>TERMINATED</td><td>10.32.35.60:3117939</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9845</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.2377</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00041</td><td>TERMINATED</td><td>10.32.35.60:3118868</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.983 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.5356</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00042</td><td>TERMINATED</td><td>10.32.35.60:3119768</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9839</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.9493</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00043</td><td>TERMINATED</td><td>10.32.35.60:3120757</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.984 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.4432</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00044</td><td>TERMINATED</td><td>10.32.35.60:3121665</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">0.9839</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.5323</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00045</td><td>TERMINATED</td><td>10.32.35.60:3122623</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9808</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         30.5881</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00046</td><td>TERMINATED</td><td>10.32.35.60:3123514</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9826</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.2957</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00047</td><td>TERMINATED</td><td>10.32.35.60:3124455</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9819</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7437</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00048</td><td>TERMINATED</td><td>10.32.35.60:3125329</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9827</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.7254</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00049</td><td>TERMINATED</td><td>10.32.35.60:3126253</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9816</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.1413</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00050</td><td>TERMINATED</td><td>10.32.35.60:3127213</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9809</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.4685</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00051</td><td>TERMINATED</td><td>10.32.35.60:3128071</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9817</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.3259</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00052</td><td>TERMINATED</td><td>10.32.35.60:3129072</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9827</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.595 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00053</td><td>TERMINATED</td><td>10.32.35.60:3130024</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9832</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.4012</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00054</td><td>TERMINATED</td><td>10.32.35.60:3130915</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9825</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         30.538 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00055</td><td>TERMINATED</td><td>10.32.35.60:3131848</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9865</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.4156</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00056</td><td>TERMINATED</td><td>10.32.35.60:3132748</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9889</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7466</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00057</td><td>TERMINATED</td><td>10.32.35.60:3133628</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9841</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.5286</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00058</td><td>TERMINATED</td><td>10.32.35.60:3134587</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9863</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.2325</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00059</td><td>TERMINATED</td><td>10.32.35.60:3135496</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9871</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.5568</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00060</td><td>TERMINATED</td><td>10.32.35.60:3136438</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9824</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.4781</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00061</td><td>TERMINATED</td><td>10.32.35.60:3137391</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9815</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.3129</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00062</td><td>TERMINATED</td><td>10.32.35.60:3138342</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9863</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.5686</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00063</td><td>TERMINATED</td><td>10.32.35.60:3139251</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9834</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         30.8235</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00064</td><td>TERMINATED</td><td>10.32.35.60:3140201</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9836</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.5203</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00065</td><td>TERMINATED</td><td>10.32.35.60:3141077</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9875</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7667</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00066</td><td>TERMINATED</td><td>10.32.35.60:3141966</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9807</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.1651</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00067</td><td>TERMINATED</td><td>10.32.35.60:3142953</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9826</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.3892</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00068</td><td>TERMINATED</td><td>10.32.35.60:3143812</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9877</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.5773</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00069</td><td>TERMINATED</td><td>10.32.35.60:3144741</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9828</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.8027</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00070</td><td>TERMINATED</td><td>10.32.35.60:3145735</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9848</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.71  </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00071</td><td>TERMINATED</td><td>10.32.35.60:3146643</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9847</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.5203</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00072</td><td>TERMINATED</td><td>10.32.35.60:3147593</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.977 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         31.2352</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00073</td><td>TERMINATED</td><td>10.32.35.60:3148512</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9815</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.3485</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00074</td><td>TERMINATED</td><td>10.32.35.60:3149430</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9834</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.8253</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00075</td><td>TERMINATED</td><td>10.32.35.60:3150328</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9758</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.4246</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00076</td><td>TERMINATED</td><td>10.32.35.60:3151239</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9785</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.2635</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00077</td><td>TERMINATED</td><td>10.32.35.60:3152164</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9837</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.5162</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00078</td><td>TERMINATED</td><td>10.32.35.60:3153021</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9765</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.7368</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00079</td><td>TERMINATED</td><td>10.32.35.60:3154049</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9818</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.2459</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00080</td><td>TERMINATED</td><td>10.32.35.60:3155031</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9837</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.6181</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00081</td><td>TERMINATED</td><td>10.32.35.60:3155953</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.8784</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         30.9048</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00082</td><td>TERMINATED</td><td>10.32.35.60:3156866</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9729</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.423 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00083</td><td>TERMINATED</td><td>10.32.35.60:3157760</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9756</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7809</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00084</td><td>TERMINATED</td><td>10.32.35.60:3158643</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9199</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.2129</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00085</td><td>TERMINATED</td><td>10.32.35.60:3159611</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.94  </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.2927</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00086</td><td>TERMINATED</td><td>10.32.35.60:3160565</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.9691</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.4929</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00087</td><td>TERMINATED</td><td>10.32.35.60:3161416</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.8925</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.4537</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00088</td><td>TERMINATED</td><td>10.32.35.60:3162413</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.8906</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.5454</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00089</td><td>TERMINATED</td><td>10.32.35.60:3163365</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.01 </td><td style=\"text-align: right;\">0.8941</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         29.0441</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00090</td><td>TERMINATED</td><td>10.32.35.60:3164270</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         30.4039</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00091</td><td>TERMINATED</td><td>10.32.35.60:3165225</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.101 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.4694</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00092</td><td>TERMINATED</td><td>10.32.35.60:3166118</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.5905</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00093</td><td>TERMINATED</td><td>10.32.35.60:3166961</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.8556</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.0017</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00094</td><td>TERMINATED</td><td>10.32.35.60:3167955</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.1075</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00095</td><td>TERMINATED</td><td>10.32.35.60:3168813</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.4826</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00096</td><td>TERMINATED</td><td>10.32.35.60:3169717</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1028</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.1092</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00097</td><td>TERMINATED</td><td>10.32.35.60:3170740</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.101 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.4475</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00098</td><td>TERMINATED</td><td>10.32.35.60:3171651</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0   </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.9379</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.9008</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00099</td><td>TERMINATED</td><td>10.32.35.60:3172602</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         31.1121</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00100</td><td>TERMINATED</td><td>10.32.35.60:3173494</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.3175</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00101</td><td>TERMINATED</td><td>10.32.35.60:3174415</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.9204</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7243</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00102</td><td>TERMINATED</td><td>10.32.35.60:3175308</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1028</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.2945</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00103</td><td>TERMINATED</td><td>10.32.35.60:3176244</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.8883</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.3192</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00104</td><td>TERMINATED</td><td>10.32.35.60:3177158</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.9181</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.625 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00105</td><td>TERMINATED</td><td>10.32.35.60:3178058</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.101 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.356 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00106</td><td>TERMINATED</td><td>10.32.35.60:3179128</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.8992</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.4972</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00107</td><td>TERMINATED</td><td>10.32.35.60:3180098</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.25</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.9248</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.5346</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00108</td><td>TERMINATED</td><td>10.32.35.60:3180999</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         31.3561</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00109</td><td>TERMINATED</td><td>10.32.35.60:3181971</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0958</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.3605</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00110</td><td>TERMINATED</td><td>10.32.35.60:3182843</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0974</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.6213</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00111</td><td>TERMINATED</td><td>10.32.35.60:3183751</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.5932</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00112</td><td>TERMINATED</td><td>10.32.35.60:3184657</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.101 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.3321</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00113</td><td>TERMINATED</td><td>10.32.35.60:3185613</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1028</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.5031</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00114</td><td>TERMINATED</td><td>10.32.35.60:3186527</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0974</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.9105</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00115</td><td>TERMINATED</td><td>10.32.35.60:3187451</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.4555</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00116</td><td>TERMINATED</td><td>10.32.35.60:3188441</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.5 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0958</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.4606</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00117</td><td>TERMINATED</td><td>10.32.35.60:3189361</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0974</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         31.3824</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00118</td><td>TERMINATED</td><td>10.32.35.60:3190330</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0998</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.3008</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00119</td><td>TERMINATED</td><td>10.32.35.60:3191237</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.098 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.699 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00120</td><td>TERMINATED</td><td>10.32.35.60:3192107</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1028</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.3912</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00121</td><td>TERMINATED</td><td>10.32.35.60:3193607</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.3562</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00122</td><td>TERMINATED</td><td>10.32.35.60:3194510</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1032</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.6054</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00123</td><td>TERMINATED</td><td>10.32.35.60:3195429</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1028</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.6836</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00124</td><td>TERMINATED</td><td>10.32.35.60:3196864</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1028</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.5074</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00125</td><td>TERMINATED</td><td>10.32.35.60:3197812</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.75</td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.101 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.5951</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00126</td><td>TERMINATED</td><td>10.32.35.60:3198763</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         31.4814</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00127</td><td>TERMINATED</td><td>10.32.35.60:3200129</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0958</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         23.3766</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00128</td><td>TERMINATED</td><td>10.32.35.60:3201062</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1028</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         19.7383</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00129</td><td>TERMINATED</td><td>10.32.35.60:3201925</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.0958</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         33.0029</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00130</td><td>TERMINATED</td><td>10.32.35.60:3203166</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         26.2793</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00131</td><td>TERMINATED</td><td>10.32.35.60:3204233</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.5883</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         22.6105</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00132</td><td>TERMINATED</td><td>10.32.35.60:3205108</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.098 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         40.6436</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00133</td><td>TERMINATED</td><td>10.32.35.60:3206375</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.098 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         32.6549</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00134</td><td>TERMINATED</td><td>10.32.35.60:3207540</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">     0.9 </td><td style=\"text-align: right;\">0.1  </td><td style=\"text-align: right;\">0.098 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         28.4822</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3069754)\u001b[0m 2023-11-24 16:00:31.576563: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3069754)\u001b[0m 2023-11-24 16:00:31.623251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3069754)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3069754)\u001b[0m 2023-11-24 16:00:32.263246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3069754)\u001b[0m 2023-11-24 16:00:36.468520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3069754)\u001b[0m 2023-11-24 16:00:38.462866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3069754)\u001b[0m 2023-11-24 16:00:41.364047: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14ecbd6225f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3069754)\u001b[0m 2023-11-24 16:00:41.364065: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3069754)\u001b[0m 2023-11-24 16:00:41.458865: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3069754)\u001b[0m 2023-11-24 16:00:41.603890: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th style=\"text-align: right;\">  mean_accuracy</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_7b9c5_00000</td><td style=\"text-align: right;\">         0.9864</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00001</td><td style=\"text-align: right;\">         0.9864</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00002</td><td style=\"text-align: right;\">         0.9873</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00003</td><td style=\"text-align: right;\">         0.9876</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00004</td><td style=\"text-align: right;\">         0.9846</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00005</td><td style=\"text-align: right;\">         0.9872</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00006</td><td style=\"text-align: right;\">         0.9855</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00007</td><td style=\"text-align: right;\">         0.9859</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00008</td><td style=\"text-align: right;\">         0.9844</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00009</td><td style=\"text-align: right;\">         0.9894</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00010</td><td style=\"text-align: right;\">         0.9881</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00011</td><td style=\"text-align: right;\">         0.9884</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00012</td><td style=\"text-align: right;\">         0.9876</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00013</td><td style=\"text-align: right;\">         0.9903</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00014</td><td style=\"text-align: right;\">         0.988 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00015</td><td style=\"text-align: right;\">         0.9904</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00016</td><td style=\"text-align: right;\">         0.9888</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00017</td><td style=\"text-align: right;\">         0.9873</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00018</td><td style=\"text-align: right;\">         0.9895</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00019</td><td style=\"text-align: right;\">         0.9871</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00020</td><td style=\"text-align: right;\">         0.9873</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00021</td><td style=\"text-align: right;\">         0.9898</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00022</td><td style=\"text-align: right;\">         0.9888</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00023</td><td style=\"text-align: right;\">         0.9887</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00024</td><td style=\"text-align: right;\">         0.989 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00025</td><td style=\"text-align: right;\">         0.9894</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00026</td><td style=\"text-align: right;\">         0.9888</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00027</td><td style=\"text-align: right;\">         0.987 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00028</td><td style=\"text-align: right;\">         0.9881</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00029</td><td style=\"text-align: right;\">         0.9862</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00030</td><td style=\"text-align: right;\">         0.9876</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00031</td><td style=\"text-align: right;\">         0.9867</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00032</td><td style=\"text-align: right;\">         0.9875</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00033</td><td style=\"text-align: right;\">         0.9878</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00034</td><td style=\"text-align: right;\">         0.9876</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00035</td><td style=\"text-align: right;\">         0.9863</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00036</td><td style=\"text-align: right;\">         0.9842</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00037</td><td style=\"text-align: right;\">         0.9813</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00038</td><td style=\"text-align: right;\">         0.9806</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00039</td><td style=\"text-align: right;\">         0.9844</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00040</td><td style=\"text-align: right;\">         0.9845</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00041</td><td style=\"text-align: right;\">         0.983 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00042</td><td style=\"text-align: right;\">         0.9839</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00043</td><td style=\"text-align: right;\">         0.984 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00044</td><td style=\"text-align: right;\">         0.9839</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00045</td><td style=\"text-align: right;\">         0.9808</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00046</td><td style=\"text-align: right;\">         0.9826</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00047</td><td style=\"text-align: right;\">         0.9819</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00048</td><td style=\"text-align: right;\">         0.9827</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00049</td><td style=\"text-align: right;\">         0.9816</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00050</td><td style=\"text-align: right;\">         0.9809</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00051</td><td style=\"text-align: right;\">         0.9817</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00052</td><td style=\"text-align: right;\">         0.9827</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00053</td><td style=\"text-align: right;\">         0.9832</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00054</td><td style=\"text-align: right;\">         0.9825</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00055</td><td style=\"text-align: right;\">         0.9865</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00056</td><td style=\"text-align: right;\">         0.9889</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00057</td><td style=\"text-align: right;\">         0.9841</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00058</td><td style=\"text-align: right;\">         0.9863</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00059</td><td style=\"text-align: right;\">         0.9871</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00060</td><td style=\"text-align: right;\">         0.9824</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00061</td><td style=\"text-align: right;\">         0.9815</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00062</td><td style=\"text-align: right;\">         0.9863</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00063</td><td style=\"text-align: right;\">         0.9834</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00064</td><td style=\"text-align: right;\">         0.9836</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00065</td><td style=\"text-align: right;\">         0.9875</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00066</td><td style=\"text-align: right;\">         0.9807</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00067</td><td style=\"text-align: right;\">         0.9826</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00068</td><td style=\"text-align: right;\">         0.9877</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00069</td><td style=\"text-align: right;\">         0.9828</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00070</td><td style=\"text-align: right;\">         0.9848</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00071</td><td style=\"text-align: right;\">         0.9847</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00072</td><td style=\"text-align: right;\">         0.977 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00073</td><td style=\"text-align: right;\">         0.9815</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00074</td><td style=\"text-align: right;\">         0.9834</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00075</td><td style=\"text-align: right;\">         0.9758</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00076</td><td style=\"text-align: right;\">         0.9785</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00077</td><td style=\"text-align: right;\">         0.9837</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00078</td><td style=\"text-align: right;\">         0.9765</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00079</td><td style=\"text-align: right;\">         0.9818</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00080</td><td style=\"text-align: right;\">         0.9837</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00081</td><td style=\"text-align: right;\">         0.8784</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00082</td><td style=\"text-align: right;\">         0.9729</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00083</td><td style=\"text-align: right;\">         0.9756</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00084</td><td style=\"text-align: right;\">         0.9199</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00085</td><td style=\"text-align: right;\">         0.94  </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00086</td><td style=\"text-align: right;\">         0.9691</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00087</td><td style=\"text-align: right;\">         0.8925</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00088</td><td style=\"text-align: right;\">         0.8906</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00089</td><td style=\"text-align: right;\">         0.8941</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00090</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00091</td><td style=\"text-align: right;\">         0.101 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00092</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00093</td><td style=\"text-align: right;\">         0.8556</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00094</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00095</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00096</td><td style=\"text-align: right;\">         0.1028</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00097</td><td style=\"text-align: right;\">         0.101 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00098</td><td style=\"text-align: right;\">         0.9379</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00099</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00100</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00101</td><td style=\"text-align: right;\">         0.9204</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00102</td><td style=\"text-align: right;\">         0.1028</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00103</td><td style=\"text-align: right;\">         0.8883</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00104</td><td style=\"text-align: right;\">         0.9181</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00105</td><td style=\"text-align: right;\">         0.101 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00106</td><td style=\"text-align: right;\">         0.8992</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00107</td><td style=\"text-align: right;\">         0.9248</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00108</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00109</td><td style=\"text-align: right;\">         0.0958</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00110</td><td style=\"text-align: right;\">         0.0974</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00111</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00112</td><td style=\"text-align: right;\">         0.101 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00113</td><td style=\"text-align: right;\">         0.1028</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00114</td><td style=\"text-align: right;\">         0.0974</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00115</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00116</td><td style=\"text-align: right;\">         0.0958</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00117</td><td style=\"text-align: right;\">         0.0974</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00118</td><td style=\"text-align: right;\">         0.0998</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00119</td><td style=\"text-align: right;\">         0.098 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00120</td><td style=\"text-align: right;\">         0.1028</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00121</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00122</td><td style=\"text-align: right;\">         0.1032</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00123</td><td style=\"text-align: right;\">         0.1028</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00124</td><td style=\"text-align: right;\">         0.1028</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00125</td><td style=\"text-align: right;\">         0.101 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00126</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00127</td><td style=\"text-align: right;\">         0.0958</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00128</td><td style=\"text-align: right;\">         0.1028</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00129</td><td style=\"text-align: right;\">         0.0958</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00130</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00131</td><td style=\"text-align: right;\">         0.5883</td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00132</td><td style=\"text-align: right;\">         0.098 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00133</td><td style=\"text-align: right;\">         0.098 </td></tr>\n",
              "<tr><td>train_mnist_7b9c5_00134</td><td style=\"text-align: right;\">         0.098 </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3070976)\u001b[0m 2023-11-24 16:01:12.884145: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3070976)\u001b[0m 2023-11-24 16:01:12.930691: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3070976)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3070976)\u001b[0m 2023-11-24 16:01:13.562473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3070976)\u001b[0m 2023-11-24 16:01:14.925942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3070976)\u001b[0m 2023-11-24 16:01:16.344447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3070976)\u001b[0m 2023-11-24 16:01:16.540805: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14e6d405d010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3070976)\u001b[0m 2023-11-24 16:01:16.540834: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3070976)\u001b[0m 2023-11-24 16:01:16.545095: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3070976)\u001b[0m 2023-11-24 16:01:16.658393: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3071872)\u001b[0m 2023-11-24 16:01:39.871829: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3071872)\u001b[0m 2023-11-24 16:01:39.918675: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3071872)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3071872)\u001b[0m 2023-11-24 16:01:40.547450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3071872)\u001b[0m 2023-11-24 16:01:41.908342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3071872)\u001b[0m 2023-11-24 16:01:43.342166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3071872)\u001b[0m 2023-11-24 16:01:43.576936: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14fee562d310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3071872)\u001b[0m 2023-11-24 16:01:43.576966: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3071872)\u001b[0m 2023-11-24 16:01:43.581395: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3071872)\u001b[0m 2023-11-24 16:01:43.694932: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3072733)\u001b[0m 2023-11-24 16:02:02.905417: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3072733)\u001b[0m 2023-11-24 16:02:02.953399: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3072733)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3072733)\u001b[0m 2023-11-24 16:02:03.611862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3072733)\u001b[0m 2023-11-24 16:02:05.005031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3072733)\u001b[0m 2023-11-24 16:02:06.436397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3072733)\u001b[0m 2023-11-24 16:02:06.636898: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15030006c100 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3072733)\u001b[0m 2023-11-24 16:02:06.636927: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3072733)\u001b[0m 2023-11-24 16:02:06.641117: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3072733)\u001b[0m 2023-11-24 16:02:06.755412: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3073722)\u001b[0m 2023-11-24 16:02:39.892449: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3073722)\u001b[0m 2023-11-24 16:02:39.939516: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3073722)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3073722)\u001b[0m 2023-11-24 16:02:40.569702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3073722)\u001b[0m 2023-11-24 16:02:41.936935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3073722)\u001b[0m 2023-11-24 16:02:43.353177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3073722)\u001b[0m 2023-11-24 16:02:43.554432: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14f6a8058a10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3073722)\u001b[0m 2023-11-24 16:02:43.554461: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3073722)\u001b[0m 2023-11-24 16:02:43.558721: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3073722)\u001b[0m 2023-11-24 16:02:43.672888: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3074964)\u001b[0m 2023-11-24 16:03:08.954892: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3074964)\u001b[0m 2023-11-24 16:03:09.001311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3074964)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3074964)\u001b[0m 2023-11-24 16:03:09.634654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3074964)\u001b[0m 2023-11-24 16:03:11.069729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3074964)\u001b[0m 2023-11-24 16:03:12.484515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3074964)\u001b[0m 2023-11-24 16:03:12.686784: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x144338059050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3074964)\u001b[0m 2023-11-24 16:03:12.686809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3074964)\u001b[0m 2023-11-24 16:03:12.691164: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3074964)\u001b[0m 2023-11-24 16:03:12.804567: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3076055)\u001b[0m 2023-11-24 16:03:34.904376: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3076055)\u001b[0m 2023-11-24 16:03:34.950831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3076055)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3076055)\u001b[0m 2023-11-24 16:03:35.593968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3076055)\u001b[0m 2023-11-24 16:03:37.009016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3076055)\u001b[0m 2023-11-24 16:03:38.426454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3076055)\u001b[0m 2023-11-24 16:03:38.623112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x147b1806b210 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3076055)\u001b[0m 2023-11-24 16:03:38.623135: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3076055)\u001b[0m 2023-11-24 16:03:38.627497: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3076055)\u001b[0m 2023-11-24 16:03:38.739755: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3077305)\u001b[0m 2023-11-24 16:04:20.982694: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3077305)\u001b[0m 2023-11-24 16:04:21.030410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3077305)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3077305)\u001b[0m 2023-11-24 16:04:21.687857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3077305)\u001b[0m 2023-11-24 16:04:23.061633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30649 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3077305)\u001b[0m 2023-11-24 16:04:24.510592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3077305)\u001b[0m 2023-11-24 16:04:24.728749: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x145d14058810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3077305)\u001b[0m 2023-11-24 16:04:24.728769: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3077305)\u001b[0m 2023-11-24 16:04:24.733225: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3077305)\u001b[0m 2023-11-24 16:04:24.849657: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3078805)\u001b[0m 2023-11-24 16:05:04.028984: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3078805)\u001b[0m 2023-11-24 16:05:04.076081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3078805)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3078805)\u001b[0m 2023-11-24 16:05:04.724437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3078805)\u001b[0m 2023-11-24 16:05:06.141972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30637 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3078805)\u001b[0m 2023-11-24 16:05:07.553886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3078805)\u001b[0m 2023-11-24 16:05:07.778582: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x149c8178a890 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3078805)\u001b[0m 2023-11-24 16:05:07.778611: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3078805)\u001b[0m 2023-11-24 16:05:07.783228: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3078805)\u001b[0m 2023-11-24 16:05:07.898827: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3080465)\u001b[0m 2023-11-24 16:06:28.755970: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3080465)\u001b[0m 2023-11-24 16:06:28.803743: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3080465)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3080465)\u001b[0m 2023-11-24 16:06:29.482933: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3080465)\u001b[0m 2023-11-24 16:06:33.288835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3080465)\u001b[0m 2023-11-24 16:06:36.812217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3080465)\u001b[0m 2023-11-24 16:06:38.668858: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14cecde48230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3080465)\u001b[0m 2023-11-24 16:06:38.668883: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3080465)\u001b[0m 2023-11-24 16:06:38.673411: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3080465)\u001b[0m 2023-11-24 16:06:40.842997: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3082482)\u001b[0m 2023-11-24 16:08:26.912622: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3082482)\u001b[0m 2023-11-24 16:08:26.961424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3082482)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3082482)\u001b[0m 2023-11-24 16:08:27.635838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3082482)\u001b[0m 2023-11-24 16:08:29.203129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3082482)\u001b[0m 2023-11-24 16:08:31.227924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3082482)\u001b[0m 2023-11-24 16:08:31.677907: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x145d8004de40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3082482)\u001b[0m 2023-11-24 16:08:31.677946: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3082482)\u001b[0m 2023-11-24 16:08:31.682730: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3082482)\u001b[0m 2023-11-24 16:08:32.094039: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3083529)\u001b[0m 2023-11-24 16:09:22.375519: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3083529)\u001b[0m 2023-11-24 16:09:22.423311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3083529)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3083529)\u001b[0m 2023-11-24 16:09:23.087076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3083529)\u001b[0m 2023-11-24 16:09:26.036707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3083529)\u001b[0m 2023-11-24 16:09:30.824713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3083529)\u001b[0m 2023-11-24 16:09:33.010854: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1533ec056660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3083529)\u001b[0m 2023-11-24 16:09:33.010885: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3083529)\u001b[0m 2023-11-24 16:09:33.015337: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3083529)\u001b[0m 2023-11-24 16:09:34.660588: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3086299)\u001b[0m 2023-11-24 16:11:02.314984: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3086299)\u001b[0m 2023-11-24 16:11:02.363497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3086299)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3086299)\u001b[0m 2023-11-24 16:11:03.027991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3086299)\u001b[0m 2023-11-24 16:11:09.778658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3086299)\u001b[0m 2023-11-24 16:11:18.898449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3086299)\u001b[0m 2023-11-24 16:11:19.559960: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x143260059230 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3086299)\u001b[0m 2023-11-24 16:11:19.559992: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3086299)\u001b[0m 2023-11-24 16:11:19.564561: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3086299)\u001b[0m 2023-11-24 16:11:22.739388: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3088282)\u001b[0m 2023-11-24 16:13:25.038563: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3088282)\u001b[0m 2023-11-24 16:13:25.085776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3088282)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3088282)\u001b[0m 2023-11-24 16:13:25.995691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3088282)\u001b[0m 2023-11-24 16:13:29.600238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3088282)\u001b[0m 2023-11-24 16:13:34.566004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3088282)\u001b[0m 2023-11-24 16:13:35.644203: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14475c06b690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3088282)\u001b[0m 2023-11-24 16:13:35.644231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3088282)\u001b[0m 2023-11-24 16:13:35.648746: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3088282)\u001b[0m 2023-11-24 16:13:37.613136: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3089514)\u001b[0m 2023-11-24 16:14:36.509152: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3089514)\u001b[0m 2023-11-24 16:14:36.557929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3089514)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3089514)\u001b[0m 2023-11-24 16:14:37.213967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3089514)\u001b[0m 2023-11-24 16:14:41.053694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3089514)\u001b[0m 2023-11-24 16:14:43.020430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3089514)\u001b[0m 2023-11-24 16:14:43.267268: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146a9805b190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3089514)\u001b[0m 2023-11-24 16:14:43.267294: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3089514)\u001b[0m 2023-11-24 16:14:43.272034: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3089514)\u001b[0m 2023-11-24 16:14:43.385167: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3090635)\u001b[0m 2023-11-24 16:15:39.676492: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3090635)\u001b[0m 2023-11-24 16:15:39.724482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3090635)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3090635)\u001b[0m 2023-11-24 16:15:40.367237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3090635)\u001b[0m 2023-11-24 16:15:41.749473: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3090635)\u001b[0m 2023-11-24 16:15:44.039242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3090635)\u001b[0m 2023-11-24 16:15:44.431973: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1473cc058bb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3090635)\u001b[0m 2023-11-24 16:15:44.431999: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3090635)\u001b[0m 2023-11-24 16:15:44.436287: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3090635)\u001b[0m 2023-11-24 16:15:45.296001: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3093221)\u001b[0m 2023-11-24 16:18:48.870908: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3093221)\u001b[0m 2023-11-24 16:18:48.919063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3093221)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3093221)\u001b[0m 2023-11-24 16:18:49.597563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3093221)\u001b[0m 2023-11-24 16:18:53.275655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3093221)\u001b[0m 2023-11-24 16:18:58.205229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3093221)\u001b[0m 2023-11-24 16:18:58.782956: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14486c059510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3093221)\u001b[0m 2023-11-24 16:18:58.782977: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3093221)\u001b[0m 2023-11-24 16:18:58.787741: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3093221)\u001b[0m 2023-11-24 16:19:01.781462: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3095460)\u001b[0m 2023-11-24 16:21:59.691320: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3095460)\u001b[0m 2023-11-24 16:21:59.739882: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3095460)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3095460)\u001b[0m 2023-11-24 16:22:00.510962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3095460)\u001b[0m 2023-11-24 16:22:05.510468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3095460)\u001b[0m 2023-11-24 16:22:15.329292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3095460)\u001b[0m 2023-11-24 16:22:19.476998: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x150b0c059870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3095460)\u001b[0m 2023-11-24 16:22:19.477034: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3095460)\u001b[0m 2023-11-24 16:22:19.481418: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3095460)\u001b[0m 2023-11-24 16:22:22.207321: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3096973)\u001b[0m 2023-11-24 16:24:13.056414: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3096973)\u001b[0m 2023-11-24 16:24:13.104213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3096973)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3096973)\u001b[0m 2023-11-24 16:24:13.746837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3096973)\u001b[0m 2023-11-24 16:24:15.844125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3096973)\u001b[0m 2023-11-24 16:24:17.507352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3096973)\u001b[0m 2023-11-24 16:24:17.919084: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14a67406afa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3096973)\u001b[0m 2023-11-24 16:24:17.919111: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3096973)\u001b[0m 2023-11-24 16:24:17.923590: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3096973)\u001b[0m 2023-11-24 16:24:18.116051: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3098087)\u001b[0m 2023-11-24 16:25:21.536880: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3098087)\u001b[0m 2023-11-24 16:25:21.584824: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3098087)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3098087)\u001b[0m 2023-11-24 16:25:22.224711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3098087)\u001b[0m 2023-11-24 16:25:23.985693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3098087)\u001b[0m 2023-11-24 16:25:25.517300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3098087)\u001b[0m 2023-11-24 16:25:26.780948: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14e5bc059690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3098087)\u001b[0m 2023-11-24 16:25:26.780977: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3098087)\u001b[0m 2023-11-24 16:25:26.785398: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3098087)\u001b[0m 2023-11-24 16:25:27.015532: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3099218)\u001b[0m 2023-11-24 16:26:19.040888: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3099218)\u001b[0m 2023-11-24 16:26:19.088171: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3099218)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3099218)\u001b[0m 2023-11-24 16:26:19.744226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3099218)\u001b[0m 2023-11-24 16:26:21.120261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3099218)\u001b[0m 2023-11-24 16:26:22.576438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3099218)\u001b[0m 2023-11-24 16:26:22.778471: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14e2f805cbd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3099218)\u001b[0m 2023-11-24 16:26:22.778494: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3099218)\u001b[0m 2023-11-24 16:26:22.782878: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3099218)\u001b[0m 2023-11-24 16:26:22.896812: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3100150)\u001b[0m 2023-11-24 16:26:42.000209: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3100150)\u001b[0m 2023-11-24 16:26:42.047522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3100150)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3100150)\u001b[0m 2023-11-24 16:26:42.680251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3100150)\u001b[0m 2023-11-24 16:26:44.050797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3100150)\u001b[0m 2023-11-24 16:26:45.488773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3100150)\u001b[0m 2023-11-24 16:26:45.685987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14af1c06efa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3100150)\u001b[0m 2023-11-24 16:26:45.686011: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3100150)\u001b[0m 2023-11-24 16:26:45.690123: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3100150)\u001b[0m 2023-11-24 16:26:45.802859: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3101117)\u001b[0m 2023-11-24 16:27:19.036823: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3101117)\u001b[0m 2023-11-24 16:27:19.084041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3101117)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3101117)\u001b[0m 2023-11-24 16:27:19.735336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3101117)\u001b[0m 2023-11-24 16:27:21.107297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3101117)\u001b[0m 2023-11-24 16:27:22.538007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3101117)\u001b[0m 2023-11-24 16:27:22.735681: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146abc058c70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3101117)\u001b[0m 2023-11-24 16:27:22.735703: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3101117)\u001b[0m 2023-11-24 16:27:22.739873: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3101117)\u001b[0m 2023-11-24 16:27:22.852966: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3101993)\u001b[0m 2023-11-24 16:27:49.015092: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3101993)\u001b[0m 2023-11-24 16:27:49.062021: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3101993)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3101993)\u001b[0m 2023-11-24 16:27:49.696269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3101993)\u001b[0m 2023-11-24 16:27:51.052036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3101993)\u001b[0m 2023-11-24 16:27:52.489435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3101993)\u001b[0m 2023-11-24 16:27:52.691273: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14bd2962f580 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3101993)\u001b[0m 2023-11-24 16:27:52.691297: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3101993)\u001b[0m 2023-11-24 16:27:52.695529: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3101993)\u001b[0m 2023-11-24 16:27:52.808087: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3102887)\u001b[0m 2023-11-24 16:28:15.012214: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3102887)\u001b[0m 2023-11-24 16:28:15.059394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3102887)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3102887)\u001b[0m 2023-11-24 16:28:15.692150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3102887)\u001b[0m 2023-11-24 16:28:17.040528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3102887)\u001b[0m 2023-11-24 16:28:18.480775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3102887)\u001b[0m 2023-11-24 16:28:18.684566: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1445ac06c470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3102887)\u001b[0m 2023-11-24 16:28:18.684591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3102887)\u001b[0m 2023-11-24 16:28:18.688858: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3102887)\u001b[0m 2023-11-24 16:28:18.802999: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3104084)\u001b[0m 2023-11-24 16:29:00.038176: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3104084)\u001b[0m 2023-11-24 16:29:00.086242: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3104084)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3104084)\u001b[0m 2023-11-24 16:29:00.741143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3104084)\u001b[0m 2023-11-24 16:29:02.153615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3104084)\u001b[0m 2023-11-24 16:29:03.604471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3104084)\u001b[0m 2023-11-24 16:29:03.815426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b118058c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3104084)\u001b[0m 2023-11-24 16:29:03.815455: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3104084)\u001b[0m 2023-11-24 16:29:03.819838: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3104084)\u001b[0m 2023-11-24 16:29:03.934450: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3105018)\u001b[0m 2023-11-24 16:29:36.045828: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3105018)\u001b[0m 2023-11-24 16:29:36.093360: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3105018)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3105018)\u001b[0m 2023-11-24 16:29:36.728826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3105018)\u001b[0m 2023-11-24 16:29:39.015789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3105018)\u001b[0m 2023-11-24 16:29:40.449738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3105018)\u001b[0m 2023-11-24 16:29:40.664220: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1440e805e1a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3105018)\u001b[0m 2023-11-24 16:29:40.664243: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3105018)\u001b[0m 2023-11-24 16:29:40.668468: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3105018)\u001b[0m 2023-11-24 16:29:40.781294: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3105949)\u001b[0m 2023-11-24 16:30:09.127594: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3105949)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3105949)\u001b[0m 2023-11-24 16:30:09.079975: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3105949)\u001b[0m 2023-11-24 16:30:09.765265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3105949)\u001b[0m 2023-11-24 16:30:11.140085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3105949)\u001b[0m 2023-11-24 16:30:12.564326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3105949)\u001b[0m 2023-11-24 16:30:12.758372: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1531fc06c080 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3105949)\u001b[0m 2023-11-24 16:30:12.758395: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3105949)\u001b[0m 2023-11-24 16:30:12.762712: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3105949)\u001b[0m 2023-11-24 16:30:12.875259: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3106873)\u001b[0m 2023-11-24 16:30:43.045809: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3106873)\u001b[0m 2023-11-24 16:30:43.092657: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3106873)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3106873)\u001b[0m 2023-11-24 16:30:43.722360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3106873)\u001b[0m 2023-11-24 16:30:45.096606: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3106873)\u001b[0m 2023-11-24 16:30:46.530074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3106873)\u001b[0m 2023-11-24 16:30:46.724035: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x148e3c059ea0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3106873)\u001b[0m 2023-11-24 16:30:46.724058: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3106873)\u001b[0m 2023-11-24 16:30:46.728451: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3106873)\u001b[0m 2023-11-24 16:30:46.844666: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3107769)\u001b[0m 2023-11-24 16:31:10.066157: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3107769)\u001b[0m 2023-11-24 16:31:10.112791: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3107769)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3107769)\u001b[0m 2023-11-24 16:31:10.748170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3107769)\u001b[0m 2023-11-24 16:31:12.138467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3107769)\u001b[0m 2023-11-24 16:31:13.603481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3107769)\u001b[0m 2023-11-24 16:31:13.801737: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14ed4978ae50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3107769)\u001b[0m 2023-11-24 16:31:13.801765: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3107769)\u001b[0m 2023-11-24 16:31:13.805953: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3107769)\u001b[0m 2023-11-24 16:31:13.918637: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3108670)\u001b[0m 2023-11-24 16:31:33.097073: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3108670)\u001b[0m 2023-11-24 16:31:33.144902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3108670)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3108670)\u001b[0m 2023-11-24 16:31:33.804500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3108670)\u001b[0m 2023-11-24 16:31:35.221104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3108670)\u001b[0m 2023-11-24 16:31:36.663339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3108670)\u001b[0m 2023-11-24 16:31:36.860613: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x145f8c058740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3108670)\u001b[0m 2023-11-24 16:31:36.860641: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3108670)\u001b[0m 2023-11-24 16:31:36.864919: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3108670)\u001b[0m 2023-11-24 16:31:36.977845: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3109566)\u001b[0m 2023-11-24 16:32:10.114187: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3109566)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3109566)\u001b[0m 2023-11-24 16:32:10.066901: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3109566)\u001b[0m 2023-11-24 16:32:10.745036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3109566)\u001b[0m 2023-11-24 16:32:12.099822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3109566)\u001b[0m 2023-11-24 16:32:13.525869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3109566)\u001b[0m 2023-11-24 16:32:13.723760: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1512b8058cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3109566)\u001b[0m 2023-11-24 16:32:13.723784: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3109566)\u001b[0m 2023-11-24 16:32:13.728192: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3109566)\u001b[0m 2023-11-24 16:32:13.841223: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3110561)\u001b[0m 2023-11-24 16:32:40.085341: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3110561)\u001b[0m 2023-11-24 16:32:40.132688: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3110561)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3110561)\u001b[0m 2023-11-24 16:32:40.764340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3110561)\u001b[0m 2023-11-24 16:32:42.121937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3110561)\u001b[0m 2023-11-24 16:32:43.545123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3110561)\u001b[0m 2023-11-24 16:32:43.746974: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1522c962b820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3110561)\u001b[0m 2023-11-24 16:32:43.747000: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3110561)\u001b[0m 2023-11-24 16:32:43.751200: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3110561)\u001b[0m 2023-11-24 16:32:43.864230: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3111453)\u001b[0m 2023-11-24 16:33:06.092394: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3111453)\u001b[0m 2023-11-24 16:33:06.139880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3111453)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3111453)\u001b[0m 2023-11-24 16:33:06.779218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3111453)\u001b[0m 2023-11-24 16:33:08.136601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3111453)\u001b[0m 2023-11-24 16:33:09.571891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3111453)\u001b[0m 2023-11-24 16:33:09.770455: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14e61d7985b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3111453)\u001b[0m 2023-11-24 16:33:09.770479: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3111453)\u001b[0m 2023-11-24 16:33:09.774872: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3111453)\u001b[0m 2023-11-24 16:33:09.887980: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3112407)\u001b[0m 2023-11-24 16:33:50.092787: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3112407)\u001b[0m 2023-11-24 16:33:50.140202: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3112407)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3112407)\u001b[0m 2023-11-24 16:33:50.777361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3112407)\u001b[0m 2023-11-24 16:33:52.140413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3112407)\u001b[0m 2023-11-24 16:33:53.582883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3112407)\u001b[0m 2023-11-24 16:33:53.786976: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14674406c8e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3112407)\u001b[0m 2023-11-24 16:33:53.787000: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3112407)\u001b[0m 2023-11-24 16:33:53.791381: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3112407)\u001b[0m 2023-11-24 16:33:53.904454: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3113376)\u001b[0m 2023-11-24 16:34:26.157532: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3113376)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3113376)\u001b[0m 2023-11-24 16:34:26.110300: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3113376)\u001b[0m 2023-11-24 16:34:26.796004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3113376)\u001b[0m 2023-11-24 16:34:28.159222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3113376)\u001b[0m 2023-11-24 16:34:29.605687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3113376)\u001b[0m 2023-11-24 16:34:29.822112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14694c05a5d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3113376)\u001b[0m 2023-11-24 16:34:29.822141: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3113376)\u001b[0m 2023-11-24 16:34:29.826501: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3113376)\u001b[0m 2023-11-24 16:34:29.939685: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3114268)\u001b[0m 2023-11-24 16:34:58.108881: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3114268)\u001b[0m 2023-11-24 16:34:58.156177: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3114268)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3114268)\u001b[0m 2023-11-24 16:34:58.796868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3114268)\u001b[0m 2023-11-24 16:35:00.167770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3114268)\u001b[0m 2023-11-24 16:35:01.618367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3114268)\u001b[0m 2023-11-24 16:35:01.819195: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x149f5162e300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3114268)\u001b[0m 2023-11-24 16:35:01.819227: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3114268)\u001b[0m 2023-11-24 16:35:01.823420: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3114268)\u001b[0m 2023-11-24 16:35:01.938464: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3115199)\u001b[0m 2023-11-24 16:35:33.110729: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3115199)\u001b[0m 2023-11-24 16:35:33.157945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3115199)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3115199)\u001b[0m 2023-11-24 16:35:33.797688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3115199)\u001b[0m 2023-11-24 16:35:35.161004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3115199)\u001b[0m 2023-11-24 16:35:36.586598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3115199)\u001b[0m 2023-11-24 16:35:36.781880: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1449cc06bbc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3115199)\u001b[0m 2023-11-24 16:35:36.781906: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3115199)\u001b[0m 2023-11-24 16:35:36.786273: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3115199)\u001b[0m 2023-11-24 16:35:36.899385: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3116108)\u001b[0m 2023-11-24 16:36:00.114246: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3116108)\u001b[0m 2023-11-24 16:36:00.161118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3116108)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3116108)\u001b[0m 2023-11-24 16:36:00.801977: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3116108)\u001b[0m 2023-11-24 16:36:02.177355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3116108)\u001b[0m 2023-11-24 16:36:03.621109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3116108)\u001b[0m 2023-11-24 16:36:03.820325: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14672c06b9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3116108)\u001b[0m 2023-11-24 16:36:03.820356: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3116108)\u001b[0m 2023-11-24 16:36:03.824754: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3116108)\u001b[0m 2023-11-24 16:36:03.939078: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3116973)\u001b[0m 2023-11-24 16:36:23.111733: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3116973)\u001b[0m 2023-11-24 16:36:23.159425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3116973)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3116973)\u001b[0m 2023-11-24 16:36:23.790302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3116973)\u001b[0m 2023-11-24 16:36:25.144923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3116973)\u001b[0m 2023-11-24 16:36:26.573855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3116973)\u001b[0m 2023-11-24 16:36:26.793883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x144b4c06a300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3116973)\u001b[0m 2023-11-24 16:36:26.793914: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3116973)\u001b[0m 2023-11-24 16:36:26.798299: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3116973)\u001b[0m 2023-11-24 16:36:26.912094: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3117939)\u001b[0m 2023-11-24 16:37:00.126589: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3117939)\u001b[0m 2023-11-24 16:37:00.173866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3117939)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3117939)\u001b[0m 2023-11-24 16:37:00.808489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3117939)\u001b[0m 2023-11-24 16:37:02.174564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3117939)\u001b[0m 2023-11-24 16:37:03.610500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3117939)\u001b[0m 2023-11-24 16:37:03.809312: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x149fd8058f30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3117939)\u001b[0m 2023-11-24 16:37:03.809340: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3117939)\u001b[0m 2023-11-24 16:37:03.813756: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3117939)\u001b[0m 2023-11-24 16:37:03.927683: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3118868)\u001b[0m 2023-11-24 16:37:30.180131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3118868)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3118868)\u001b[0m 2023-11-24 16:37:30.132710: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3118868)\u001b[0m 2023-11-24 16:37:30.836685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3118868)\u001b[0m 2023-11-24 16:37:32.227855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3118868)\u001b[0m 2023-11-24 16:37:33.670607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3118868)\u001b[0m 2023-11-24 16:37:33.880100: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x148935614890 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3118868)\u001b[0m 2023-11-24 16:37:33.880134: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3118868)\u001b[0m 2023-11-24 16:37:33.884406: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3118868)\u001b[0m 2023-11-24 16:37:33.999690: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3119768)\u001b[0m 2023-11-24 16:37:56.156829: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3119768)\u001b[0m 2023-11-24 16:37:56.203807: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3119768)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3119768)\u001b[0m 2023-11-24 16:37:56.862348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3119768)\u001b[0m 2023-11-24 16:37:58.247500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3119768)\u001b[0m 2023-11-24 16:37:59.688274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3119768)\u001b[0m 2023-11-24 16:37:59.887527: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15074c042690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3119768)\u001b[0m 2023-11-24 16:37:59.887555: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3119768)\u001b[0m 2023-11-24 16:37:59.891801: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3119768)\u001b[0m 2023-11-24 16:38:00.005573: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3120757)\u001b[0m 2023-11-24 16:38:40.160901: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3120757)\u001b[0m 2023-11-24 16:38:40.208391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3120757)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3120757)\u001b[0m 2023-11-24 16:38:40.848836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3120757)\u001b[0m 2023-11-24 16:38:42.224298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3120757)\u001b[0m 2023-11-24 16:38:43.640264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3120757)\u001b[0m 2023-11-24 16:38:43.847014: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b904059490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3120757)\u001b[0m 2023-11-24 16:38:43.847038: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3120757)\u001b[0m 2023-11-24 16:38:43.851412: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3120757)\u001b[0m 2023-11-24 16:38:43.964283: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3121665)\u001b[0m 2023-11-24 16:39:16.147868: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3121665)\u001b[0m 2023-11-24 16:39:16.195115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3121665)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3121665)\u001b[0m 2023-11-24 16:39:16.844440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3121665)\u001b[0m 2023-11-24 16:39:18.237601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3121665)\u001b[0m 2023-11-24 16:39:19.674594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3121665)\u001b[0m 2023-11-24 16:39:19.894633: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1439340700f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3121665)\u001b[0m 2023-11-24 16:39:19.894663: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3121665)\u001b[0m 2023-11-24 16:39:19.899027: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3121665)\u001b[0m 2023-11-24 16:39:20.013484: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3122623)\u001b[0m 2023-11-24 16:39:48.151626: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3122623)\u001b[0m 2023-11-24 16:39:48.199432: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3122623)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3122623)\u001b[0m 2023-11-24 16:39:48.834928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3122623)\u001b[0m 2023-11-24 16:39:50.202046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3122623)\u001b[0m 2023-11-24 16:39:51.609938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3122623)\u001b[0m 2023-11-24 16:39:51.806627: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x150d24056220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3122623)\u001b[0m 2023-11-24 16:39:51.806653: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3122623)\u001b[0m 2023-11-24 16:39:51.810887: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3122623)\u001b[0m 2023-11-24 16:39:51.923783: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3123514)\u001b[0m 2023-11-24 16:40:22.180479: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3123514)\u001b[0m 2023-11-24 16:40:22.227581: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3123514)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3123514)\u001b[0m 2023-11-24 16:40:22.855284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3123514)\u001b[0m 2023-11-24 16:40:24.222165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3123514)\u001b[0m 2023-11-24 16:40:25.654512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3123514)\u001b[0m 2023-11-24 16:40:25.853489: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14862c06c430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3123514)\u001b[0m 2023-11-24 16:40:25.853515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3123514)\u001b[0m 2023-11-24 16:40:25.857969: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3123514)\u001b[0m 2023-11-24 16:40:25.971295: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3124455)\u001b[0m 2023-11-24 16:40:49.176089: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3124455)\u001b[0m 2023-11-24 16:40:49.223300: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3124455)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3124455)\u001b[0m 2023-11-24 16:40:49.855394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3124455)\u001b[0m 2023-11-24 16:40:51.212469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3124455)\u001b[0m 2023-11-24 16:40:52.617292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3124455)\u001b[0m 2023-11-24 16:40:52.816203: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x149ee18fe090 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3124455)\u001b[0m 2023-11-24 16:40:52.816225: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3124455)\u001b[0m 2023-11-24 16:40:52.820565: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3124455)\u001b[0m 2023-11-24 16:40:52.933813: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3125329)\u001b[0m 2023-11-24 16:41:12.171206: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3125329)\u001b[0m 2023-11-24 16:41:12.218120: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3125329)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3125329)\u001b[0m 2023-11-24 16:41:12.851620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3125329)\u001b[0m 2023-11-24 16:41:14.207669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3125329)\u001b[0m 2023-11-24 16:41:15.602827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3125329)\u001b[0m 2023-11-24 16:41:15.798185: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14666406b500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3125329)\u001b[0m 2023-11-24 16:41:15.798205: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3125329)\u001b[0m 2023-11-24 16:41:15.802542: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3125329)\u001b[0m 2023-11-24 16:41:15.915030: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3126253)\u001b[0m 2023-11-24 16:41:48.236515: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3126253)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3126253)\u001b[0m 2023-11-24 16:41:48.189144: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3126253)\u001b[0m 2023-11-24 16:41:48.885457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3126253)\u001b[0m 2023-11-24 16:41:50.264866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3126253)\u001b[0m 2023-11-24 16:41:51.698282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3126253)\u001b[0m 2023-11-24 16:41:51.895961: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15246406c610 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3126253)\u001b[0m 2023-11-24 16:41:51.895987: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3126253)\u001b[0m 2023-11-24 16:41:51.900380: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3126253)\u001b[0m 2023-11-24 16:41:52.014030: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3127213)\u001b[0m 2023-11-24 16:42:18.190231: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3127213)\u001b[0m 2023-11-24 16:42:18.237319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3127213)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3127213)\u001b[0m 2023-11-24 16:42:18.872206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3127213)\u001b[0m 2023-11-24 16:42:20.231219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3127213)\u001b[0m 2023-11-24 16:42:21.654009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3127213)\u001b[0m 2023-11-24 16:42:21.856752: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14bf2c06c350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3127213)\u001b[0m 2023-11-24 16:42:21.856778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3127213)\u001b[0m 2023-11-24 16:42:21.861028: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3127213)\u001b[0m 2023-11-24 16:42:21.973694: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3128071)\u001b[0m 2023-11-24 16:42:44.222832: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3128071)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3128071)\u001b[0m 2023-11-24 16:42:44.175402: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3128071)\u001b[0m 2023-11-24 16:42:44.855947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3128071)\u001b[0m 2023-11-24 16:42:46.223842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3128071)\u001b[0m 2023-11-24 16:42:47.627930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3128071)\u001b[0m 2023-11-24 16:42:47.828537: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15255c057dd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3128071)\u001b[0m 2023-11-24 16:42:47.828562: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3128071)\u001b[0m 2023-11-24 16:42:47.832741: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3128071)\u001b[0m 2023-11-24 16:42:47.945341: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3129072)\u001b[0m 2023-11-24 16:43:28.273903: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3129072)\u001b[0m 2023-11-24 16:43:28.321266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3129072)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3129072)\u001b[0m 2023-11-24 16:43:28.959283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3129072)\u001b[0m 2023-11-24 16:43:30.323800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3129072)\u001b[0m 2023-11-24 16:43:31.761498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3129072)\u001b[0m 2023-11-24 16:43:31.970378: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14640006c670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3129072)\u001b[0m 2023-11-24 16:43:31.970404: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3129072)\u001b[0m 2023-11-24 16:43:31.974781: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3129072)\u001b[0m 2023-11-24 16:43:32.087722: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3130024)\u001b[0m 2023-11-24 16:44:04.213126: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3130024)\u001b[0m 2023-11-24 16:44:04.260684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3130024)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3130024)\u001b[0m 2023-11-24 16:44:04.901313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3130024)\u001b[0m 2023-11-24 16:44:06.274720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3130024)\u001b[0m 2023-11-24 16:44:07.702153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3130024)\u001b[0m 2023-11-24 16:44:07.914644: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1520c0058450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3130024)\u001b[0m 2023-11-24 16:44:07.914669: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3130024)\u001b[0m 2023-11-24 16:44:07.919089: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3130024)\u001b[0m 2023-11-24 16:44:08.031950: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3130915)\u001b[0m 2023-11-24 16:44:36.231648: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3130915)\u001b[0m 2023-11-24 16:44:36.278413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3130915)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3130915)\u001b[0m 2023-11-24 16:44:36.912687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3130915)\u001b[0m 2023-11-24 16:44:38.299186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3130915)\u001b[0m 2023-11-24 16:44:39.746770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3130915)\u001b[0m 2023-11-24 16:44:39.940791: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14d971613be0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3130915)\u001b[0m 2023-11-24 16:44:39.940821: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3130915)\u001b[0m 2023-11-24 16:44:39.945065: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3130915)\u001b[0m 2023-11-24 16:44:40.057913: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3131848)\u001b[0m 2023-11-24 16:45:10.218581: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3131848)\u001b[0m 2023-11-24 16:45:10.265749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3131848)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3131848)\u001b[0m 2023-11-24 16:45:10.899782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3131848)\u001b[0m 2023-11-24 16:45:12.254600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3131848)\u001b[0m 2023-11-24 16:45:13.679574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3131848)\u001b[0m 2023-11-24 16:45:13.875037: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1514e4058ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3131848)\u001b[0m 2023-11-24 16:45:13.875061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3131848)\u001b[0m 2023-11-24 16:45:13.879435: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3131848)\u001b[0m 2023-11-24 16:45:13.992429: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3132748)\u001b[0m 2023-11-24 16:45:37.219976: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3132748)\u001b[0m 2023-11-24 16:45:37.267105: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3132748)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3132748)\u001b[0m 2023-11-24 16:45:37.897212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3132748)\u001b[0m 2023-11-24 16:45:39.290343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3132748)\u001b[0m 2023-11-24 16:45:40.732139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3132748)\u001b[0m 2023-11-24 16:45:40.935604: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14ecec06ab50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3132748)\u001b[0m 2023-11-24 16:45:40.935633: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3132748)\u001b[0m 2023-11-24 16:45:40.939998: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3132748)\u001b[0m 2023-11-24 16:45:41.054189: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3133628)\u001b[0m 2023-11-24 16:46:00.266016: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3133628)\u001b[0m 2023-11-24 16:46:00.312840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3133628)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3133628)\u001b[0m 2023-11-24 16:46:00.953061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3133628)\u001b[0m 2023-11-24 16:46:02.331702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3133628)\u001b[0m 2023-11-24 16:46:03.786380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3133628)\u001b[0m 2023-11-24 16:46:03.986611: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14bcd8058790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3133628)\u001b[0m 2023-11-24 16:46:03.986643: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3133628)\u001b[0m 2023-11-24 16:46:03.991152: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3133628)\u001b[0m 2023-11-24 16:46:04.108271: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3134587)\u001b[0m 2023-11-24 16:46:37.233994: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3134587)\u001b[0m 2023-11-24 16:46:37.281680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3134587)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3134587)\u001b[0m 2023-11-24 16:46:37.915293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3134587)\u001b[0m 2023-11-24 16:46:39.270194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3134587)\u001b[0m 2023-11-24 16:46:40.698193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3134587)\u001b[0m 2023-11-24 16:46:40.896595: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x145a68059730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3134587)\u001b[0m 2023-11-24 16:46:40.896618: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3134587)\u001b[0m 2023-11-24 16:46:40.900975: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3134587)\u001b[0m 2023-11-24 16:46:41.013522: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3135496)\u001b[0m 2023-11-24 16:47:07.279176: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3135496)\u001b[0m 2023-11-24 16:47:07.326075: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3135496)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3135496)\u001b[0m 2023-11-24 16:47:07.970552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3135496)\u001b[0m 2023-11-24 16:47:09.343755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3135496)\u001b[0m 2023-11-24 16:47:10.778569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3135496)\u001b[0m 2023-11-24 16:47:10.980336: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x147a78058b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3135496)\u001b[0m 2023-11-24 16:47:10.980361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3135496)\u001b[0m 2023-11-24 16:47:10.984621: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3135496)\u001b[0m 2023-11-24 16:47:11.097366: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3136438)\u001b[0m 2023-11-24 16:47:33.256645: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3136438)\u001b[0m 2023-11-24 16:47:33.303945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3136438)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3136438)\u001b[0m 2023-11-24 16:47:33.945178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3136438)\u001b[0m 2023-11-24 16:47:35.318546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3136438)\u001b[0m 2023-11-24 16:47:36.755695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3136438)\u001b[0m 2023-11-24 16:47:36.958534: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b0440425e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3136438)\u001b[0m 2023-11-24 16:47:36.958564: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3136438)\u001b[0m 2023-11-24 16:47:36.962824: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3136438)\u001b[0m 2023-11-24 16:47:37.076891: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3137391)\u001b[0m 2023-11-24 16:48:17.253433: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3137391)\u001b[0m 2023-11-24 16:48:17.301264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3137391)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3137391)\u001b[0m 2023-11-24 16:48:17.935655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3137391)\u001b[0m 2023-11-24 16:48:19.293677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3137391)\u001b[0m 2023-11-24 16:48:20.728515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3137391)\u001b[0m 2023-11-24 16:48:20.933586: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1488c4058c20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3137391)\u001b[0m 2023-11-24 16:48:20.933611: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3137391)\u001b[0m 2023-11-24 16:48:20.937867: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3137391)\u001b[0m 2023-11-24 16:48:21.050721: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3138342)\u001b[0m 2023-11-24 16:48:54.278840: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3138342)\u001b[0m 2023-11-24 16:48:54.325764: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3138342)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3138342)\u001b[0m 2023-11-24 16:48:54.965348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3138342)\u001b[0m 2023-11-24 16:48:56.311933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3138342)\u001b[0m 2023-11-24 16:48:57.749510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3138342)\u001b[0m 2023-11-24 16:48:57.962157: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14be2805af20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3138342)\u001b[0m 2023-11-24 16:48:57.962183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3138342)\u001b[0m 2023-11-24 16:48:57.966585: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3138342)\u001b[0m 2023-11-24 16:48:58.079619: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3139251)\u001b[0m 2023-11-24 16:49:26.259233: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3139251)\u001b[0m 2023-11-24 16:49:26.305946: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3139251)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3139251)\u001b[0m 2023-11-24 16:49:26.943258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3139251)\u001b[0m 2023-11-24 16:49:28.306143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3139251)\u001b[0m 2023-11-24 16:49:29.751327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3139251)\u001b[0m 2023-11-24 16:49:29.947758: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14cc2c06f9c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3139251)\u001b[0m 2023-11-24 16:49:29.947787: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3139251)\u001b[0m 2023-11-24 16:49:29.951953: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3139251)\u001b[0m 2023-11-24 16:49:30.065222: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3140201)\u001b[0m 2023-11-24 16:50:00.284324: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3140201)\u001b[0m 2023-11-24 16:50:00.331787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3140201)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3140201)\u001b[0m 2023-11-24 16:50:00.975953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3140201)\u001b[0m 2023-11-24 16:50:02.348798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3140201)\u001b[0m 2023-11-24 16:50:03.773881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3140201)\u001b[0m 2023-11-24 16:50:03.972676: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14787405a360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3140201)\u001b[0m 2023-11-24 16:50:03.972703: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3140201)\u001b[0m 2023-11-24 16:50:03.976858: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3140201)\u001b[0m 2023-11-24 16:50:04.090674: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3141077)\u001b[0m 2023-11-24 16:50:27.286281: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3141077)\u001b[0m 2023-11-24 16:50:27.333413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3141077)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3141077)\u001b[0m 2023-11-24 16:50:27.968107: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3141077)\u001b[0m 2023-11-24 16:50:29.339601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3141077)\u001b[0m 2023-11-24 16:50:30.780257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3141077)\u001b[0m 2023-11-24 16:50:30.978271: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14ce10059010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3141077)\u001b[0m 2023-11-24 16:50:30.978299: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3141077)\u001b[0m 2023-11-24 16:50:30.982524: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3141077)\u001b[0m 2023-11-24 16:50:31.095724: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3141966)\u001b[0m 2023-11-24 16:50:50.282453: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3141966)\u001b[0m 2023-11-24 16:50:50.329514: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3141966)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3141966)\u001b[0m 2023-11-24 16:50:50.961155: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3141966)\u001b[0m 2023-11-24 16:50:52.311137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3141966)\u001b[0m 2023-11-24 16:50:53.735869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3141966)\u001b[0m 2023-11-24 16:50:53.933485: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14414c05d0a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3141966)\u001b[0m 2023-11-24 16:50:53.933510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3141966)\u001b[0m 2023-11-24 16:50:53.937889: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3141966)\u001b[0m 2023-11-24 16:50:54.050740: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3142953)\u001b[0m 2023-11-24 16:51:26.286513: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3142953)\u001b[0m 2023-11-24 16:51:26.333763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3142953)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3142953)\u001b[0m 2023-11-24 16:51:26.972355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3142953)\u001b[0m 2023-11-24 16:51:28.336583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3142953)\u001b[0m 2023-11-24 16:51:29.786772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3142953)\u001b[0m 2023-11-24 16:51:29.986310: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x152690058f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3142953)\u001b[0m 2023-11-24 16:51:29.986342: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3142953)\u001b[0m 2023-11-24 16:51:29.990723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3142953)\u001b[0m 2023-11-24 16:51:30.104361: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3143812)\u001b[0m 2023-11-24 16:51:56.284689: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3143812)\u001b[0m 2023-11-24 16:51:56.332121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3143812)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3143812)\u001b[0m 2023-11-24 16:51:56.965799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3143812)\u001b[0m 2023-11-24 16:51:58.319892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3143812)\u001b[0m 2023-11-24 16:51:59.771476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3143812)\u001b[0m 2023-11-24 16:51:59.975515: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x145dc8058bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3143812)\u001b[0m 2023-11-24 16:51:59.975543: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3143812)\u001b[0m 2023-11-24 16:51:59.979962: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3143812)\u001b[0m 2023-11-24 16:52:00.093486: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3144741)\u001b[0m 2023-11-24 16:52:22.310962: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3144741)\u001b[0m 2023-11-24 16:52:22.358757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3144741)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3144741)\u001b[0m 2023-11-24 16:52:23.013393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3144741)\u001b[0m 2023-11-24 16:52:24.397101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3144741)\u001b[0m 2023-11-24 16:52:25.845680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3144741)\u001b[0m 2023-11-24 16:52:26.043736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14a700058ae0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3144741)\u001b[0m 2023-11-24 16:52:26.043765: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3144741)\u001b[0m 2023-11-24 16:52:26.047983: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3144741)\u001b[0m 2023-11-24 16:52:26.161015: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3145735)\u001b[0m 2023-11-24 16:53:06.346973: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3145735)\u001b[0m 2023-11-24 16:53:06.394041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3145735)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3145735)\u001b[0m 2023-11-24 16:53:07.057599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3145735)\u001b[0m 2023-11-24 16:53:08.423796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3145735)\u001b[0m 2023-11-24 16:53:09.856759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3145735)\u001b[0m 2023-11-24 16:53:10.059871: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b63c06abc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3145735)\u001b[0m 2023-11-24 16:53:10.059897: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3145735)\u001b[0m 2023-11-24 16:53:10.064248: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3145735)\u001b[0m 2023-11-24 16:53:10.176427: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3146643)\u001b[0m 2023-11-24 16:53:42.323707: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3146643)\u001b[0m 2023-11-24 16:53:42.370357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3146643)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3146643)\u001b[0m 2023-11-24 16:53:43.005897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3146643)\u001b[0m 2023-11-24 16:53:44.369508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3146643)\u001b[0m 2023-11-24 16:53:45.814858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3146643)\u001b[0m 2023-11-24 16:53:46.027112: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x147f30059790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3146643)\u001b[0m 2023-11-24 16:53:46.027143: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3146643)\u001b[0m 2023-11-24 16:53:46.031523: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3146643)\u001b[0m 2023-11-24 16:53:46.144346: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3147593)\u001b[0m 2023-11-24 16:54:14.315363: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3147593)\u001b[0m 2023-11-24 16:54:14.362753: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3147593)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3147593)\u001b[0m 2023-11-24 16:54:14.995984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3147593)\u001b[0m 2023-11-24 16:54:16.352650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3147593)\u001b[0m 2023-11-24 16:54:17.777867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3147593)\u001b[0m 2023-11-24 16:54:17.972574: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14ea6c058710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3147593)\u001b[0m 2023-11-24 16:54:17.972600: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3147593)\u001b[0m 2023-11-24 16:54:17.976983: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3147593)\u001b[0m 2023-11-24 16:54:18.089951: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3148512)\u001b[0m 2023-11-24 16:54:49.390243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3148512)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3148512)\u001b[0m 2023-11-24 16:54:49.343224: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3148512)\u001b[0m 2023-11-24 16:54:50.015675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3148512)\u001b[0m 2023-11-24 16:54:51.399096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3148512)\u001b[0m 2023-11-24 16:54:52.840508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3148512)\u001b[0m 2023-11-24 16:54:53.036227: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x152ff005dbe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3148512)\u001b[0m 2023-11-24 16:54:53.036253: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3148512)\u001b[0m 2023-11-24 16:54:53.040477: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3148512)\u001b[0m 2023-11-24 16:54:53.153850: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3149430)\u001b[0m 2023-11-24 16:55:16.341692: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3149430)\u001b[0m 2023-11-24 16:55:16.388754: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3149430)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3149430)\u001b[0m 2023-11-24 16:55:17.017727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3149430)\u001b[0m 2023-11-24 16:55:18.375099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3149430)\u001b[0m 2023-11-24 16:55:19.807292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3149430)\u001b[0m 2023-11-24 16:55:20.006034: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1470dc06bc30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3149430)\u001b[0m 2023-11-24 16:55:20.006057: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3149430)\u001b[0m 2023-11-24 16:55:20.010470: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3149430)\u001b[0m 2023-11-24 16:55:20.123088: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3150328)\u001b[0m 2023-11-24 16:55:39.342035: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3150328)\u001b[0m 2023-11-24 16:55:39.389287: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3150328)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3150328)\u001b[0m 2023-11-24 16:55:40.022884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3150328)\u001b[0m 2023-11-24 16:55:41.378369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3150328)\u001b[0m 2023-11-24 16:55:42.800316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3150328)\u001b[0m 2023-11-24 16:55:42.998289: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14767c06bff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3150328)\u001b[0m 2023-11-24 16:55:42.998316: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3150328)\u001b[0m 2023-11-24 16:55:43.002687: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3150328)\u001b[0m 2023-11-24 16:55:43.115352: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3151239)\u001b[0m 2023-11-24 16:56:16.387231: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3151239)\u001b[0m 2023-11-24 16:56:16.434039: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3151239)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3151239)\u001b[0m 2023-11-24 16:56:17.087373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3151239)\u001b[0m 2023-11-24 16:56:18.455980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3151239)\u001b[0m 2023-11-24 16:56:19.886715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3151239)\u001b[0m 2023-11-24 16:56:20.084050: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x150b0c070700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3151239)\u001b[0m 2023-11-24 16:56:20.084073: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3151239)\u001b[0m 2023-11-24 16:56:20.088506: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3151239)\u001b[0m 2023-11-24 16:56:20.201097: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3152164)\u001b[0m 2023-11-24 16:56:46.356171: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3152164)\u001b[0m 2023-11-24 16:56:46.402779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3152164)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3152164)\u001b[0m 2023-11-24 16:56:47.039927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3152164)\u001b[0m 2023-11-24 16:56:48.404461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3152164)\u001b[0m 2023-11-24 16:56:49.832627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3152164)\u001b[0m 2023-11-24 16:56:50.033168: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14f940058db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3152164)\u001b[0m 2023-11-24 16:56:50.033192: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3152164)\u001b[0m 2023-11-24 16:56:50.037314: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3152164)\u001b[0m 2023-11-24 16:56:50.150189: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3153021)\u001b[0m 2023-11-24 16:57:12.362633: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3153021)\u001b[0m 2023-11-24 16:57:12.409802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3153021)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3153021)\u001b[0m 2023-11-24 16:57:13.040029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3153021)\u001b[0m 2023-11-24 16:57:14.379052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3153021)\u001b[0m 2023-11-24 16:57:15.807640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3153021)\u001b[0m 2023-11-24 16:57:16.006793: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1482980664c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3153021)\u001b[0m 2023-11-24 16:57:16.006830: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3153021)\u001b[0m 2023-11-24 16:57:16.011175: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3153021)\u001b[0m 2023-11-24 16:57:16.126458: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3154049)\u001b[0m 2023-11-24 16:57:56.359383: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3154049)\u001b[0m 2023-11-24 16:57:56.406933: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3154049)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3154049)\u001b[0m 2023-11-24 16:57:57.042429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3154049)\u001b[0m 2023-11-24 16:57:58.408690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3154049)\u001b[0m 2023-11-24 16:57:59.854094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3154049)\u001b[0m 2023-11-24 16:58:00.060023: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x149bf8058fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3154049)\u001b[0m 2023-11-24 16:58:00.060050: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3154049)\u001b[0m 2023-11-24 16:58:00.064246: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3154049)\u001b[0m 2023-11-24 16:58:00.177007: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3155031)\u001b[0m 2023-11-24 16:58:33.385379: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3155031)\u001b[0m 2023-11-24 16:58:33.432699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3155031)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3155031)\u001b[0m 2023-11-24 16:58:34.071785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3155031)\u001b[0m 2023-11-24 16:58:35.437762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3155031)\u001b[0m 2023-11-24 16:58:36.879283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3155031)\u001b[0m 2023-11-24 16:58:37.095416: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x144c1d627ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3155031)\u001b[0m 2023-11-24 16:58:37.095440: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3155031)\u001b[0m 2023-11-24 16:58:37.099671: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3155031)\u001b[0m 2023-11-24 16:58:37.212568: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3155953)\u001b[0m 2023-11-24 16:59:05.424100: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3155953)\u001b[0m 2023-11-24 16:59:05.471588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3155953)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3155953)\u001b[0m 2023-11-24 16:59:06.130831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3155953)\u001b[0m 2023-11-24 16:59:07.532849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3155953)\u001b[0m 2023-11-24 16:59:08.963986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3155953)\u001b[0m 2023-11-24 16:59:09.158382: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146f440429b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3155953)\u001b[0m 2023-11-24 16:59:09.158408: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3155953)\u001b[0m 2023-11-24 16:59:09.162734: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3155953)\u001b[0m 2023-11-24 16:59:09.275415: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3156866)\u001b[0m 2023-11-24 16:59:39.391494: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3156866)\u001b[0m 2023-11-24 16:59:39.439214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3156866)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3156866)\u001b[0m 2023-11-24 16:59:40.072021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3156866)\u001b[0m 2023-11-24 16:59:41.431683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3156866)\u001b[0m 2023-11-24 16:59:42.869737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3156866)\u001b[0m 2023-11-24 16:59:43.068374: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14ce6005aa70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3156866)\u001b[0m 2023-11-24 16:59:43.068399: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3156866)\u001b[0m 2023-11-24 16:59:43.072625: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3156866)\u001b[0m 2023-11-24 16:59:43.185783: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3157760)\u001b[0m 2023-11-24 17:00:06.429066: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3157760)\u001b[0m 2023-11-24 17:00:06.475879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3157760)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3157760)\u001b[0m 2023-11-24 17:00:07.108082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3157760)\u001b[0m 2023-11-24 17:00:08.468112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3157760)\u001b[0m 2023-11-24 17:00:09.906544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3157760)\u001b[0m 2023-11-24 17:00:10.114688: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14452805d9a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3157760)\u001b[0m 2023-11-24 17:00:10.114713: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3157760)\u001b[0m 2023-11-24 17:00:10.118947: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3157760)\u001b[0m 2023-11-24 17:00:10.233462: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3158643)\u001b[0m 2023-11-24 17:00:29.433219: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3158643)\u001b[0m 2023-11-24 17:00:29.481090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3158643)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3158643)\u001b[0m 2023-11-24 17:00:30.142199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3158643)\u001b[0m 2023-11-24 17:00:31.558680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3158643)\u001b[0m 2023-11-24 17:00:33.006887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3158643)\u001b[0m 2023-11-24 17:00:33.209156: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14c46d62fba0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3158643)\u001b[0m 2023-11-24 17:00:33.209184: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3158643)\u001b[0m 2023-11-24 17:00:33.213556: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3158643)\u001b[0m 2023-11-24 17:00:33.326617: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3159611)\u001b[0m 2023-11-24 17:01:06.416016: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3159611)\u001b[0m 2023-11-24 17:01:06.463886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3159611)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3159611)\u001b[0m 2023-11-24 17:01:07.102753: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3159611)\u001b[0m 2023-11-24 17:01:08.462037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3159611)\u001b[0m 2023-11-24 17:01:09.907483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3159611)\u001b[0m 2023-11-24 17:01:10.106152: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14dc4005c3d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3159611)\u001b[0m 2023-11-24 17:01:10.106178: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3159611)\u001b[0m 2023-11-24 17:01:10.110394: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3159611)\u001b[0m 2023-11-24 17:01:10.223766: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3160565)\u001b[0m 2023-11-24 17:01:36.447001: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3160565)\u001b[0m 2023-11-24 17:01:36.494173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3160565)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3160565)\u001b[0m 2023-11-24 17:01:37.137303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3160565)\u001b[0m 2023-11-24 17:01:38.494451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3160565)\u001b[0m 2023-11-24 17:01:39.926678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3160565)\u001b[0m 2023-11-24 17:01:40.129018: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14e5a0059300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3160565)\u001b[0m 2023-11-24 17:01:40.129043: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3160565)\u001b[0m 2023-11-24 17:01:40.133218: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3160565)\u001b[0m 2023-11-24 17:01:40.245961: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3161416)\u001b[0m 2023-11-24 17:02:02.428856: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3161416)\u001b[0m 2023-11-24 17:02:02.476144: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3161416)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3161416)\u001b[0m 2023-11-24 17:02:03.113256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3161416)\u001b[0m 2023-11-24 17:02:04.479220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3161416)\u001b[0m 2023-11-24 17:02:05.905914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3161416)\u001b[0m 2023-11-24 17:02:06.112044: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1524980582c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3161416)\u001b[0m 2023-11-24 17:02:06.112070: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3161416)\u001b[0m 2023-11-24 17:02:06.116277: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3161416)\u001b[0m 2023-11-24 17:02:06.229612: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3162413)\u001b[0m 2023-11-24 17:02:46.433516: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3162413)\u001b[0m 2023-11-24 17:02:46.480559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3162413)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3162413)\u001b[0m 2023-11-24 17:02:47.116797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3162413)\u001b[0m 2023-11-24 17:02:48.479211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3162413)\u001b[0m 2023-11-24 17:02:49.919577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3162413)\u001b[0m 2023-11-24 17:02:50.140119: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14f6fc059520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3162413)\u001b[0m 2023-11-24 17:02:50.140143: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3162413)\u001b[0m 2023-11-24 17:02:50.144307: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3162413)\u001b[0m 2023-11-24 17:02:50.257386: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3163365)\u001b[0m 2023-11-24 17:03:22.454488: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3163365)\u001b[0m 2023-11-24 17:03:22.501676: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3163365)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3163365)\u001b[0m 2023-11-24 17:03:23.137414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3163365)\u001b[0m 2023-11-24 17:03:24.494787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3163365)\u001b[0m 2023-11-24 17:03:25.939873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3163365)\u001b[0m 2023-11-24 17:03:26.154604: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14a005641470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3163365)\u001b[0m 2023-11-24 17:03:26.154624: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3163365)\u001b[0m 2023-11-24 17:03:26.159005: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3163365)\u001b[0m 2023-11-24 17:03:26.271882: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3164270)\u001b[0m 2023-11-24 17:03:54.440955: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3164270)\u001b[0m 2023-11-24 17:03:54.488121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3164270)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3164270)\u001b[0m 2023-11-24 17:03:55.121781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3164270)\u001b[0m 2023-11-24 17:03:56.471102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3164270)\u001b[0m 2023-11-24 17:03:57.905132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3164270)\u001b[0m 2023-11-24 17:03:58.105654: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14bf84042a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3164270)\u001b[0m 2023-11-24 17:03:58.105682: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3164270)\u001b[0m 2023-11-24 17:03:58.109839: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3164270)\u001b[0m 2023-11-24 17:03:58.224255: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3165225)\u001b[0m 2023-11-24 17:04:28.445547: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3165225)\u001b[0m 2023-11-24 17:04:28.492550: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3165225)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3165225)\u001b[0m 2023-11-24 17:04:29.128402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3165225)\u001b[0m 2023-11-24 17:04:30.497233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3165225)\u001b[0m 2023-11-24 17:04:31.924135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3165225)\u001b[0m 2023-11-24 17:04:32.121480: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1444c006ba00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3165225)\u001b[0m 2023-11-24 17:04:32.121508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3165225)\u001b[0m 2023-11-24 17:04:32.125901: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3165225)\u001b[0m 2023-11-24 17:04:32.239608: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3166118)\u001b[0m 2023-11-24 17:04:55.451056: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3166118)\u001b[0m 2023-11-24 17:04:55.498379: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3166118)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3166118)\u001b[0m 2023-11-24 17:04:56.131318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3166118)\u001b[0m 2023-11-24 17:04:57.501791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3166118)\u001b[0m 2023-11-24 17:04:58.918003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3166118)\u001b[0m 2023-11-24 17:04:59.117250: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x147f7c0598b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3166118)\u001b[0m 2023-11-24 17:04:59.117278: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3166118)\u001b[0m 2023-11-24 17:04:59.121486: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3166118)\u001b[0m 2023-11-24 17:04:59.234599: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3166961)\u001b[0m 2023-11-24 17:05:18.454781: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3166961)\u001b[0m 2023-11-24 17:05:18.502077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3166961)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3166961)\u001b[0m 2023-11-24 17:05:19.135287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3166961)\u001b[0m 2023-11-24 17:05:20.502109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3166961)\u001b[0m 2023-11-24 17:05:21.920374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3166961)\u001b[0m 2023-11-24 17:05:22.116394: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1534f006b2a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3166961)\u001b[0m 2023-11-24 17:05:22.116415: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3166961)\u001b[0m 2023-11-24 17:05:22.120832: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3166961)\u001b[0m 2023-11-24 17:05:22.234704: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3167955)\u001b[0m 2023-11-24 17:05:54.463167: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3167955)\u001b[0m 2023-11-24 17:05:54.510276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3167955)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3167955)\u001b[0m 2023-11-24 17:05:55.146553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3167955)\u001b[0m 2023-11-24 17:05:56.498745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3167955)\u001b[0m 2023-11-24 17:05:57.918937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3167955)\u001b[0m 2023-11-24 17:05:58.117080: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14fdfc0586d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3167955)\u001b[0m 2023-11-24 17:05:58.117106: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3167955)\u001b[0m 2023-11-24 17:05:58.121491: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3167955)\u001b[0m 2023-11-24 17:05:58.234874: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3168813)\u001b[0m 2023-11-24 17:06:23.515566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3168813)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3168813)\u001b[0m 2023-11-24 17:06:23.468283: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3168813)\u001b[0m 2023-11-24 17:06:24.148551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3168813)\u001b[0m 2023-11-24 17:06:25.506068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3168813)\u001b[0m 2023-11-24 17:06:26.924194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3168813)\u001b[0m 2023-11-24 17:06:27.128051: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14a55d622a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3168813)\u001b[0m 2023-11-24 17:06:27.128076: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3168813)\u001b[0m 2023-11-24 17:06:27.132443: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3168813)\u001b[0m 2023-11-24 17:06:27.245253: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3169717)\u001b[0m 2023-11-24 17:06:49.481141: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3169717)\u001b[0m 2023-11-24 17:06:49.528946: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3169717)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3169717)\u001b[0m 2023-11-24 17:06:50.184890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3169717)\u001b[0m 2023-11-24 17:06:51.579980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3169717)\u001b[0m 2023-11-24 17:06:52.998376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3169717)\u001b[0m 2023-11-24 17:06:53.197114: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x152a8c059730 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3169717)\u001b[0m 2023-11-24 17:06:53.197141: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3169717)\u001b[0m 2023-11-24 17:06:53.201373: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3169717)\u001b[0m 2023-11-24 17:06:53.314639: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3170740)\u001b[0m 2023-11-24 17:07:33.523115: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3170740)\u001b[0m 2023-11-24 17:07:33.570193: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3170740)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3170740)\u001b[0m 2023-11-24 17:07:34.244593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3170740)\u001b[0m 2023-11-24 17:07:35.643142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3170740)\u001b[0m 2023-11-24 17:07:37.060981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3170740)\u001b[0m 2023-11-24 17:07:37.264179: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14fc00058da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3170740)\u001b[0m 2023-11-24 17:07:37.264205: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3170740)\u001b[0m 2023-11-24 17:07:37.268442: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3170740)\u001b[0m 2023-11-24 17:07:37.381069: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3171651)\u001b[0m 2023-11-24 17:08:09.478705: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3171651)\u001b[0m 2023-11-24 17:08:09.525494: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3171651)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3171651)\u001b[0m 2023-11-24 17:08:10.155830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3171651)\u001b[0m 2023-11-24 17:08:11.509208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3171651)\u001b[0m 2023-11-24 17:08:12.936277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3171651)\u001b[0m 2023-11-24 17:08:13.148299: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x148ab4058810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3171651)\u001b[0m 2023-11-24 17:08:13.148324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3171651)\u001b[0m 2023-11-24 17:08:13.152663: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3171651)\u001b[0m 2023-11-24 17:08:13.265202: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3172602)\u001b[0m 2023-11-24 17:08:41.541552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3172602)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3172602)\u001b[0m 2023-11-24 17:08:41.494347: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3172602)\u001b[0m 2023-11-24 17:08:42.178250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3172602)\u001b[0m 2023-11-24 17:08:43.521761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3172602)\u001b[0m 2023-11-24 17:08:44.962037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3172602)\u001b[0m 2023-11-24 17:08:45.156990: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1523f4058af0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3172602)\u001b[0m 2023-11-24 17:08:45.157015: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3172602)\u001b[0m 2023-11-24 17:08:45.161379: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3172602)\u001b[0m 2023-11-24 17:08:45.274166: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3173494)\u001b[0m 2023-11-24 17:09:15.516445: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3173494)\u001b[0m 2023-11-24 17:09:15.563884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3173494)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3173494)\u001b[0m 2023-11-24 17:09:16.215928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3173494)\u001b[0m 2023-11-24 17:09:17.579396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3173494)\u001b[0m 2023-11-24 17:09:19.024127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3173494)\u001b[0m 2023-11-24 17:09:19.224811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14c7d9789d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3173494)\u001b[0m 2023-11-24 17:09:19.224846: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3173494)\u001b[0m 2023-11-24 17:09:19.229061: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3173494)\u001b[0m 2023-11-24 17:09:19.343203: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3174415)\u001b[0m 2023-11-24 17:09:42.502914: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3174415)\u001b[0m 2023-11-24 17:09:42.549731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3174415)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3174415)\u001b[0m 2023-11-24 17:09:43.194903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3174415)\u001b[0m 2023-11-24 17:09:44.538557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3174415)\u001b[0m 2023-11-24 17:09:45.960727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3174415)\u001b[0m 2023-11-24 17:09:46.157373: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x153768058f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3174415)\u001b[0m 2023-11-24 17:09:46.157399: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3174415)\u001b[0m 2023-11-24 17:09:46.161794: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3174415)\u001b[0m 2023-11-24 17:09:46.274991: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3175308)\u001b[0m 2023-11-24 17:10:05.522917: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3175308)\u001b[0m 2023-11-24 17:10:05.570483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3175308)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3175308)\u001b[0m 2023-11-24 17:10:06.210624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3175308)\u001b[0m 2023-11-24 17:10:07.585974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3175308)\u001b[0m 2023-11-24 17:10:09.007518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3175308)\u001b[0m 2023-11-24 17:10:09.203331: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15353c05d050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3175308)\u001b[0m 2023-11-24 17:10:09.203355: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3175308)\u001b[0m 2023-11-24 17:10:09.207561: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3175308)\u001b[0m 2023-11-24 17:10:09.320413: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3176244)\u001b[0m 2023-11-24 17:10:42.511386: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3176244)\u001b[0m 2023-11-24 17:10:42.557930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3176244)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3176244)\u001b[0m 2023-11-24 17:10:43.215225: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3176244)\u001b[0m 2023-11-24 17:10:44.598719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3176244)\u001b[0m 2023-11-24 17:10:46.038444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3176244)\u001b[0m 2023-11-24 17:10:46.236415: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14bcd0059ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3176244)\u001b[0m 2023-11-24 17:10:46.236440: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3176244)\u001b[0m 2023-11-24 17:10:46.240678: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3176244)\u001b[0m 2023-11-24 17:10:46.354204: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3177158)\u001b[0m 2023-11-24 17:11:12.504797: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3177158)\u001b[0m 2023-11-24 17:11:12.551847: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3177158)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3177158)\u001b[0m 2023-11-24 17:11:13.181995: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3177158)\u001b[0m 2023-11-24 17:11:14.525849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3177158)\u001b[0m 2023-11-24 17:11:15.948032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3177158)\u001b[0m 2023-11-24 17:11:16.148036: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1474780591d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3177158)\u001b[0m 2023-11-24 17:11:16.148061: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3177158)\u001b[0m 2023-11-24 17:11:16.152408: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3177158)\u001b[0m 2023-11-24 17:11:16.265151: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3178058)\u001b[0m 2023-11-24 17:11:38.511660: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3178058)\u001b[0m 2023-11-24 17:11:38.558868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3178058)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3178058)\u001b[0m 2023-11-24 17:11:39.188714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3178058)\u001b[0m 2023-11-24 17:11:40.542926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3178058)\u001b[0m 2023-11-24 17:11:41.969563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3178058)\u001b[0m 2023-11-24 17:11:42.180086: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1527180589d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3178058)\u001b[0m 2023-11-24 17:11:42.180111: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3178058)\u001b[0m 2023-11-24 17:11:42.184474: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3178058)\u001b[0m 2023-11-24 17:11:42.296746: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3179128)\u001b[0m 2023-11-24 17:12:22.514133: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3179128)\u001b[0m 2023-11-24 17:12:22.561494: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3179128)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3179128)\u001b[0m 2023-11-24 17:12:23.191058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3179128)\u001b[0m 2023-11-24 17:12:24.532823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3179128)\u001b[0m 2023-11-24 17:12:25.956657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3179128)\u001b[0m 2023-11-24 17:12:26.166511: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14f2dd622000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3179128)\u001b[0m 2023-11-24 17:12:26.166542: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3179128)\u001b[0m 2023-11-24 17:12:26.170777: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3179128)\u001b[0m 2023-11-24 17:12:26.285014: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3180098)\u001b[0m 2023-11-24 17:12:58.528382: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3180098)\u001b[0m 2023-11-24 17:12:58.575567: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3180098)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3180098)\u001b[0m 2023-11-24 17:12:59.212348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3180098)\u001b[0m 2023-11-24 17:13:00.588175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3180098)\u001b[0m 2023-11-24 17:13:02.018473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3180098)\u001b[0m 2023-11-24 17:13:02.233475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146b51622810 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3180098)\u001b[0m 2023-11-24 17:13:02.233507: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3180098)\u001b[0m 2023-11-24 17:13:02.237846: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3180098)\u001b[0m 2023-11-24 17:13:02.351553: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3180999)\u001b[0m 2023-11-24 17:13:30.559651: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3180999)\u001b[0m 2023-11-24 17:13:30.606709: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3180999)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3180999)\u001b[0m 2023-11-24 17:13:31.247084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3180999)\u001b[0m 2023-11-24 17:13:32.651010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3180999)\u001b[0m 2023-11-24 17:13:34.274202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3180999)\u001b[0m 2023-11-24 17:13:34.478012: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14e9f178a2e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3180999)\u001b[0m 2023-11-24 17:13:34.478053: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3180999)\u001b[0m 2023-11-24 17:13:34.482784: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3180999)\u001b[0m 2023-11-24 17:13:34.634372: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3181971)\u001b[0m 2023-11-24 17:14:05.550660: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3181971)\u001b[0m 2023-11-24 17:14:05.597874: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3181971)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3181971)\u001b[0m 2023-11-24 17:14:06.240834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3181971)\u001b[0m 2023-11-24 17:14:07.619000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3181971)\u001b[0m 2023-11-24 17:14:09.056751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3181971)\u001b[0m 2023-11-24 17:14:09.254059: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1521480598a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3181971)\u001b[0m 2023-11-24 17:14:09.254085: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3181971)\u001b[0m 2023-11-24 17:14:09.258316: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3181971)\u001b[0m 2023-11-24 17:14:09.370798: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3182843)\u001b[0m 2023-11-24 17:14:32.563564: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3182843)\u001b[0m 2023-11-24 17:14:32.610978: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3182843)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3182843)\u001b[0m 2023-11-24 17:14:33.266713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3182843)\u001b[0m 2023-11-24 17:14:34.631238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3182843)\u001b[0m 2023-11-24 17:14:36.084164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3182843)\u001b[0m 2023-11-24 17:14:36.285829: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x147cf006dc20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3182843)\u001b[0m 2023-11-24 17:14:36.285863: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3182843)\u001b[0m 2023-11-24 17:14:36.290324: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3182843)\u001b[0m 2023-11-24 17:14:36.405178: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3183751)\u001b[0m 2023-11-24 17:14:55.544196: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3183751)\u001b[0m 2023-11-24 17:14:55.591227: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3183751)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3183751)\u001b[0m 2023-11-24 17:14:56.228094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3183751)\u001b[0m 2023-11-24 17:14:57.620216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3183751)\u001b[0m 2023-11-24 17:14:59.088188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3183751)\u001b[0m 2023-11-24 17:14:59.284578: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14fe44058300 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3183751)\u001b[0m 2023-11-24 17:14:59.284610: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3183751)\u001b[0m 2023-11-24 17:14:59.288967: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3183751)\u001b[0m 2023-11-24 17:14:59.402138: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3184657)\u001b[0m 2023-11-24 17:15:32.556199: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3184657)\u001b[0m 2023-11-24 17:15:32.603427: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3184657)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3184657)\u001b[0m 2023-11-24 17:15:33.244890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3184657)\u001b[0m 2023-11-24 17:15:34.611139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3184657)\u001b[0m 2023-11-24 17:15:36.051558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3184657)\u001b[0m 2023-11-24 17:15:36.258560: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14dd0405acd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3184657)\u001b[0m 2023-11-24 17:15:36.258589: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3184657)\u001b[0m 2023-11-24 17:15:36.262779: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3184657)\u001b[0m 2023-11-24 17:15:36.377731: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3185613)\u001b[0m 2023-11-24 17:16:02.598671: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3185613)\u001b[0m 2023-11-24 17:16:02.645486: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3185613)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3185613)\u001b[0m 2023-11-24 17:16:03.293247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3185613)\u001b[0m 2023-11-24 17:16:04.678468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3185613)\u001b[0m 2023-11-24 17:16:06.108429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3185613)\u001b[0m 2023-11-24 17:16:06.309938: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b10805d4c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3185613)\u001b[0m 2023-11-24 17:16:06.309965: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3185613)\u001b[0m 2023-11-24 17:16:06.314168: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3185613)\u001b[0m 2023-11-24 17:16:06.427081: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3186527)\u001b[0m 2023-11-24 17:16:28.556961: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3186527)\u001b[0m 2023-11-24 17:16:28.604079: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3186527)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3186527)\u001b[0m 2023-11-24 17:16:29.242088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3186527)\u001b[0m 2023-11-24 17:16:30.609742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3186527)\u001b[0m 2023-11-24 17:16:32.051388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3186527)\u001b[0m 2023-11-24 17:16:32.253349: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x147d40058e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3186527)\u001b[0m 2023-11-24 17:16:32.253379: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3186527)\u001b[0m 2023-11-24 17:16:32.257625: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3186527)\u001b[0m 2023-11-24 17:16:32.371146: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3187451)\u001b[0m 2023-11-24 17:17:12.557917: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3187451)\u001b[0m 2023-11-24 17:17:12.605610: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3187451)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3187451)\u001b[0m 2023-11-24 17:17:13.241363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3187451)\u001b[0m 2023-11-24 17:17:14.596846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3187451)\u001b[0m 2023-11-24 17:17:16.032036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3187451)\u001b[0m 2023-11-24 17:17:16.236933: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146be80591f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3187451)\u001b[0m 2023-11-24 17:17:16.236960: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3187451)\u001b[0m 2023-11-24 17:17:16.241145: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3187451)\u001b[0m 2023-11-24 17:17:16.354198: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3188441)\u001b[0m 2023-11-24 17:17:48.581487: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3188441)\u001b[0m 2023-11-24 17:17:48.628299: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3188441)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3188441)\u001b[0m 2023-11-24 17:17:49.263687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3188441)\u001b[0m 2023-11-24 17:17:50.621674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3188441)\u001b[0m 2023-11-24 17:17:52.055060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3188441)\u001b[0m 2023-11-24 17:17:52.268719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146ed0059110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3188441)\u001b[0m 2023-11-24 17:17:52.268746: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3188441)\u001b[0m 2023-11-24 17:17:52.273072: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3188441)\u001b[0m 2023-11-24 17:17:52.385599: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3189361)\u001b[0m 2023-11-24 17:18:20.561420: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3189361)\u001b[0m 2023-11-24 17:18:20.608004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3189361)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3189361)\u001b[0m 2023-11-24 17:18:21.238736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3189361)\u001b[0m 2023-11-24 17:18:22.598465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3189361)\u001b[0m 2023-11-24 17:18:24.036929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3189361)\u001b[0m 2023-11-24 17:18:24.232081: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14dc80058750 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3189361)\u001b[0m 2023-11-24 17:18:24.232105: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3189361)\u001b[0m 2023-11-24 17:18:24.236318: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3189361)\u001b[0m 2023-11-24 17:18:24.349157: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3190330)\u001b[0m 2023-11-24 17:18:55.574284: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3190330)\u001b[0m 2023-11-24 17:18:55.621803: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3190330)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3190330)\u001b[0m 2023-11-24 17:18:56.260811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3190330)\u001b[0m 2023-11-24 17:18:57.629074: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3190330)\u001b[0m 2023-11-24 17:18:59.065115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3190330)\u001b[0m 2023-11-24 17:18:59.262471: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15221178a680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3190330)\u001b[0m 2023-11-24 17:18:59.262499: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3190330)\u001b[0m 2023-11-24 17:18:59.266883: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3190330)\u001b[0m 2023-11-24 17:18:59.380150: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3191237)\u001b[0m 2023-11-24 17:19:22.567965: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3191237)\u001b[0m 2023-11-24 17:19:22.615102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3191237)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3191237)\u001b[0m 2023-11-24 17:19:23.252061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3191237)\u001b[0m 2023-11-24 17:19:24.620974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3191237)\u001b[0m 2023-11-24 17:19:26.050456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3191237)\u001b[0m 2023-11-24 17:19:26.249419: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x149eac05d9f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3191237)\u001b[0m 2023-11-24 17:19:26.249442: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3191237)\u001b[0m 2023-11-24 17:19:26.253698: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3191237)\u001b[0m 2023-11-24 17:19:26.367377: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3192107)\u001b[0m 2023-11-24 17:19:45.572258: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3192107)\u001b[0m 2023-11-24 17:19:45.619576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3192107)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3192107)\u001b[0m 2023-11-24 17:19:46.254231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3192107)\u001b[0m 2023-11-24 17:19:47.610833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3192107)\u001b[0m 2023-11-24 17:19:49.046460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3192107)\u001b[0m 2023-11-24 17:19:49.242697: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14c3718fd640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3192107)\u001b[0m 2023-11-24 17:19:49.242722: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3192107)\u001b[0m 2023-11-24 17:19:49.247158: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3192107)\u001b[0m 2023-11-24 17:19:49.360186: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3193607)\u001b[0m 2023-11-24 17:20:22.589516: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3193607)\u001b[0m 2023-11-24 17:20:22.637204: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3193607)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3193607)\u001b[0m 2023-11-24 17:20:23.272059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3193607)\u001b[0m 2023-11-24 17:20:24.653457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3193607)\u001b[0m 2023-11-24 17:20:26.088235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3193607)\u001b[0m 2023-11-24 17:20:26.286953: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b8a8059240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3193607)\u001b[0m 2023-11-24 17:20:26.286979: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3193607)\u001b[0m 2023-11-24 17:20:26.291384: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3193607)\u001b[0m 2023-11-24 17:20:26.404835: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3194510)\u001b[0m 2023-11-24 17:20:52.574429: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3194510)\u001b[0m 2023-11-24 17:20:52.621183: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3194510)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3194510)\u001b[0m 2023-11-24 17:20:53.264101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3194510)\u001b[0m 2023-11-24 17:20:54.656229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3194510)\u001b[0m 2023-11-24 17:20:56.108325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3194510)\u001b[0m 2023-11-24 17:20:56.315665: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x153814059550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3194510)\u001b[0m 2023-11-24 17:20:56.315693: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3194510)\u001b[0m 2023-11-24 17:20:56.320064: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3194510)\u001b[0m 2023-11-24 17:20:56.435043: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3195429)\u001b[0m 2023-11-24 17:21:18.605779: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3195429)\u001b[0m 2023-11-24 17:21:18.652661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3195429)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3195429)\u001b[0m 2023-11-24 17:21:19.318533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3195429)\u001b[0m 2023-11-24 17:21:20.685758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3195429)\u001b[0m 2023-11-24 17:21:22.109386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3195429)\u001b[0m 2023-11-24 17:21:22.307684: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x152808058b00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3195429)\u001b[0m 2023-11-24 17:21:22.307703: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3195429)\u001b[0m 2023-11-24 17:21:22.311899: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3195429)\u001b[0m 2023-11-24 17:21:22.424953: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3196864)\u001b[0m 2023-11-24 17:22:02.617206: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3196864)\u001b[0m 2023-11-24 17:22:02.664015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3196864)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3196864)\u001b[0m 2023-11-24 17:22:03.310273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3196864)\u001b[0m 2023-11-24 17:22:04.674332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3196864)\u001b[0m 2023-11-24 17:22:06.102901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3196864)\u001b[0m 2023-11-24 17:22:06.306130: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b79578a720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3196864)\u001b[0m 2023-11-24 17:22:06.306157: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3196864)\u001b[0m 2023-11-24 17:22:06.310548: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3196864)\u001b[0m 2023-11-24 17:22:06.423554: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3197812)\u001b[0m 2023-11-24 17:22:38.605577: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3197812)\u001b[0m 2023-11-24 17:22:38.653637: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3197812)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3197812)\u001b[0m 2023-11-24 17:22:39.309404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3197812)\u001b[0m 2023-11-24 17:22:40.695188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3197812)\u001b[0m 2023-11-24 17:22:42.142497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3197812)\u001b[0m 2023-11-24 17:22:42.362032: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146f70058cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3197812)\u001b[0m 2023-11-24 17:22:42.362064: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3197812)\u001b[0m 2023-11-24 17:22:42.366443: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3197812)\u001b[0m 2023-11-24 17:22:42.480686: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3198763)\u001b[0m 2023-11-24 17:23:10.587383: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3198763)\u001b[0m 2023-11-24 17:23:10.634830: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3198763)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3198763)\u001b[0m 2023-11-24 17:23:11.264130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3198763)\u001b[0m 2023-11-24 17:23:12.641887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3198763)\u001b[0m 2023-11-24 17:23:14.061571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3198763)\u001b[0m 2023-11-24 17:23:14.257408: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14da7c056c20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3198763)\u001b[0m 2023-11-24 17:23:14.257434: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3198763)\u001b[0m 2023-11-24 17:23:14.261837: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3198763)\u001b[0m 2023-11-24 17:23:14.374755: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3200129)\u001b[0m 2023-11-24 17:23:45.620521: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3200129)\u001b[0m 2023-11-24 17:23:45.667539: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3200129)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3200129)\u001b[0m 2023-11-24 17:23:46.319586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3200129)\u001b[0m 2023-11-24 17:23:47.717150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3200129)\u001b[0m 2023-11-24 17:23:49.157683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3200129)\u001b[0m 2023-11-24 17:23:49.353680: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14a40d789ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3200129)\u001b[0m 2023-11-24 17:23:49.353706: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3200129)\u001b[0m 2023-11-24 17:23:49.357868: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3200129)\u001b[0m 2023-11-24 17:23:49.470959: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3201062)\u001b[0m 2023-11-24 17:24:12.600728: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3201062)\u001b[0m 2023-11-24 17:24:12.648259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3201062)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3201062)\u001b[0m 2023-11-24 17:24:13.281470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3201062)\u001b[0m 2023-11-24 17:24:14.640921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3201062)\u001b[0m 2023-11-24 17:24:16.071361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3201062)\u001b[0m 2023-11-24 17:24:16.267593: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x146a74059840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3201062)\u001b[0m 2023-11-24 17:24:16.267618: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3201062)\u001b[0m 2023-11-24 17:24:16.272024: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3201062)\u001b[0m 2023-11-24 17:24:16.384787: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3201925)\u001b[0m 2023-11-24 17:24:35.615150: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3201925)\u001b[0m 2023-11-24 17:24:35.662559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3201925)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3201925)\u001b[0m 2023-11-24 17:24:36.303747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3201925)\u001b[0m 2023-11-24 17:24:37.682030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3201925)\u001b[0m 2023-11-24 17:24:39.110858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3201925)\u001b[0m 2023-11-24 17:24:39.310561: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1510d0058310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3201925)\u001b[0m 2023-11-24 17:24:39.310585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3201925)\u001b[0m 2023-11-24 17:24:39.314782: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3201925)\u001b[0m 2023-11-24 17:24:39.427526: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3203166)\u001b[0m 2023-11-24 17:25:11.635948: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3203166)\u001b[0m 2023-11-24 17:25:11.683028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3203166)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3203166)\u001b[0m 2023-11-24 17:25:12.330177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3203166)\u001b[0m 2023-11-24 17:25:13.678845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3203166)\u001b[0m 2023-11-24 17:25:15.099575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3203166)\u001b[0m 2023-11-24 17:25:15.298012: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1482880594e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3203166)\u001b[0m 2023-11-24 17:25:15.298037: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3203166)\u001b[0m 2023-11-24 17:25:15.302264: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3203166)\u001b[0m 2023-11-24 17:25:15.415055: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3204233)\u001b[0m 2023-11-24 17:25:41.599716: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3204233)\u001b[0m 2023-11-24 17:25:41.646247: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3204233)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3204233)\u001b[0m 2023-11-24 17:25:42.273307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3204233)\u001b[0m 2023-11-24 17:25:43.620795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3204233)\u001b[0m 2023-11-24 17:25:45.054025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3204233)\u001b[0m 2023-11-24 17:25:45.255461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x149a1c06c1f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3204233)\u001b[0m 2023-11-24 17:25:45.255485: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3204233)\u001b[0m 2023-11-24 17:25:45.259864: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3204233)\u001b[0m 2023-11-24 17:25:45.372386: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3205108)\u001b[0m 2023-11-24 17:26:07.611964: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3205108)\u001b[0m 2023-11-24 17:26:07.659564: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3205108)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3205108)\u001b[0m 2023-11-24 17:26:08.292565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3205108)\u001b[0m 2023-11-24 17:26:09.643349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3205108)\u001b[0m 2023-11-24 17:26:11.092172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3205108)\u001b[0m 2023-11-24 17:26:11.293186: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1444f578a420 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3205108)\u001b[0m 2023-11-24 17:26:11.293210: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3205108)\u001b[0m 2023-11-24 17:26:11.297596: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3205108)\u001b[0m 2023-11-24 17:26:11.410958: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3206375)\u001b[0m 2023-11-24 17:26:51.658093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3206375)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3206375)\u001b[0m 2023-11-24 17:26:51.610974: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3206375)\u001b[0m 2023-11-24 17:26:52.307840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3206375)\u001b[0m 2023-11-24 17:26:53.692267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3206375)\u001b[0m 2023-11-24 17:26:55.123362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3206375)\u001b[0m 2023-11-24 17:26:55.331923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1447b8058cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3206375)\u001b[0m 2023-11-24 17:26:55.331955: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3206375)\u001b[0m 2023-11-24 17:26:55.336401: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3206375)\u001b[0m 2023-11-24 17:26:55.450332: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "\u001b[36m(pid=3207540)\u001b[0m 2023-11-24 17:27:27.618040: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3207540)\u001b[0m 2023-11-24 17:27:27.665497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3207540)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3207540)\u001b[0m 2023-11-24 17:27:28.304458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(train_mnist pid=3207540)\u001b[0m 2023-11-24 17:27:29.679278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30641 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\n",
            "\u001b[36m(train_mnist pid=3207540)\u001b[0m 2023-11-24 17:27:31.110373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n",
            "\u001b[36m(train_mnist pid=3207540)\u001b[0m 2023-11-24 17:27:31.325465: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14a85006ca00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "\u001b[36m(train_mnist pid=3207540)\u001b[0m 2023-11-24 17:27:31.325493: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
            "\u001b[36m(train_mnist pid=3207540)\u001b[0m 2023-11-24 17:27:31.329682: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "\u001b[36m(train_mnist pid=3207540)\u001b[0m 2023-11-24 17:27:31.442798: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "2023-11-24 17:27:57,729\tINFO tune.py:1047 -- Total run time: 5255.26 seconds (5247.58 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "grid_analysis = tune.run(\n",
        "    train_mnist,\n",
        "    name=\"exp\",\n",
        "    metric=\"mean_accuracy\",\n",
        "    mode=\"max\",\n",
        "    stop={\"mean_accuracy\": 0.99},\n",
        "    resources_per_trial={\"gpu\": 1},\n",
        "    config={\n",
        "        \"conv_filters\": tune.grid_search([64, 128, 256]),\n",
        "        \"lr\": tune.grid_search([0.001, 0.01, 0.1]),\n",
        "        \"batch_size\": tune.grid_search([64, 128, 256]),\n",
        "        \"dropout\": tune.grid_search([0.0, 0.25, 0.5, 0.75, 0.9]),\n",
        "    }\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "grid_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxRYScZyjlw3"
      },
      "source": [
        "### Beyesian Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a1KI_nWvUat8"
      },
      "outputs": [],
      "source": [
        "#!pip install bayesian-optimization #--yes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4FfUau0Oj-V"
      },
      "outputs": [],
      "source": [
        "def train_mnist(config):\n",
        "    # Convert continuous parameter back to categorical\n",
        "    batch_size_map = {0: 64, 1: 128, 2: 256}\n",
        "\n",
        "    config[\"conv_filters\"] = int(round(config[\"conv_filters\"]))\n",
        "    config[\"batch_size\"] = batch_size_map[int(round(config[\"batch_size\"]))]\n",
        "\n",
        "    # Load MNIST data\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_test = x_train.reshape(-1, 28, 28, 1) / 255.0, x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "    # Define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(filters=config[\"conv_filters\"], kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(config[\"dropout\"]),\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\")  # Assuming 10 classes for MNIST\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"lr\"]),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        epochs=12,  # or any other number of epochs you wish to use\n",
        "        verbose=1,\n",
        "        validation_data=(x_test, y_test)\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    _, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "    session.report({\"mean_accuracy\": accuracy})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbjVWve3OkVo",
        "outputId": "190ed7bb-c6a0-4858-e0e6-d1f130e32aca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-05 01:39:33,602\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
            "2023-12-05 01:39:33,620\tWARNING bayesopt_search.py:431 -- BayesOpt does not support specific sampling methods. The LogUniform sampler will be dropped.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-12-05 02:00:18</td></tr>\n",
              "<tr><td>Running for: </td><td>00:20:44.58        </td></tr>\n",
              "<tr><td>Memory:      </td><td>65.4/377.3 GiB     </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/48 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:RTX)\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_filters</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">   acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_fb005315</td><td>TERMINATED</td><td>10.32.35.160:3295966</td><td style=\"text-align: right;\">    0.74908 </td><td style=\"text-align: right;\">      246.537 </td><td style=\"text-align: right;\">0.731994 </td><td style=\"text-align: right;\">0.0602672</td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        179.266 </td></tr>\n",
              "<tr><td>train_mnist_f2b95443</td><td>TERMINATED</td><td>10.32.35.160:3296841</td><td style=\"text-align: right;\">    0.312037</td><td style=\"text-align: right;\">       93.9509</td><td style=\"text-align: right;\">0.0580836</td><td style=\"text-align: right;\">0.0867514</td><td style=\"text-align: right;\">0.9085</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         90.4304</td></tr>\n",
              "<tr><td>train_mnist_b9914140</td><td>TERMINATED</td><td>10.32.35.160:3297460</td><td style=\"text-align: right;\">    1.20223 </td><td style=\"text-align: right;\">      199.95  </td><td style=\"text-align: right;\">0.0205845</td><td style=\"text-align: right;\">0.0970211</td><td style=\"text-align: right;\">0.862 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        159.762 </td></tr>\n",
              "<tr><td>train_mnist_ffca9cac</td><td>TERMINATED</td><td>10.32.35.160:3298267</td><td style=\"text-align: right;\">    1.66489 </td><td style=\"text-align: right;\">      104.769 </td><td style=\"text-align: right;\">0.181825 </td><td style=\"text-align: right;\">0.019157 </td><td style=\"text-align: right;\">0.9812</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         70.9554</td></tr>\n",
              "<tr><td>train_mnist_a60fcaaf</td><td>TERMINATED</td><td>10.32.35.160:3298881</td><td style=\"text-align: right;\">    0.608484</td><td style=\"text-align: right;\">      164.753 </td><td style=\"text-align: right;\">0.431945 </td><td style=\"text-align: right;\">0.0298317</td><td style=\"text-align: right;\">0.9616</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        116.677 </td></tr>\n",
              "<tr><td>train_mnist_65d41f89</td><td>TERMINATED</td><td>10.32.35.160:3299519</td><td style=\"text-align: right;\">    1.22371 </td><td style=\"text-align: right;\">       90.7828</td><td style=\"text-align: right;\">0.292145 </td><td style=\"text-align: right;\">0.0372698</td><td style=\"text-align: right;\">0.9508</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.2914</td></tr>\n",
              "<tr><td>train_mnist_cd0082ba</td><td>TERMINATED</td><td>10.32.35.160:3300066</td><td style=\"text-align: right;\">    0.91214 </td><td style=\"text-align: right;\">      214.754 </td><td style=\"text-align: right;\">0.199674 </td><td style=\"text-align: right;\">0.0519092</td><td style=\"text-align: right;\">0.9597</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        179.012 </td></tr>\n",
              "<tr><td>train_mnist_1a912c10</td><td>TERMINATED</td><td>10.32.35.160:3300944</td><td style=\"text-align: right;\">    1.18483 </td><td style=\"text-align: right;\">       72.9185</td><td style=\"text-align: right;\">0.607545 </td><td style=\"text-align: right;\">0.0178819</td><td style=\"text-align: right;\">0.9793</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         53.3985</td></tr>\n",
              "<tr><td>train_mnist_eecf567c</td><td>TERMINATED</td><td>10.32.35.160:3301447</td><td style=\"text-align: right;\">    0.130103</td><td style=\"text-align: right;\">      246.186 </td><td style=\"text-align: right;\">0.965632 </td><td style=\"text-align: right;\">0.0810313</td><td style=\"text-align: right;\">0.1135</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        226.149 </td></tr>\n",
              "<tr><td>train_mnist_40c5a41c</td><td>TERMINATED</td><td>10.32.35.160:3302450</td><td style=\"text-align: right;\">    0.609228</td><td style=\"text-align: right;\">       82.753 </td><td style=\"text-align: right;\">0.684233 </td><td style=\"text-align: right;\">0.0445751</td><td style=\"text-align: right;\">0.8942</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.0914</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3295966)\u001b[0m 2023-12-05 01:39:35.572982: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3295966)\u001b[0m 2023-12-05 01:39:35.575219: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3295966)\u001b[0m 2023-12-05 01:39:35.620141: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3295966)\u001b[0m 2023-12-05 01:39:35.620637: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3295966)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3295966)\u001b[0m 2023-12-05 01:39:36.541606: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3295966)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3295966)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3295966)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3295966)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3295966)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3295966)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3295966)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3295966)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m 2023-12-05 01:39:38.231551: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 1/12\n",
            "  1/469 [..............................] - ETA: 6:11 - loss: 2.3099 - accuracy: 0.0781\n",
            "  5/469 [..............................] - ETA: 13s - loss: 46.7496 - accuracy: 0.1031\n",
            "  9/469 [..............................] - ETA: 13s - loss: 27.0000 - accuracy: 0.1050\n",
            " 13/469 [..............................] - ETA: 13s - loss: 19.4022 - accuracy: 0.1064\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 15.3785 - accuracy: 0.1085\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 14.0030 - accuracy: 0.1090\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 11.9684 - accuracy: 0.1073\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 10.5366 - accuracy: 0.1045\n",
            " 31/469 [>.............................] - ETA: 12s - loss: 9.4743 - accuracy: 0.1058\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 8.6547 - accuracy: 0.1051\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 8.3116 - accuracy: 0.1047\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 7.7252 - accuracy: 0.1059\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 7.2431 - accuracy: 0.1056\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 6.8397 - accuracy: 0.1048\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 6.6618 - accuracy: 0.1059\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 6.4975 - accuracy: 0.1054\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 6.3450 - accuracy: 0.1057\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 6.0710 - accuracy: 0.1053\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 5.8323 - accuracy: 0.1049\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 5.6217 - accuracy: 0.1040\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 5.5256 - accuracy: 0.1042\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 5.3490 - accuracy: 0.1043\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 5.1909 - accuracy: 0.1044\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 5.0487 - accuracy: 0.1046\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 4.9196 - accuracy: 0.1040\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 4.8596 - accuracy: 0.1031\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 4.7473 - accuracy: 0.1031\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 4.6442 - accuracy: 0.1039\n",
            " 99/469 [=====>........................] - ETA: 10s - loss: 4.5495 - accuracy: 0.1046\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 4.4623 - accuracy: 0.1036\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 4.4215 - accuracy: 0.1034\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 4.3439 - accuracy: 0.1044\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 4.2715 - accuracy: 0.1056\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 4.2043 - accuracy: 0.1061\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 4.1414 - accuracy: 0.1065\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 4.1113 - accuracy: 0.1069\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 4.0543 - accuracy: 0.1073\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 4.0007 - accuracy: 0.1078\n",
            "135/469 [=======>......................] - ETA: 9s - loss: 3.9500 - accuracy: 0.1094\n",
            "139/469 [=======>......................] - ETA: 9s - loss: 3.9026 - accuracy: 0.1092\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 3.8802 - accuracy: 0.1087\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 3.8366 - accuracy: 0.1088\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 3.7953 - accuracy: 0.1093\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 3.7564 - accuracy: 0.1089\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 3.7193 - accuracy: 0.1085\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 3.7015 - accuracy: 0.1082\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 3.6671 - accuracy: 0.1077\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 3.6345 - accuracy: 0.1082\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 3.6032 - accuracy: 0.1086\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 3.5881 - accuracy: 0.1084\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 3.5594 - accuracy: 0.1081\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 3.5318 - accuracy: 0.1079\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 3.5053 - accuracy: 0.1076\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 3.4925 - accuracy: 0.1072\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 3.4799 - accuracy: 0.1073\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 3.4676 - accuracy: 0.1073\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 3.4437 - accuracy: 0.1073\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 3.4207 - accuracy: 0.1075\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 3.3987 - accuracy: 0.1075\n",
            "207/469 [============>.................] - ETA: 7s - loss: 3.3775 - accuracy: 0.1073\n",
            "209/469 [============>.................] - ETA: 7s - loss: 3.3673 - accuracy: 0.1072\n",
            "213/469 [============>.................] - ETA: 7s - loss: 3.3474 - accuracy: 0.1072\n",
            "217/469 [============>.................] - ETA: 7s - loss: 3.3281 - accuracy: 0.1074\n",
            "221/469 [=============>................] - ETA: 7s - loss: 3.3097 - accuracy: 0.1076\n",
            "225/469 [=============>................] - ETA: 7s - loss: 3.2917 - accuracy: 0.1076\n",
            "227/469 [=============>................] - ETA: 7s - loss: 3.2830 - accuracy: 0.1075\n",
            "231/469 [=============>................] - ETA: 7s - loss: 3.2660 - accuracy: 0.1077\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 3.2496 - accuracy: 0.1079\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 3.2335 - accuracy: 0.1086\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 3.2182 - accuracy: 0.1086\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 3.2108 - accuracy: 0.1086\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 3.1963 - accuracy: 0.1089\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 3.1821 - accuracy: 0.1089\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 3.1684 - accuracy: 0.1088\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 3.1553 - accuracy: 0.1083\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 3.1489 - accuracy: 0.1083\n",
            "267/469 [================>.............] - ETA: 5s - loss: 3.1363 - accuracy: 0.1082\n",
            "271/469 [================>.............] - ETA: 5s - loss: 3.1240 - accuracy: 0.1081\n",
            "275/469 [================>.............] - ETA: 5s - loss: 3.1121 - accuracy: 0.1078\n",
            "279/469 [================>.............] - ETA: 5s - loss: 3.1006 - accuracy: 0.1075\n",
            "281/469 [================>.............] - ETA: 5s - loss: 3.0950 - accuracy: 0.1074\n",
            "285/469 [=================>............] - ETA: 5s - loss: 3.0839 - accuracy: 0.1075\n",
            "289/469 [=================>............] - ETA: 5s - loss: 3.0732 - accuracy: 0.1072\n",
            "293/469 [=================>............] - ETA: 5s - loss: 3.0627 - accuracy: 0.1066\n",
            "295/469 [=================>............] - ETA: 5s - loss: 3.0576 - accuracy: 0.1065\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 3.0475 - accuracy: 0.1064\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 3.0378 - accuracy: 0.1064\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 3.0283 - accuracy: 0.1064\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 3.0236 - accuracy: 0.1064\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 3.0144 - accuracy: 0.1065\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 3.0054 - accuracy: 0.1067\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.9967 - accuracy: 0.1065\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.9882 - accuracy: 0.1067\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 2.9840 - accuracy: 0.1068\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.9798 - accuracy: 0.1068\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 2.9758 - accuracy: 0.1067\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.9678 - accuracy: 0.1065\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.9600 - accuracy: 0.1065\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.9523 - accuracy: 0.1066\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 2.9485 - accuracy: 0.1066\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 2.9411 - accuracy: 0.1068\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.9338 - accuracy: 0.1070\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.9267 - accuracy: 0.1071\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.9198 - accuracy: 0.1073\n",
            "363/469 [======================>.......] - ETA: 3s - loss: 2.9165 - accuracy: 0.1073\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 2.9099 - accuracy: 0.1074\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.9033 - accuracy: 0.1072\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.8970 - accuracy: 0.1070\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 2.8938 - accuracy: 0.1070\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 2.8876 - accuracy: 0.1070\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 2.8816 - accuracy: 0.1068\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 2.8757 - accuracy: 0.1068\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.8699 - accuracy: 0.1066\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 2.8671 - accuracy: 0.1064\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.8642 - accuracy: 0.1064\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 2.8614 - accuracy: 0.1065\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 2.8559 - accuracy: 0.1065\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 2.8506 - accuracy: 0.1064\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.8453 - accuracy: 0.1065\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.8401 - accuracy: 0.1062\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 2.8375 - accuracy: 0.1062\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 2.8324 - accuracy: 0.1066\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.8274 - accuracy: 0.1066\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.8226 - accuracy: 0.1066\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.8178 - accuracy: 0.1066\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 2.8155 - accuracy: 0.1065\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 2.8109 - accuracy: 0.1065\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 2.8063 - accuracy: 0.1065\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.8019 - accuracy: 0.1064\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.7974 - accuracy: 0.1064\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 2.7953 - accuracy: 0.1064\n",
            "457/469 [============================>.] - ETA: 0s - loss: 2.7910 - accuracy: 0.1062\n",
            "461/469 [============================>.] - ETA: 0s - loss: 2.7868 - accuracy: 0.1063\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.7827 - accuracy: 0.1061\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.7789 - accuracy: 0.1061\n",
            "469/469 [==============================] - 16s 32ms/step - loss: 2.7789 - accuracy: 0.1061 - val_loss: 2.3039 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 2/12\n",
            "  1/469 [..............................] - ETA: 14s - loss: 2.3035 - accuracy: 0.1094\n",
            "  5/469 [..............................] - ETA: 14s - loss: 2.3074 - accuracy: 0.1141\n",
            "  9/469 [..............................] - ETA: 14s - loss: 2.3050 - accuracy: 0.1094\n",
            " 11/469 [..............................] - ETA: 14s - loss: 2.3052 - accuracy: 0.1044\n",
            " 13/469 [..............................] - ETA: 13s - loss: 2.3045 - accuracy: 0.1034\n",
            " 15/469 [..............................] - ETA: 13s - loss: 2.3040 - accuracy: 0.1057\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 2.3044 - accuracy: 0.1040\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 2.3032 - accuracy: 0.1056\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 2.3015 - accuracy: 0.1097\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 2.3016 - accuracy: 0.1102\n",
            " 33/469 [=>............................] - ETA: 13s - loss: 2.3013 - accuracy: 0.1106\n",
            " 37/469 [=>............................] - ETA: 13s - loss: 2.3025 - accuracy: 0.1113\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 2.3025 - accuracy: 0.1141\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 2.3028 - accuracy: 0.1130\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 2.3039 - accuracy: 0.1107\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 2.3041 - accuracy: 0.1117\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 2.3042 - accuracy: 0.1114\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 2.3044 - accuracy: 0.1115\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3044 - accuracy: 0.1108\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 2.3045 - accuracy: 0.1104\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 2.3045 - accuracy: 0.1100\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 2.3043 - accuracy: 0.1098\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 2.3043 - accuracy: 0.1100\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 2.3048 - accuracy: 0.1086\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1090\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3043 - accuracy: 0.1092\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3041 - accuracy: 0.1103\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 2.3042 - accuracy: 0.1107\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 2.3039 - accuracy: 0.1113\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1109\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1109\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1115\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1117\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 2.3043 - accuracy: 0.1125\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 2.3042 - accuracy: 0.1127\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 2.3042 - accuracy: 0.1119\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 2.3041 - accuracy: 0.1116\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3040 - accuracy: 0.1119\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3039 - accuracy: 0.1116\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 2.3039 - accuracy: 0.1115\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 2.3039 - accuracy: 0.1113\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 2.3037 - accuracy: 0.1114\n",
            "135/469 [=======>......................] - ETA: 9s - loss: 2.3037 - accuracy: 0.1112 \n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3038 - accuracy: 0.1109\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.3038 - accuracy: 0.1105\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3039 - accuracy: 0.1109\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3039 - accuracy: 0.1110\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.3040 - accuracy: 0.1109\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.3039 - accuracy: 0.1108\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.3040 - accuracy: 0.1104\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3042 - accuracy: 0.1100\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3042 - accuracy: 0.1095\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 2.3042 - accuracy: 0.1093\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3042 - accuracy: 0.1094\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3043 - accuracy: 0.1088\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3041 - accuracy: 0.1089\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3040 - accuracy: 0.1091\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 2.3040 - accuracy: 0.1089\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3039 - accuracy: 0.1091\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3039 - accuracy: 0.1090\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 2.3040 - accuracy: 0.1089\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3041 - accuracy: 0.1090\n",
            "205/469 [============>.................] - ETA: 7s - loss: 2.3040 - accuracy: 0.1090\n",
            "207/469 [============>.................] - ETA: 7s - loss: 2.3040 - accuracy: 0.1088\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3041 - accuracy: 0.1086\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3040 - accuracy: 0.1086\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3040 - accuracy: 0.1083\n",
            "221/469 [=============>................] - ETA: 7s - loss: 2.3041 - accuracy: 0.1086\n",
            "223/469 [=============>................] - ETA: 7s - loss: 2.3042 - accuracy: 0.1081\n",
            "227/469 [=============>................] - ETA: 7s - loss: 2.3041 - accuracy: 0.1081\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3040 - accuracy: 0.1078\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3041 - accuracy: 0.1076\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3040 - accuracy: 0.1078\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 2.3040 - accuracy: 0.1078\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 2.3042 - accuracy: 0.1074\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3040 - accuracy: 0.1076\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3041 - accuracy: 0.1076\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 2.3041 - accuracy: 0.1075\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3041 - accuracy: 0.1075\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 2.3041 - accuracy: 0.1075\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 2.3041 - accuracy: 0.1071\n",
            "267/469 [================>.............] - ETA: 6s - loss: 2.3042 - accuracy: 0.1069\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3041 - accuracy: 0.1069\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3042 - accuracy: 0.1065\n",
            "277/469 [================>.............] - ETA: 5s - loss: 2.3042 - accuracy: 0.1066\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3041 - accuracy: 0.1068\n",
            "285/469 [=================>............] - ETA: 5s - loss: 2.3042 - accuracy: 0.1066\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3042 - accuracy: 0.1065\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3042 - accuracy: 0.1062\n",
            "295/469 [=================>............] - ETA: 5s - loss: 2.3042 - accuracy: 0.1062\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3042 - accuracy: 0.1061\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 2.3042 - accuracy: 0.1057\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3042 - accuracy: 0.1061\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3041 - accuracy: 0.1062\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 2.3041 - accuracy: 0.1060\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 2.3041 - accuracy: 0.1060\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3041 - accuracy: 0.1058\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3042 - accuracy: 0.1058\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3042 - accuracy: 0.1059\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3041 - accuracy: 0.1059\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 2.3042 - accuracy: 0.1059\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3042 - accuracy: 0.1059\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.3041 - accuracy: 0.1060\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3042 - accuracy: 0.1058\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3042 - accuracy: 0.1058\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 2.3042 - accuracy: 0.1057\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3042 - accuracy: 0.1057\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3041 - accuracy: 0.1058\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.3041 - accuracy: 0.1059\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3042 - accuracy: 0.1059\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3042 - accuracy: 0.1060\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 2.3041 - accuracy: 0.1059\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.3041 - accuracy: 0.1061\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3041 - accuracy: 0.1061\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.3041 - accuracy: 0.1061\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3042 - accuracy: 0.1059\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3041 - accuracy: 0.1061\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 2.3042 - accuracy: 0.1060\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3042 - accuracy: 0.1058\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.3042 - accuracy: 0.1059\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3042 - accuracy: 0.1061\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3042 - accuracy: 0.1060\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3043 - accuracy: 0.1059\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3043 - accuracy: 0.1059\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.3044 - accuracy: 0.1058\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3044 - accuracy: 0.1055\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 2.3044 - accuracy: 0.1056\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3045 - accuracy: 0.1055\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3045 - accuracy: 0.1054\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3044 - accuracy: 0.1054\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3044 - accuracy: 0.1054\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.3045 - accuracy: 0.1054\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 2.3045 - accuracy: 0.1052\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3045 - accuracy: 0.1053\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3045 - accuracy: 0.1052\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3045 - accuracy: 0.1052\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3046 - accuracy: 0.1050\n",
            "455/469 [============================>.] - ETA: 0s - loss: 2.3045 - accuracy: 0.1051\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3045 - accuracy: 0.1052\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3044 - accuracy: 0.1055\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.3044 - accuracy: 0.1055\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3045 - accuracy: 0.1055\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3045 - accuracy: 0.1055 - val_loss: 2.3064 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 3/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 2.3097 - accuracy: 0.1172\n",
            "  7/469 [..............................] - ETA: 13s - loss: 2.3050 - accuracy: 0.1228\n",
            "  9/469 [..............................] - ETA: 13s - loss: 2.3070 - accuracy: 0.1085\n",
            " 13/469 [..............................] - ETA: 13s - loss: 2.3062 - accuracy: 0.1130\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3055 - accuracy: 0.1163\n",
            " 21/469 [>.............................] - ETA: 12s - loss: 2.3064 - accuracy: 0.1097\n",
            " 25/469 [>.............................] - ETA: 12s - loss: 2.3048 - accuracy: 0.1131\n",
            " 29/469 [>.............................] - ETA: 12s - loss: 2.3048 - accuracy: 0.1107\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 2.3057 - accuracy: 0.1103\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 2.3056 - accuracy: 0.1105\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 2.3057 - accuracy: 0.1102\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 2.3056 - accuracy: 0.1098\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 2.3059 - accuracy: 0.1077\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 2.3063 - accuracy: 0.1064\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 2.3063 - accuracy: 0.1063\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3064 - accuracy: 0.1063\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3064 - accuracy: 0.1046\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3065 - accuracy: 0.1035\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 2.3066 - accuracy: 0.1029\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 2.3063 - accuracy: 0.1029\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 2.3059 - accuracy: 0.1035\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3059 - accuracy: 0.1044\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 2.3056 - accuracy: 0.1044\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3057 - accuracy: 0.1045\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3061 - accuracy: 0.1042\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 2.3062 - accuracy: 0.1043\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 2.3061 - accuracy: 0.1044\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 2.3057 - accuracy: 0.1052\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 2.3055 - accuracy: 0.1041\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1050\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1054\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1047\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1041\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1033\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1036\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1034\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1042\n",
            "131/469 [=======>......................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1044 \n",
            "135/469 [=======>......................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1039\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3059 - accuracy: 0.1042\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1044\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1050\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1053\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1056\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1057\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1054\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1049\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3058 - accuracy: 0.1045\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1052\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1047\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1046\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1044\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1045\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1042\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1045\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1046\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1048\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1047\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3056 - accuracy: 0.1045\n",
            "205/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1049\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1050\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1049\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1046\n",
            "219/469 [=============>................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1046\n",
            "221/469 [=============>................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1044\n",
            "223/469 [=============>................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1046\n",
            "227/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1045\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1041\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3054 - accuracy: 0.1040\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 2.3054 - accuracy: 0.1038\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 2.3055 - accuracy: 0.1038\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 2.3055 - accuracy: 0.1038\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3055 - accuracy: 0.1034\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3054 - accuracy: 0.1034\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3055 - accuracy: 0.1034\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 2.3054 - accuracy: 0.1035\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 2.3055 - accuracy: 0.1035\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.3054 - accuracy: 0.1038\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3054 - accuracy: 0.1039\n",
            "273/469 [================>.............] - ETA: 5s - loss: 2.3053 - accuracy: 0.1040\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1041\n",
            "277/469 [================>.............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1043\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1045\n",
            "285/469 [=================>............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1044\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1040\n",
            "291/469 [=================>............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1038\n",
            "295/469 [=================>............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1035\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3053 - accuracy: 0.1033\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1033\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1031\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1029\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1032\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1030\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1028\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1029\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 2.3053 - accuracy: 0.1029\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 2.3053 - accuracy: 0.1030\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1033\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1033\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1033\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1034\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1035\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1035\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1039\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3051 - accuracy: 0.1041\n",
            "363/469 [======================>.......] - ETA: 3s - loss: 2.3051 - accuracy: 0.1041\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 2.3052 - accuracy: 0.1042\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1038\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1038\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1039\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1038\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1038\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1038\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.3052 - accuracy: 0.1036\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3052 - accuracy: 0.1036\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3052 - accuracy: 0.1035\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1034\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3053 - accuracy: 0.1034\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1033\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3053 - accuracy: 0.1033\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 2.3053 - accuracy: 0.1032\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1032\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1033\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1034\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1035\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1036\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 2.3051 - accuracy: 0.1038\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1038\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1040\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3052 - accuracy: 0.1040\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3052 - accuracy: 0.1039\n",
            "455/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1038\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1038\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1040\n",
            "467/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1041\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3050 - accuracy: 0.1041\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3050 - accuracy: 0.1041 - val_loss: 2.3058 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 4/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 2.3067 - accuracy: 0.1068\n",
            "  7/469 [..............................] - ETA: 13s - loss: 2.3094 - accuracy: 0.0971\n",
            "  9/469 [..............................] - ETA: 13s - loss: 2.3085 - accuracy: 0.0972\n",
            " 13/469 [..............................] - ETA: 13s - loss: 2.3057 - accuracy: 0.1070\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3050 - accuracy: 0.1075\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 2.3055 - accuracy: 0.1038\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 2.3048 - accuracy: 0.1025\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 2.3047 - accuracy: 0.1019\n",
            " 31/469 [>.............................] - ETA: 12s - loss: 2.3046 - accuracy: 0.1036\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 2.3040 - accuracy: 0.1049\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 2.3043 - accuracy: 0.1066\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 2.3043 - accuracy: 0.1088\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 2.3045 - accuracy: 0.1080\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 2.3042 - accuracy: 0.1085\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 2.3036 - accuracy: 0.1095\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3037 - accuracy: 0.1110\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3037 - accuracy: 0.1098\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3036 - accuracy: 0.1101\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 2.3038 - accuracy: 0.1100\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 2.3040 - accuracy: 0.1098\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 2.3040 - accuracy: 0.1084\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3042 - accuracy: 0.1072\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 2.3042 - accuracy: 0.1074\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3041 - accuracy: 0.1075\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3041 - accuracy: 0.1074\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 2.3040 - accuracy: 0.1076\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 2.3041 - accuracy: 0.1071\n",
            " 97/469 [=====>........................] - ETA: 10s - loss: 2.3043 - accuracy: 0.1066\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3043 - accuracy: 0.1064\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1057\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 2.3045 - accuracy: 0.1051\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1050\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 2.3047 - accuracy: 0.1039\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3047 - accuracy: 0.1042\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 2.3047 - accuracy: 0.1041\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 2.3048 - accuracy: 0.1031\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 2.3049 - accuracy: 0.1030\n",
            "133/469 [=======>......................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1023\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1023\n",
            "139/469 [=======>......................] - ETA: 9s - loss: 2.3047 - accuracy: 0.1030\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1030\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 2.3053 - accuracy: 0.1023\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 2.3052 - accuracy: 0.1016\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.3051 - accuracy: 0.1016\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 2.3050 - accuracy: 0.1014\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 2.3048 - accuracy: 0.1021\n",
            "165/469 [=========>....................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1025\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1029\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1031\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1035\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1039\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 2.3046 - accuracy: 0.1044\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 2.3044 - accuracy: 0.1046\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1043\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1045\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1044\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1042\n",
            "205/469 [============>.................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1040\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1040\n",
            "211/469 [============>.................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1043\n",
            "215/469 [============>.................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1044\n",
            "219/469 [=============>................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1046\n",
            "223/469 [=============>................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1049\n",
            "227/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1047\n",
            "229/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1049\n",
            "233/469 [=============>................] - ETA: 6s - loss: 2.3051 - accuracy: 0.1044\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1048\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1048\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1050\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1050\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1051\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1048\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1046\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1046\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1046\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1043\n",
            "269/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1042\n",
            "273/469 [================>.............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1043\n",
            "277/469 [================>.............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1046\n",
            "279/469 [================>.............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1047\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1044\n",
            "283/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1045\n",
            "287/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1046\n",
            "291/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1044\n",
            "295/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1044\n",
            "297/469 [=================>............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1045\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3050 - accuracy: 0.1045\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1047\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1046\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1049\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1052\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1054\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1053\n",
            "323/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1053\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1055\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1062\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3048 - accuracy: 0.1063\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 2.3048 - accuracy: 0.1065\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 2.3047 - accuracy: 0.1066\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 2.3048 - accuracy: 0.1066\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 2.3048 - accuracy: 0.1065\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3048 - accuracy: 0.1065\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 2.3048 - accuracy: 0.1066\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1064\n",
            "363/469 [======================>.......] - ETA: 3s - loss: 2.3050 - accuracy: 0.1061\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 2.3050 - accuracy: 0.1059\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.3049 - accuracy: 0.1063\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3048 - accuracy: 0.1062\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 2.3049 - accuracy: 0.1062\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 2.3049 - accuracy: 0.1062\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 2.3048 - accuracy: 0.1061\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 2.3048 - accuracy: 0.1061\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3048 - accuracy: 0.1062\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 2.3049 - accuracy: 0.1060\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 2.3049 - accuracy: 0.1059\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1057\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1057\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1057\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1057\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1056\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1055\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1057\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1057\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1058\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3048 - accuracy: 0.1059\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1057\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1056\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1057\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1059\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1060\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1059\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1058\n",
            "457/469 [============================>.] - ETA: 0s - loss: 2.3049 - accuracy: 0.1057\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3049 - accuracy: 0.1058\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3048 - accuracy: 0.1059\n",
            "467/469 [============================>.] - ETA: 0s - loss: 2.3048 - accuracy: 0.1061\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3048 - accuracy: 0.1061\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3048 - accuracy: 0.1061 - val_loss: 2.3062 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 5/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 2.2960 - accuracy: 0.1458\n",
            "  7/469 [..............................] - ETA: 13s - loss: 2.2967 - accuracy: 0.1306\n",
            " 11/469 [..............................] - ETA: 13s - loss: 2.3006 - accuracy: 0.1236\n",
            " 13/469 [..............................] - ETA: 13s - loss: 2.3020 - accuracy: 0.1220\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3013 - accuracy: 0.1199\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 2.3050 - accuracy: 0.1142\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 2.3055 - accuracy: 0.1103\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 2.3058 - accuracy: 0.1075\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 2.3058 - accuracy: 0.1076\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 2.3052 - accuracy: 0.1098\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 2.3048 - accuracy: 0.1094\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 2.3056 - accuracy: 0.1096\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 2.3054 - accuracy: 0.1085\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 2.3049 - accuracy: 0.1100\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3052 - accuracy: 0.1088\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3053 - accuracy: 0.1087\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3050 - accuracy: 0.1092\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 2.3050 - accuracy: 0.1087\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 2.3050 - accuracy: 0.1076\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 2.3049 - accuracy: 0.1075\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1064\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 2.3055 - accuracy: 0.1056\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3054 - accuracy: 0.1050\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1052\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 2.3052 - accuracy: 0.1049\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 2.3055 - accuracy: 0.1043\n",
            " 97/469 [=====>........................] - ETA: 10s - loss: 2.3054 - accuracy: 0.1042\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1044\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1038\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 2.3055 - accuracy: 0.1046\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 2.3054 - accuracy: 0.1047\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 2.3055 - accuracy: 0.1048\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3052 - accuracy: 0.1058\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3058 - accuracy: 0.1049\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 2.3058 - accuracy: 0.1044\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 2.3058 - accuracy: 0.1042\n",
            "133/469 [=======>......................] - ETA: 9s - loss: 2.3059 - accuracy: 0.1046\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1054\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1056\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1058\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1061\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1068\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.3055 - accuracy: 0.1071\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.3054 - accuracy: 0.1069\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3054 - accuracy: 0.1066\n",
            "165/469 [=========>....................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1067\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1071\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1072\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1075\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1070\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1069\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 2.3058 - accuracy: 0.1066\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 2.3059 - accuracy: 0.1064\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3061 - accuracy: 0.1061\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3060 - accuracy: 0.1056\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 2.3060 - accuracy: 0.1059\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 2.3061 - accuracy: 0.1055\n",
            "205/469 [============>.................] - ETA: 7s - loss: 2.3061 - accuracy: 0.1050\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3060 - accuracy: 0.1051\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3061 - accuracy: 0.1052\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3061 - accuracy: 0.1051\n",
            "219/469 [=============>................] - ETA: 7s - loss: 2.3062 - accuracy: 0.1050\n",
            "223/469 [=============>................] - ETA: 7s - loss: 2.3062 - accuracy: 0.1051\n",
            "227/469 [=============>................] - ETA: 7s - loss: 2.3062 - accuracy: 0.1050\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3062 - accuracy: 0.1048\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3062 - accuracy: 0.1048\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 2.3063 - accuracy: 0.1047\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3062 - accuracy: 0.1049\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 2.3062 - accuracy: 0.1048\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 2.3060 - accuracy: 0.1053\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3061 - accuracy: 0.1053\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3062 - accuracy: 0.1054\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 2.3061 - accuracy: 0.1055\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3061 - accuracy: 0.1055\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 2.3061 - accuracy: 0.1055\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 2.3060 - accuracy: 0.1056\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.3059 - accuracy: 0.1055\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3061 - accuracy: 0.1055\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3062 - accuracy: 0.1051\n",
            "277/469 [================>.............] - ETA: 5s - loss: 2.3061 - accuracy: 0.1051\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3061 - accuracy: 0.1048\n",
            "285/469 [=================>............] - ETA: 5s - loss: 2.3062 - accuracy: 0.1050\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3061 - accuracy: 0.1050\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3061 - accuracy: 0.1052\n",
            "297/469 [=================>............] - ETA: 5s - loss: 2.3062 - accuracy: 0.1052\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3062 - accuracy: 0.1052\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 2.3061 - accuracy: 0.1054\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3060 - accuracy: 0.1054\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3060 - accuracy: 0.1056\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 2.3061 - accuracy: 0.1054\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3061 - accuracy: 0.1054\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3061 - accuracy: 0.1052\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3061 - accuracy: 0.1052\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3062 - accuracy: 0.1053\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 2.3062 - accuracy: 0.1053\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3061 - accuracy: 0.1052\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.3061 - accuracy: 0.1051\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3061 - accuracy: 0.1050\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3061 - accuracy: 0.1050\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3061 - accuracy: 0.1050\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3060 - accuracy: 0.1052\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.3060 - accuracy: 0.1052\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3060 - accuracy: 0.1052\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3059 - accuracy: 0.1054\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 2.3059 - accuracy: 0.1056\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3059 - accuracy: 0.1057\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.3060 - accuracy: 0.1056\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3059 - accuracy: 0.1059\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3060 - accuracy: 0.1059\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 2.3059 - accuracy: 0.1060\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3060 - accuracy: 0.1059\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.3060 - accuracy: 0.1059\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3059 - accuracy: 0.1059\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3059 - accuracy: 0.1059\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3058 - accuracy: 0.1061\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3058 - accuracy: 0.1061\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.3059 - accuracy: 0.1062\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3059 - accuracy: 0.1062\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 2.3058 - accuracy: 0.1063\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3058 - accuracy: 0.1062\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3058 - accuracy: 0.1063\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3058 - accuracy: 0.1061\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.3057 - accuracy: 0.1062\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 2.3057 - accuracy: 0.1062\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 2.3057 - accuracy: 0.1061\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 2.3057 - accuracy: 0.1060\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3057 - accuracy: 0.1061\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3056 - accuracy: 0.1065\n",
            "455/469 [============================>.] - ETA: 0s - loss: 2.3056 - accuracy: 0.1066\n",
            "457/469 [============================>.] - ETA: 0s - loss: 2.3056 - accuracy: 0.1065\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3056 - accuracy: 0.1067\n",
            "461/469 [============================>.] - ETA: 0s - loss: 2.3056 - accuracy: 0.1067\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.3056 - accuracy: 0.1067\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3056 - accuracy: 0.1067\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3056 - accuracy: 0.1067 - val_loss: 2.3055 - val_accuracy: 0.1009\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 6/12\n",
            "  1/469 [..............................] - ETA: 14s - loss: 2.3147 - accuracy: 0.0938\n",
            "  5/469 [..............................] - ETA: 13s - loss: 2.3078 - accuracy: 0.0906\n",
            "  9/469 [..............................] - ETA: 13s - loss: 2.3084 - accuracy: 0.0920\n",
            " 13/469 [..............................] - ETA: 13s - loss: 2.3073 - accuracy: 0.0986\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3056 - accuracy: 0.1002\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 2.3041 - accuracy: 0.1053\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 2.3052 - accuracy: 0.1019\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 2.3059 - accuracy: 0.1039\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 2.3066 - accuracy: 0.1061\n",
            " 33/469 [=>............................] - ETA: 13s - loss: 2.3064 - accuracy: 0.1065\n",
            " 35/469 [=>............................] - ETA: 13s - loss: 2.3063 - accuracy: 0.1060\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 2.3060 - accuracy: 0.1066\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 2.3061 - accuracy: 0.1071\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 2.3064 - accuracy: 0.1061\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 2.3059 - accuracy: 0.1079\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3059 - accuracy: 0.1069\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 2.3056 - accuracy: 0.1081\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3055 - accuracy: 0.1076\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 2.3053 - accuracy: 0.1070\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 2.3052 - accuracy: 0.1085\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 2.3046 - accuracy: 0.1105\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 2.3046 - accuracy: 0.1111\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3040 - accuracy: 0.1130\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 2.3042 - accuracy: 0.1128\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 2.3047 - accuracy: 0.1126\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1126\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 2.3046 - accuracy: 0.1131\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 2.3046 - accuracy: 0.1126\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 2.3047 - accuracy: 0.1123\n",
            " 99/469 [=====>........................] - ETA: 10s - loss: 2.3046 - accuracy: 0.1114\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 2.3046 - accuracy: 0.1113\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 2.3041 - accuracy: 0.1112\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 2.3042 - accuracy: 0.1111\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 2.3045 - accuracy: 0.1110\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 2.3046 - accuracy: 0.1107\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 2.3047 - accuracy: 0.1108\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1111\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 2.3045 - accuracy: 0.1106\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1105\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1106\n",
            "135/469 [=======>......................] - ETA: 9s - loss: 2.3045 - accuracy: 0.1101\n",
            "139/469 [=======>......................] - ETA: 9s - loss: 2.3046 - accuracy: 0.1100\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 2.3048 - accuracy: 0.1094\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3048 - accuracy: 0.1094\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3048 - accuracy: 0.1084\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1085\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 2.3050 - accuracy: 0.1087\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1090\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1089\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1092\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1089\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1085\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1084\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1084\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3046 - accuracy: 0.1088\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3046 - accuracy: 0.1089\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 2.3046 - accuracy: 0.1091\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1090\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1087\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1088\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1087\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1086\n",
            "207/469 [============>.................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1081\n",
            "211/469 [============>.................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1079\n",
            "215/469 [============>.................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1076\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1076\n",
            "221/469 [=============>................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1078\n",
            "225/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1078\n",
            "229/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1080\n",
            "233/469 [=============>................] - ETA: 7s - loss: 2.3051 - accuracy: 0.1084\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1082\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1085\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1089\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1091\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1090\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1091\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1092\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1093\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1090\n",
            "269/469 [================>.............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1088\n",
            "273/469 [================>.............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1088\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1088\n",
            "279/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1083\n",
            "283/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1084\n",
            "287/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1083\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1082\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1083\n",
            "297/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1083\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1083\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1081\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1083\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1084\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1083\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1083\n",
            "323/469 [===================>..........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1082\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1080\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1081\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1082\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1080\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1080\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1081\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1079\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1077\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 2.3050 - accuracy: 0.1077\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 2.3050 - accuracy: 0.1076\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3051 - accuracy: 0.1075\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3051 - accuracy: 0.1072\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1070\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1068\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1065\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1063\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1062\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1064\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3052 - accuracy: 0.1063\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1065\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1067\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1068\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1068\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1069\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1071\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1071\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 2.3050 - accuracy: 0.1071\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1071\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1071\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1071\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 2.3050 - accuracy: 0.1070\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 2.3050 - accuracy: 0.1069\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1069\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1068\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1067\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1068\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1067\n",
            "455/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1067\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1067\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1065\n",
            "467/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1064\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3051 - accuracy: 0.1064\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3051 - accuracy: 0.1064 - val_loss: 2.3075 - val_accuracy: 0.0982\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 7/12\n",
            "  3/469 [..............................] - ETA: 14s - loss: 2.3000 - accuracy: 0.1172\n",
            "  7/469 [..............................] - ETA: 13s - loss: 2.3073 - accuracy: 0.1027\n",
            " 11/469 [..............................] - ETA: 13s - loss: 2.3056 - accuracy: 0.1087\n",
            " 15/469 [..............................] - ETA: 13s - loss: 2.3069 - accuracy: 0.1109\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3081 - accuracy: 0.1066\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 2.3076 - accuracy: 0.1073\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1057\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1078\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 2.3071 - accuracy: 0.1118\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 2.3065 - accuracy: 0.1146\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 2.3073 - accuracy: 0.1129\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 2.3071 - accuracy: 0.1122\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 2.3061 - accuracy: 0.1132\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 2.3063 - accuracy: 0.1127\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 2.3061 - accuracy: 0.1126\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3061 - accuracy: 0.1126\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3062 - accuracy: 0.1114\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3057 - accuracy: 0.1105\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1115\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1121\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 2.3056 - accuracy: 0.1113\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3057 - accuracy: 0.1114\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 2.3060 - accuracy: 0.1112\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3059 - accuracy: 0.1110\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 2.3058 - accuracy: 0.1101\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 2.3055 - accuracy: 0.1096\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 2.3060 - accuracy: 0.1084\n",
            " 97/469 [=====>........................] - ETA: 10s - loss: 2.3061 - accuracy: 0.1078\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3058 - accuracy: 0.1083\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1092\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1086\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 2.3059 - accuracy: 0.1081\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 2.3062 - accuracy: 0.1078\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3061 - accuracy: 0.1072\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3061 - accuracy: 0.1071\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 2.3059 - accuracy: 0.1065\n",
            "131/469 [=======>......................] - ETA: 9s - loss: 2.3060 - accuracy: 0.1068 \n",
            "135/469 [=======>......................] - ETA: 9s - loss: 2.3060 - accuracy: 0.1065\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3061 - accuracy: 0.1064\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1077\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1082\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1082\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1081\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1080\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1081\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3058 - accuracy: 0.1079\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3058 - accuracy: 0.1077\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 2.3058 - accuracy: 0.1074\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1076\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1077\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3058 - accuracy: 0.1073\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1073\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1072\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1068\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1064\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1065\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 2.3057 - accuracy: 0.1064\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3056 - accuracy: 0.1063\n",
            "207/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1064\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3056 - accuracy: 0.1062\n",
            "211/469 [============>.................] - ETA: 7s - loss: 2.3056 - accuracy: 0.1063\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1064\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1065\n",
            "221/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1070\n",
            "225/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1072\n",
            "229/469 [=============>................] - ETA: 7s - loss: 2.3052 - accuracy: 0.1074\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3051 - accuracy: 0.1074\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1074\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1073\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1072\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1073\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1074\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1076\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1076\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1078\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1078\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1076\n",
            "269/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1078\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1077\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1075\n",
            "279/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1074\n",
            "283/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1075\n",
            "287/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1079\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1081\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1084\n",
            "297/469 [=================>............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1087\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1089\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1090\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1090\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1092\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1091\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1091\n",
            "323/469 [===================>..........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1088\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1089\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1089\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1088\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 2.3050 - accuracy: 0.1086\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 2.3050 - accuracy: 0.1086\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3050 - accuracy: 0.1086\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1087\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3050 - accuracy: 0.1086\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1086\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1086\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3050 - accuracy: 0.1086\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3049 - accuracy: 0.1086\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1085\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1083\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1083\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1081\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3049 - accuracy: 0.1080\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 2.3049 - accuracy: 0.1080\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3049 - accuracy: 0.1079\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 2.3049 - accuracy: 0.1081\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3048 - accuracy: 0.1082\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3048 - accuracy: 0.1081\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3048 - accuracy: 0.1083\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1083\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 2.3048 - accuracy: 0.1084\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3048 - accuracy: 0.1083\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 2.3048 - accuracy: 0.1083\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1082\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1082\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1081\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1081\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1080\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 2.3049 - accuracy: 0.1079\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1078\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1076\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1076\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3049 - accuracy: 0.1075\n",
            "455/469 [============================>.] - ETA: 0s - loss: 2.3049 - accuracy: 0.1074\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3049 - accuracy: 0.1075\n",
            "461/469 [============================>.] - ETA: 0s - loss: 2.3049 - accuracy: 0.1075\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3049 - accuracy: 0.1075\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.3049 - accuracy: 0.1073\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3050 - accuracy: 0.1072\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3050 - accuracy: 0.1072 - val_loss: 2.3035 - val_accuracy: 0.0958\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 8/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 2.3013 - accuracy: 0.1016\n",
            "  7/469 [..............................] - ETA: 13s - loss: 2.3034 - accuracy: 0.1060\n",
            " 11/469 [..............................] - ETA: 13s - loss: 2.3040 - accuracy: 0.1037\n",
            " 13/469 [..............................] - ETA: 13s - loss: 2.3028 - accuracy: 0.1022\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3040 - accuracy: 0.1002\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 2.3054 - accuracy: 0.0990\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 2.3056 - accuracy: 0.1009\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 2.3055 - accuracy: 0.1016\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 2.3054 - accuracy: 0.1008\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 2.3056 - accuracy: 0.1003\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 2.3061 - accuracy: 0.1000\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 2.3059 - accuracy: 0.0988\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 2.3065 - accuracy: 0.0972\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 2.3064 - accuracy: 0.0976\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 2.3061 - accuracy: 0.0981\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3056 - accuracy: 0.0982\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3058 - accuracy: 0.0983\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3054 - accuracy: 0.0987\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 2.3054 - accuracy: 0.1001\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 2.3056 - accuracy: 0.1005\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 2.3057 - accuracy: 0.1014\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 2.3056 - accuracy: 0.1023\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3058 - accuracy: 0.1029\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 2.3058 - accuracy: 0.1040\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3058 - accuracy: 0.1038\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3057 - accuracy: 0.1034\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 2.3057 - accuracy: 0.1031\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 2.3056 - accuracy: 0.1041\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 2.3055 - accuracy: 0.1041\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3054 - accuracy: 0.1044\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1041\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 2.3053 - accuracy: 0.1046\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 2.3054 - accuracy: 0.1049\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1046\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3053 - accuracy: 0.1043\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3053 - accuracy: 0.1046\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 2.3055 - accuracy: 0.1044\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 2.3055 - accuracy: 0.1041\n",
            "133/469 [=======>......................] - ETA: 9s - loss: 2.3054 - accuracy: 0.1043\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3052 - accuracy: 0.1045\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.3053 - accuracy: 0.1041\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3052 - accuracy: 0.1043\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 2.3052 - accuracy: 0.1043\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3052 - accuracy: 0.1044\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 2.3053 - accuracy: 0.1045\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.3055 - accuracy: 0.1047\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.3054 - accuracy: 0.1056\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3054 - accuracy: 0.1055\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1057\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1054\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1052\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1052\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1051\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1055\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1054\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3053 - accuracy: 0.1057\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3052 - accuracy: 0.1060\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 2.3051 - accuracy: 0.1065\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3051 - accuracy: 0.1070\n",
            "205/469 [============>.................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1067\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1068\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1071\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1068\n",
            "221/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1071\n",
            "223/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1070\n",
            "225/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1071\n",
            "227/469 [=============>................] - ETA: 7s - loss: 2.3052 - accuracy: 0.1072\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3052 - accuracy: 0.1070\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1068\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1073\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1068\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1065\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1069\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1071\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1073\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1072\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1074\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1074\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1074\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1074\n",
            "279/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1074\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1074\n",
            "285/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1071\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1073\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1069\n",
            "297/469 [=================>............] - ETA: 5s - loss: 2.3052 - accuracy: 0.1068\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3052 - accuracy: 0.1067\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1069\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1070\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1072\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1071\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1070\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1072\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1075\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1073\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1072\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1073\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1071\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1071\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1070\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1068\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1069\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1070\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1068\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3052 - accuracy: 0.1064\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3051 - accuracy: 0.1064\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1063\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1062\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1063\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1064\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1065\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1067\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1066\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1066\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3049 - accuracy: 0.1068\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.3050 - accuracy: 0.1067\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1067\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1066\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1065\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1065\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1064\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1063\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1064\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1065\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1065\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1064\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1064\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1066\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 2.3051 - accuracy: 0.1068\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3052 - accuracy: 0.1069\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3052 - accuracy: 0.1068\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3052 - accuracy: 0.1067\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1068\n",
            "455/469 [============================>.] - ETA: 0s - loss: 2.3052 - accuracy: 0.1066\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3052 - accuracy: 0.1064\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3052 - accuracy: 0.1063\n",
            "467/469 [============================>.] - ETA: 0s - loss: 2.3052 - accuracy: 0.1061\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3052 - accuracy: 0.1061\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3052 - accuracy: 0.1061 - val_loss: 2.3069 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 9/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 2.3036 - accuracy: 0.1224\n",
            "  7/469 [..............................] - ETA: 13s - loss: 2.3053 - accuracy: 0.1217\n",
            " 11/469 [..............................] - ETA: 13s - loss: 2.3069 - accuracy: 0.1172\n",
            " 15/469 [..............................] - ETA: 13s - loss: 2.3074 - accuracy: 0.1141\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3061 - accuracy: 0.1167\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 2.3065 - accuracy: 0.1135\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 2.3058 - accuracy: 0.1112\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 2.3063 - accuracy: 0.1119\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 2.3070 - accuracy: 0.1102\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 2.3068 - accuracy: 0.1101\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 2.3066 - accuracy: 0.1100\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 2.3066 - accuracy: 0.1094\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 2.3066 - accuracy: 0.1079\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 2.3066 - accuracy: 0.1085\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 2.3064 - accuracy: 0.1083\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 2.3063 - accuracy: 0.1085\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3063 - accuracy: 0.1079\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3061 - accuracy: 0.1082\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 2.3058 - accuracy: 0.1081\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 2.3056 - accuracy: 0.1086\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 2.3058 - accuracy: 0.1092\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3056 - accuracy: 0.1094\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 2.3056 - accuracy: 0.1096\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3054 - accuracy: 0.1098\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 2.3052 - accuracy: 0.1095\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1107\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 2.3050 - accuracy: 0.1114\n",
            " 97/469 [=====>........................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1115\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3051 - accuracy: 0.1111\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 2.3052 - accuracy: 0.1111\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 2.3052 - accuracy: 0.1101\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 2.3051 - accuracy: 0.1105\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1108\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1104\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1101\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1096\n",
            "131/469 [=======>......................] - ETA: 9s - loss: 2.3052 - accuracy: 0.1088 \n",
            "135/469 [=======>......................] - ETA: 9s - loss: 2.3051 - accuracy: 0.1084\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3051 - accuracy: 0.1082\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.3052 - accuracy: 0.1074\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3055 - accuracy: 0.1069\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1070\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1071\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1074\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1076\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1075\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1079\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1085\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1084\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1083\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1080\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1079\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1082\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1080\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1079\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1080\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1074\n",
            "205/469 [============>.................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1072\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1067\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1064\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1060\n",
            "221/469 [=============>................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1060\n",
            "225/469 [=============>................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1060\n",
            "229/469 [=============>................] - ETA: 7s - loss: 2.3056 - accuracy: 0.1058\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3058 - accuracy: 0.1057\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3058 - accuracy: 0.1055\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3059 - accuracy: 0.1052\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.3059 - accuracy: 0.1050\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 2.3058 - accuracy: 0.1050\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3057 - accuracy: 0.1049\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3056 - accuracy: 0.1051\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3056 - accuracy: 0.1052\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 2.3056 - accuracy: 0.1051\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 2.3055 - accuracy: 0.1049\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.3055 - accuracy: 0.1052\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3056 - accuracy: 0.1049\n",
            "273/469 [================>.............] - ETA: 5s - loss: 2.3055 - accuracy: 0.1053\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3054 - accuracy: 0.1056\n",
            "277/469 [================>.............] - ETA: 5s - loss: 2.3054 - accuracy: 0.1057\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3053 - accuracy: 0.1058\n",
            "285/469 [=================>............] - ETA: 5s - loss: 2.3054 - accuracy: 0.1057\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3054 - accuracy: 0.1058\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3054 - accuracy: 0.1056\n",
            "295/469 [=================>............] - ETA: 5s - loss: 2.3053 - accuracy: 0.1057\n",
            "297/469 [=================>............] - ETA: 5s - loss: 2.3053 - accuracy: 0.1061\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3053 - accuracy: 0.1061\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 2.3053 - accuracy: 0.1060\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3053 - accuracy: 0.1063\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3053 - accuracy: 0.1063\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1066\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1067\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1069\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1069\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1067\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1068\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1065\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1066\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1065\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1065\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1066\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1067\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1067\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1066\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3051 - accuracy: 0.1070\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3051 - accuracy: 0.1070\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1072\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1071\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1075\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1073\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1073\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1072\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 2.3050 - accuracy: 0.1074\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.3050 - accuracy: 0.1074\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1074\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1073\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3051 - accuracy: 0.1072\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1071\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1072\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.3050 - accuracy: 0.1072\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3050 - accuracy: 0.1074\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1072\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1070\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1072\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3050 - accuracy: 0.1074\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.3050 - accuracy: 0.1077\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 2.3050 - accuracy: 0.1077\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1076\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1076\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1076\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1076\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1076\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1076\n",
            "457/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1076\n",
            "461/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1075\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1075\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3051 - accuracy: 0.1074\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3051 - accuracy: 0.1074 - val_loss: 2.3034 - val_accuracy: 0.1010\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 10/12\n",
            "  1/469 [..............................] - ETA: 14s - loss: 2.2959 - accuracy: 0.1250\n",
            "  5/469 [..............................] - ETA: 14s - loss: 2.3024 - accuracy: 0.0984\n",
            "  9/469 [..............................] - ETA: 13s - loss: 2.3021 - accuracy: 0.0955\n",
            " 11/469 [..............................] - ETA: 13s - loss: 2.3022 - accuracy: 0.0916\n",
            " 13/469 [..............................] - ETA: 13s - loss: 2.3035 - accuracy: 0.0913\n",
            " 15/469 [..............................] - ETA: 13s - loss: 2.3027 - accuracy: 0.0969\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3021 - accuracy: 0.0979\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 2.3022 - accuracy: 0.1024\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 2.3038 - accuracy: 0.1026\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 2.3035 - accuracy: 0.1068\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 2.3037 - accuracy: 0.1069\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 2.3031 - accuracy: 0.1078\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 2.3036 - accuracy: 0.1083\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 2.3033 - accuracy: 0.1096\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 2.3032 - accuracy: 0.1087\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 2.3031 - accuracy: 0.1099\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3037 - accuracy: 0.1073\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 2.3041 - accuracy: 0.1057\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 2.3046 - accuracy: 0.1039\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 2.3045 - accuracy: 0.1044\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1046\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 2.3043 - accuracy: 0.1059\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1055\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1063\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 2.3042 - accuracy: 0.1060\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3043 - accuracy: 0.1055\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1065\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1066\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1067\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 2.3043 - accuracy: 0.1068\n",
            " 99/469 [=====>........................] - ETA: 10s - loss: 2.3049 - accuracy: 0.1057\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 2.3049 - accuracy: 0.1057\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1051\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 2.3049 - accuracy: 0.1051\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1049\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1050\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 2.3050 - accuracy: 0.1044\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 2.3051 - accuracy: 0.1045\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 2.3051 - accuracy: 0.1044\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 2.3051 - accuracy: 0.1045\n",
            "131/469 [=======>......................] - ETA: 9s - loss: 2.3050 - accuracy: 0.1045 \n",
            "135/469 [=======>......................] - ETA: 9s - loss: 2.3050 - accuracy: 0.1045\n",
            "139/469 [=======>......................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1045\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 2.3050 - accuracy: 0.1045\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3051 - accuracy: 0.1041\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3050 - accuracy: 0.1042\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.3048 - accuracy: 0.1042\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1042\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1045\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3049 - accuracy: 0.1047\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 2.3048 - accuracy: 0.1050\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1053\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1060\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1061\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1063\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1065\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1066\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1065\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1069\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1069\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1067\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1067\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3046 - accuracy: 0.1066\n",
            "207/469 [============>.................] - ETA: 7s - loss: 2.3046 - accuracy: 0.1059\n",
            "211/469 [============>.................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1060\n",
            "215/469 [============>.................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1058\n",
            "219/469 [=============>................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1057\n",
            "223/469 [=============>................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1057\n",
            "225/469 [=============>................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1061\n",
            "229/469 [=============>................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1062\n",
            "233/469 [=============>................] - ETA: 6s - loss: 2.3048 - accuracy: 0.1066\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1060\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1057\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1053\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1053\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1055\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1054\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1054\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1053\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1054\n",
            "269/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1055\n",
            "273/469 [================>.............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1057\n",
            "277/469 [================>.............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1059\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1057\n",
            "283/469 [=================>............] - ETA: 5s - loss: 2.3048 - accuracy: 0.1055\n",
            "287/469 [=================>............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1056\n",
            "291/469 [=================>............] - ETA: 5s - loss: 2.3048 - accuracy: 0.1053\n",
            "295/469 [=================>............] - ETA: 5s - loss: 2.3048 - accuracy: 0.1053\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3048 - accuracy: 0.1051\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1052\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 2.3047 - accuracy: 0.1053\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 2.3047 - accuracy: 0.1052\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1053\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3048 - accuracy: 0.1053\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1051\n",
            "323/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1052\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1051\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 2.3049 - accuracy: 0.1048\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1045\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1044\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1044\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1044\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1045\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1046\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 2.3049 - accuracy: 0.1046\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 2.3048 - accuracy: 0.1048\n",
            "363/469 [======================>.......] - ETA: 3s - loss: 2.3048 - accuracy: 0.1048\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 2.3047 - accuracy: 0.1049\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.3047 - accuracy: 0.1049\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3047 - accuracy: 0.1049\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 2.3048 - accuracy: 0.1052\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 2.3048 - accuracy: 0.1051\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 2.3048 - accuracy: 0.1052\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 2.3048 - accuracy: 0.1051\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.3048 - accuracy: 0.1053\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 2.3049 - accuracy: 0.1051\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 2.3049 - accuracy: 0.1050\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1050\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1049\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1052\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 2.3049 - accuracy: 0.1052\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 2.3048 - accuracy: 0.1054\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 2.3048 - accuracy: 0.1051\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3049 - accuracy: 0.1052\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3048 - accuracy: 0.1053\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 2.3048 - accuracy: 0.1054\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 2.3048 - accuracy: 0.1056\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 2.3047 - accuracy: 0.1058\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 2.3047 - accuracy: 0.1058\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3048 - accuracy: 0.1058\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 2.3047 - accuracy: 0.1058\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 2.3047 - accuracy: 0.1059\n",
            "457/469 [============================>.] - ETA: 0s - loss: 2.3047 - accuracy: 0.1061\n",
            "461/469 [============================>.] - ETA: 0s - loss: 2.3047 - accuracy: 0.1063\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.3048 - accuracy: 0.1060\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3048 - accuracy: 0.1062\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3048 - accuracy: 0.1062 - val_loss: 2.3034 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 11/12\n",
            "  1/469 [..............................] - ETA: 13s - loss: 2.3096 - accuracy: 0.0859\n",
            "  5/469 [..............................] - ETA: 13s - loss: 2.3086 - accuracy: 0.0938\n",
            "  9/469 [..............................] - ETA: 13s - loss: 2.3047 - accuracy: 0.1033\n",
            " 11/469 [..............................] - ETA: 13s - loss: 2.3034 - accuracy: 0.1030\n",
            " 15/469 [..............................] - ETA: 13s - loss: 2.3045 - accuracy: 0.0964\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 2.3053 - accuracy: 0.0975\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 2.3044 - accuracy: 0.0971\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 2.3037 - accuracy: 0.0958\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 2.3033 - accuracy: 0.0975\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 2.3031 - accuracy: 0.0990\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 2.3035 - accuracy: 0.0971\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 2.3041 - accuracy: 0.0963\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 2.3041 - accuracy: 0.0993\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 2.3044 - accuracy: 0.0983\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 2.3048 - accuracy: 0.0965\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3048 - accuracy: 0.0961\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 2.3047 - accuracy: 0.0956\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 2.3046 - accuracy: 0.0965\n",
            " 63/469 [===>..........................] - ETA: 11s - loss: 2.3046 - accuracy: 0.0980\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.0995\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1003\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1016\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 2.3041 - accuracy: 0.1033\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 2.3046 - accuracy: 0.1031\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1031\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 2.3041 - accuracy: 0.1036\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 2.3048 - accuracy: 0.1036\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1035\n",
            " 99/469 [=====>........................] - ETA: 10s - loss: 2.3047 - accuracy: 0.1030\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 2.3045 - accuracy: 0.1029\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1026\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 2.3045 - accuracy: 0.1020\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1016\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 2.3044 - accuracy: 0.1023\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 2.3047 - accuracy: 0.1023\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3047 - accuracy: 0.1023\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 2.3046 - accuracy: 0.1033\n",
            "131/469 [=======>......................] - ETA: 9s - loss: 2.3047 - accuracy: 0.1033 \n",
            "135/469 [=======>......................] - ETA: 9s - loss: 2.3047 - accuracy: 0.1032\n",
            "139/469 [=======>......................] - ETA: 9s - loss: 2.3046 - accuracy: 0.1038\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 2.3047 - accuracy: 0.1033\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.3047 - accuracy: 0.1030\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.3047 - accuracy: 0.1031\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.3046 - accuracy: 0.1037\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 2.3046 - accuracy: 0.1039\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 2.3047 - accuracy: 0.1036\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.3048 - accuracy: 0.1038\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1040\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 2.3046 - accuracy: 0.1046\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 2.3047 - accuracy: 0.1046\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3048 - accuracy: 0.1044\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1042\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1045\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1043\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1043\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1044\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3049 - accuracy: 0.1046\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1051\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1052\n",
            "207/469 [============>.................] - ETA: 7s - loss: 2.3047 - accuracy: 0.1054\n",
            "211/469 [============>.................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1055\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3048 - accuracy: 0.1055\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1051\n",
            "221/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1053\n",
            "225/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1048\n",
            "229/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1051\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3049 - accuracy: 0.1052\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3049 - accuracy: 0.1050\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1046\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.3048 - accuracy: 0.1052\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 2.3047 - accuracy: 0.1055\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3048 - accuracy: 0.1055\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3048 - accuracy: 0.1057\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1054\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1051\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1050\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 2.3050 - accuracy: 0.1047\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1050\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1049\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1049\n",
            "279/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1049\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1051\n",
            "285/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1054\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3049 - accuracy: 0.1057\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1055\n",
            "295/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1055\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3051 - accuracy: 0.1051\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1051\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1052\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1050\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1047\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1044\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1045\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1042\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1045\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1044\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1046\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1043\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1044\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1042\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1041\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1042\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1041\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3053 - accuracy: 0.1042\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3052 - accuracy: 0.1040\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1042\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1042\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1045\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1044\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1046\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1045\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1044\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1045\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3052 - accuracy: 0.1045\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 2.3052 - accuracy: 0.1043\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 2.3053 - accuracy: 0.1044\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 2.3054 - accuracy: 0.1045\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3053 - accuracy: 0.1046\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1050\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 2.3053 - accuracy: 0.1049\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1051\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 2.3052 - accuracy: 0.1054\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1057\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1059\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1058\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1059\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 2.3052 - accuracy: 0.1060\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 2.3051 - accuracy: 0.1061\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1060\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1059\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1060\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1058\n",
            "455/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1056\n",
            "459/469 [============================>.] - ETA: 0s - loss: 2.3051 - accuracy: 0.1055\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3052 - accuracy: 0.1054\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.3052 - accuracy: 0.1055\n",
            "469/469 [==============================] - ETA: 0s - loss: 2.3052 - accuracy: 0.1054\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 2.3052 - accuracy: 0.1054 - val_loss: 2.3041 - val_accuracy: 0.1028\n",
            "\u001b[36m(train_mnist pid=3295966)\u001b[0m Epoch 12/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 2.2981 - accuracy: 0.1380\n",
            "  7/469 [..............................] - ETA: 13s - loss: 2.3002 - accuracy: 0.1261\n",
            " 11/469 [..............................] - ETA: 13s - loss: 2.3012 - accuracy: 0.1214\n",
            " 15/469 [..............................] - ETA: 13s - loss: 2.2990 - accuracy: 0.1193\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 2.3017 - accuracy: 0.1158\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 2.3030 - accuracy: 0.1109\n",
            " 25/469 [>.............................] - ETA: 12s - loss: 2.3039 - accuracy: 0.1094\n",
            " 29/469 [>.............................] - ETA: 12s - loss: 2.3036 - accuracy: 0.1110\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 2.3037 - accuracy: 0.1108\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 2.3038 - accuracy: 0.1109\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 2.3041 - accuracy: 0.1068\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 2.3040 - accuracy: 0.1056\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 2.3039 - accuracy: 0.1051\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 2.3041 - accuracy: 0.1036\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 2.3043 - accuracy: 0.1039\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 2.3045 - accuracy: 0.1024\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 2.3046 - accuracy: 0.1013\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 2.3046 - accuracy: 0.1027\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 2.3044 - accuracy: 0.1037\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 2.3039 - accuracy: 0.1051\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 2.3046 - accuracy: 0.1048\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 2.3049 - accuracy: 0.1042\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 2.3045 - accuracy: 0.1048\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 2.3050 - accuracy: 0.1039\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1033\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 2.3052 - accuracy: 0.1038\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 2.3051 - accuracy: 0.1036\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 2.3050 - accuracy: 0.1033\n",
            " 97/469 [=====>........................] - ETA: 10s - loss: 2.3051 - accuracy: 0.1042\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 2.3053 - accuracy: 0.1043\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 2.3054 - accuracy: 0.1040\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 2.3054 - accuracy: 0.1043\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1043\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 2.3058 - accuracy: 0.1043\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 2.3057 - accuracy: 0.1043\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1042\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1039\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 2.3056 - accuracy: 0.1038\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 2.3059 - accuracy: 0.1032\n",
            "133/469 [=======>......................] - ETA: 9s - loss: 2.3061 - accuracy: 0.1031 \n",
            "137/469 [=======>......................] - ETA: 9s - loss: 2.3060 - accuracy: 0.1029\n",
            "139/469 [=======>......................] - ETA: 9s - loss: 2.3059 - accuracy: 0.1026\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.3060 - accuracy: 0.1026\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 2.3059 - accuracy: 0.1024\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 2.3056 - accuracy: 0.1033\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1034\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.3055 - accuracy: 0.1035\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1032\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 2.3057 - accuracy: 0.1033\n",
            "165/469 [=========>....................] - ETA: 8s - loss: 2.3057 - accuracy: 0.1035\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1034\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1040\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.3053 - accuracy: 0.1043\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1035\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1035\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1033\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.3054 - accuracy: 0.1032\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.3055 - accuracy: 0.1028\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 2.3056 - accuracy: 0.1027\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1032\n",
            "205/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1032\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.3055 - accuracy: 0.1033\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1039\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1044\n",
            "219/469 [=============>................] - ETA: 7s - loss: 2.3052 - accuracy: 0.1045\n",
            "223/469 [=============>................] - ETA: 7s - loss: 2.3054 - accuracy: 0.1042\n",
            "227/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1047\n",
            "231/469 [=============>................] - ETA: 7s - loss: 2.3053 - accuracy: 0.1048\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.3053 - accuracy: 0.1050\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1048\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1048\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1052\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1053\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 2.3052 - accuracy: 0.1053\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1054\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1055\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 2.3051 - accuracy: 0.1056\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1058\n",
            "271/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1056\n",
            "275/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1053\n",
            "277/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1051\n",
            "279/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1050\n",
            "281/469 [================>.............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1049\n",
            "285/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1050\n",
            "289/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1051\n",
            "293/469 [=================>............] - ETA: 5s - loss: 2.3051 - accuracy: 0.1052\n",
            "295/469 [=================>............] - ETA: 5s - loss: 2.3050 - accuracy: 0.1054\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 2.3050 - accuracy: 0.1054\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 2.3050 - accuracy: 0.1054\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1053\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1050\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1049\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1048\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1049\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 2.3051 - accuracy: 0.1051\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1049\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 2.3052 - accuracy: 0.1048\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 2.3053 - accuracy: 0.1047\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1045\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 2.3054 - accuracy: 0.1045\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 2.3054 - accuracy: 0.1046\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 2.3053 - accuracy: 0.1048\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 2.3054 - accuracy: 0.1049\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 2.3054 - accuracy: 0.1049\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1051\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1053\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 2.3052 - accuracy: 0.1053\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 2.3051 - accuracy: 0.1054\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 2.3052 - accuracy: 0.1054\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 2.3053 - accuracy: 0.1052\n",
            "369/469 [======================>.......] - ETA: 3s - loss: 2.3052 - accuracy: 0.1054\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1055\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1058\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1057\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1058\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 2.3052 - accuracy: 0.1059\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 2.3051 - accuracy: 0.1060\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 2.3050 - accuracy: 0.1062\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 2.3050 - accuracy: 0.1063\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 2.3050 - accuracy: 0.1064\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 2.3050 - accuracy: 0.1066\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 2.3050 - accuracy: 0.1068\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1068\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1066\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1065\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 2.3051 - accuracy: 0.1066\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1065\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1066\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1066\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 2.3051 - accuracy: 0.1066\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1065\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1066\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 2.3050 - accuracy: 0.1066\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1066\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 2.3051 - accuracy: 0.1066\n",
            "457/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1066\n",
            "461/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1067\n",
            "463/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1066\n",
            "465/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1066\n",
            "467/469 [============================>.] - ETA: 0s - loss: 2.3050 - accuracy: 0.1067\n",
            "469/469 [==============================] - 15s 32ms/step - loss: 2.3050 - accuracy: 0.1067 - val_loss: 2.3087 - val_accuracy: 0.1135\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name          </th><th style=\"text-align: right;\">  mean_accuracy</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_1a912c10</td><td style=\"text-align: right;\">         0.9793</td></tr>\n",
              "<tr><td>train_mnist_40c5a41c</td><td style=\"text-align: right;\">         0.8942</td></tr>\n",
              "<tr><td>train_mnist_65d41f89</td><td style=\"text-align: right;\">         0.9508</td></tr>\n",
              "<tr><td>train_mnist_a60fcaaf</td><td style=\"text-align: right;\">         0.9616</td></tr>\n",
              "<tr><td>train_mnist_b9914140</td><td style=\"text-align: right;\">         0.862 </td></tr>\n",
              "<tr><td>train_mnist_cd0082ba</td><td style=\"text-align: right;\">         0.9597</td></tr>\n",
              "<tr><td>train_mnist_eecf567c</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_f2b95443</td><td style=\"text-align: right;\">         0.9085</td></tr>\n",
              "<tr><td>train_mnist_fb005315</td><td style=\"text-align: right;\">         0.1135</td></tr>\n",
              "<tr><td>train_mnist_ffca9cac</td><td style=\"text-align: right;\">         0.9812</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3296841)\u001b[0m 2023-12-05 01:42:38.962325: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3296841)\u001b[0m 2023-12-05 01:42:38.964613: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3296841)\u001b[0m 2023-12-05 01:42:39.013146: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3296841)\u001b[0m 2023-12-05 01:42:39.013681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3296841)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3296841)\u001b[0m 2023-12-05 01:42:39.912834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3296841)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3296841)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3296841)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3296841)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3296841)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3296841)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3296841)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3296841)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m 2023-12-05 01:42:41.597357: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 1/12\n",
            "  1/938 [..............................] - ETA: 9:55 - loss: 2.3031 - accuracy: 0.0625\n",
            " 15/938 [..............................] - ETA: 7s - loss: 28.0819 - accuracy: 0.2708\n",
            " 29/938 [..............................] - ETA: 6s - loss: 15.0646 - accuracy: 0.4634\n",
            " 43/938 [>.............................] - ETA: 6s - loss: 10.4302 - accuracy: 0.5632\n",
            " 57/938 [>.............................] - ETA: 6s - loss: 8.0299 - accuracy: 0.6234\n",
            " 71/938 [=>............................] - ETA: 6s - loss: 6.5844 - accuracy: 0.6613\n",
            " 85/938 [=>............................] - ETA: 6s - loss: 5.5761 - accuracy: 0.6963\n",
            " 99/938 [==>...........................] - ETA: 6s - loss: 4.8650 - accuracy: 0.7181\n",
            "113/938 [==>...........................] - ETA: 6s - loss: 4.3104 - accuracy: 0.7389\n",
            "127/938 [===>..........................] - ETA: 6s - loss: 3.8837 - accuracy: 0.7546\n",
            "141/938 [===>..........................] - ETA: 5s - loss: 3.5404 - accuracy: 0.7673\n",
            "155/938 [===>..........................] - ETA: 5s - loss: 3.2663 - accuracy: 0.7746\n",
            "169/938 [====>.........................] - ETA: 5s - loss: 3.0346 - accuracy: 0.7821\n",
            "183/938 [====>.........................] - ETA: 5s - loss: 2.8325 - accuracy: 0.7900\n",
            "197/938 [=====>........................] - ETA: 5s - loss: 2.6585 - accuracy: 0.7966\n",
            "211/938 [=====>........................] - ETA: 5s - loss: 2.5061 - accuracy: 0.8027\n",
            "225/938 [======>.......................] - ETA: 5s - loss: 2.3810 - accuracy: 0.8064\n",
            "239/938 [======>.......................] - ETA: 5s - loss: 2.2644 - accuracy: 0.8115\n",
            "253/938 [=======>......................] - ETA: 5s - loss: 2.1606 - accuracy: 0.8164\n",
            "267/938 [=======>......................] - ETA: 5s - loss: 2.0706 - accuracy: 0.8198\n",
            "281/938 [=======>......................] - ETA: 4s - loss: 1.9875 - accuracy: 0.8238\n",
            "295/938 [========>.....................] - ETA: 4s - loss: 1.9137 - accuracy: 0.8267\n",
            "309/938 [========>.....................] - ETA: 4s - loss: 1.8458 - accuracy: 0.8298\n",
            "323/938 [=========>....................] - ETA: 4s - loss: 1.7792 - accuracy: 0.8330\n",
            "337/938 [=========>....................] - ETA: 4s - loss: 1.7186 - accuracy: 0.8359\n",
            "351/938 [==========>...................] - ETA: 4s - loss: 1.6631 - accuracy: 0.8392\n",
            "365/938 [==========>...................] - ETA: 4s - loss: 1.6134 - accuracy: 0.8422\n",
            "372/938 [==========>...................] - ETA: 4s - loss: 1.5914 - accuracy: 0.8427\n",
            "386/938 [===========>..................] - ETA: 4s - loss: 1.5475 - accuracy: 0.8451\n",
            "400/938 [===========>..................] - ETA: 4s - loss: 1.5049 - accuracy: 0.8475\n",
            "414/938 [============>.................] - ETA: 3s - loss: 1.4658 - accuracy: 0.8487\n",
            "428/938 [============>.................] - ETA: 3s - loss: 1.4292 - accuracy: 0.8508\n",
            "442/938 [=============>................] - ETA: 3s - loss: 1.3978 - accuracy: 0.8515\n",
            "456/938 [=============>................] - ETA: 3s - loss: 1.3668 - accuracy: 0.8529\n",
            "470/938 [==============>...............] - ETA: 3s - loss: 1.3371 - accuracy: 0.8541\n",
            "484/938 [==============>...............] - ETA: 3s - loss: 1.3071 - accuracy: 0.8558\n",
            "498/938 [==============>...............] - ETA: 3s - loss: 1.2813 - accuracy: 0.8567\n",
            "512/938 [===============>..............] - ETA: 3s - loss: 1.2556 - accuracy: 0.8581\n",
            "526/938 [===============>..............] - ETA: 3s - loss: 1.2326 - accuracy: 0.8591\n",
            "540/938 [================>.............] - ETA: 2s - loss: 1.2089 - accuracy: 0.8604\n",
            "554/938 [================>.............] - ETA: 2s - loss: 1.1861 - accuracy: 0.8615\n",
            "568/938 [=================>............] - ETA: 2s - loss: 1.1635 - accuracy: 0.8631\n",
            "582/938 [=================>............] - ETA: 2s - loss: 1.1441 - accuracy: 0.8647\n",
            "596/938 [==================>...........] - ETA: 2s - loss: 1.1246 - accuracy: 0.8658\n",
            "610/938 [==================>...........] - ETA: 2s - loss: 1.1071 - accuracy: 0.8669\n",
            "624/938 [==================>...........] - ETA: 2s - loss: 1.0908 - accuracy: 0.8676\n",
            "638/938 [===================>..........] - ETA: 2s - loss: 1.0746 - accuracy: 0.8683\n",
            "652/938 [===================>..........] - ETA: 2s - loss: 1.0601 - accuracy: 0.8686\n",
            "666/938 [====================>.........] - ETA: 2s - loss: 1.0451 - accuracy: 0.8694\n",
            "680/938 [====================>.........] - ETA: 1s - loss: 1.0317 - accuracy: 0.8703\n",
            "694/938 [=====================>........] - ETA: 1s - loss: 1.0180 - accuracy: 0.8709\n",
            "708/938 [=====================>........] - ETA: 1s - loss: 1.0055 - accuracy: 0.8717\n",
            "715/938 [=====================>........] - ETA: 1s - loss: 0.9987 - accuracy: 0.8722\n",
            "729/938 [======================>.......] - ETA: 1s - loss: 0.9870 - accuracy: 0.8728\n",
            "743/938 [======================>.......] - ETA: 1s - loss: 0.9741 - accuracy: 0.8735\n",
            "757/938 [=======================>......] - ETA: 1s - loss: 0.9621 - accuracy: 0.8741\n",
            "771/938 [=======================>......] - ETA: 1s - loss: 0.9509 - accuracy: 0.8748\n",
            "785/938 [========================>.....] - ETA: 1s - loss: 0.9409 - accuracy: 0.8753\n",
            "799/938 [========================>.....] - ETA: 1s - loss: 0.9301 - accuracy: 0.8761\n",
            "813/938 [=========================>....] - ETA: 0s - loss: 0.9195 - accuracy: 0.8766\n",
            "827/938 [=========================>....] - ETA: 0s - loss: 0.9088 - accuracy: 0.8775\n",
            "841/938 [=========================>....] - ETA: 0s - loss: 0.8981 - accuracy: 0.8781\n",
            "855/938 [==========================>...] - ETA: 0s - loss: 0.8877 - accuracy: 0.8790\n",
            "869/938 [==========================>...] - ETA: 0s - loss: 0.8788 - accuracy: 0.8794\n",
            "883/938 [===========================>..] - ETA: 0s - loss: 0.8696 - accuracy: 0.8800\n",
            "897/938 [===========================>..] - ETA: 0s - loss: 0.8615 - accuracy: 0.8805\n",
            "911/938 [============================>.] - ETA: 0s - loss: 0.8535 - accuracy: 0.8810\n",
            "925/938 [============================>.] - ETA: 0s - loss: 0.8458 - accuracy: 0.8813\n",
            "932/938 [============================>.] - ETA: 0s - loss: 0.8423 - accuracy: 0.8815\n",
            "938/938 [==============================] - 8s 8ms/step - loss: 0.8394 - accuracy: 0.8817 - val_loss: 0.3210 - val_accuracy: 0.9084\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 2/12\n",
            "  8/938 [..............................] - ETA: 7s - loss: 0.4405 - accuracy: 0.8770 \n",
            " 22/938 [..............................] - ETA: 6s - loss: 0.3281 - accuracy: 0.9091\n",
            " 36/938 [>.............................] - ETA: 6s - loss: 0.3118 - accuracy: 0.9175\n",
            " 50/938 [>.............................] - ETA: 6s - loss: 0.3476 - accuracy: 0.9109\n",
            " 64/938 [=>............................] - ETA: 6s - loss: 0.3373 - accuracy: 0.9114\n",
            " 78/938 [=>............................] - ETA: 6s - loss: 0.3116 - accuracy: 0.9181\n",
            " 92/938 [=>............................] - ETA: 6s - loss: 0.3177 - accuracy: 0.9197\n",
            " 99/938 [==>...........................] - ETA: 6s - loss: 0.3138 - accuracy: 0.9192\n",
            "113/938 [==>...........................] - ETA: 6s - loss: 0.3162 - accuracy: 0.9165\n",
            "127/938 [===>..........................] - ETA: 5s - loss: 0.3082 - accuracy: 0.9181\n",
            "141/938 [===>..........................] - ETA: 5s - loss: 0.3155 - accuracy: 0.9168\n",
            "155/938 [===>..........................] - ETA: 5s - loss: 0.3100 - accuracy: 0.9177\n",
            "169/938 [====>.........................] - ETA: 5s - loss: 0.3044 - accuracy: 0.9190\n",
            "183/938 [====>.........................] - ETA: 5s - loss: 0.3039 - accuracy: 0.9196\n",
            "197/938 [=====>........................] - ETA: 5s - loss: 0.3016 - accuracy: 0.9195\n",
            "211/938 [=====>........................] - ETA: 5s - loss: 0.3055 - accuracy: 0.9190\n",
            "225/938 [======>.......................] - ETA: 5s - loss: 0.3086 - accuracy: 0.9179\n",
            "239/938 [======>.......................] - ETA: 5s - loss: 0.3118 - accuracy: 0.9181\n",
            "253/938 [=======>......................] - ETA: 5s - loss: 0.3091 - accuracy: 0.9186\n",
            "267/938 [=======>......................] - ETA: 4s - loss: 0.3097 - accuracy: 0.9188\n",
            "281/938 [=======>......................] - ETA: 4s - loss: 0.3105 - accuracy: 0.9183\n",
            "295/938 [========>.....................] - ETA: 4s - loss: 0.3121 - accuracy: 0.9173\n",
            "309/938 [========>.....................] - ETA: 4s - loss: 0.3129 - accuracy: 0.9176\n",
            "316/938 [=========>....................] - ETA: 4s - loss: 0.3132 - accuracy: 0.9175\n",
            "330/938 [=========>....................] - ETA: 4s - loss: 0.3124 - accuracy: 0.9174\n",
            "344/938 [==========>...................] - ETA: 4s - loss: 0.3158 - accuracy: 0.9169\n",
            "358/938 [==========>...................] - ETA: 4s - loss: 0.3166 - accuracy: 0.9170\n",
            "372/938 [==========>...................] - ETA: 4s - loss: 0.3172 - accuracy: 0.9163\n",
            "386/938 [===========>..................] - ETA: 4s - loss: 0.3157 - accuracy: 0.9170\n",
            "400/938 [===========>..................] - ETA: 3s - loss: 0.3167 - accuracy: 0.9170\n",
            "414/938 [============>.................] - ETA: 3s - loss: 0.3180 - accuracy: 0.9167\n",
            "428/938 [============>.................] - ETA: 3s - loss: 0.3199 - accuracy: 0.9171\n",
            "442/938 [=============>................] - ETA: 3s - loss: 0.3206 - accuracy: 0.9167\n",
            "456/938 [=============>................] - ETA: 3s - loss: 0.3218 - accuracy: 0.9164\n",
            "470/938 [==============>...............] - ETA: 3s - loss: 0.3221 - accuracy: 0.9160\n",
            "484/938 [==============>...............] - ETA: 3s - loss: 0.3224 - accuracy: 0.9156\n",
            "498/938 [==============>...............] - ETA: 3s - loss: 0.3231 - accuracy: 0.9154\n",
            "512/938 [===============>..............] - ETA: 3s - loss: 0.3230 - accuracy: 0.9158\n",
            "526/938 [===============>..............] - ETA: 3s - loss: 0.3230 - accuracy: 0.9159\n",
            "540/938 [================>.............] - ETA: 2s - loss: 0.3226 - accuracy: 0.9158\n",
            "554/938 [================>.............] - ETA: 2s - loss: 0.3222 - accuracy: 0.9160\n",
            "568/938 [=================>............] - ETA: 2s - loss: 0.3212 - accuracy: 0.9164\n",
            "575/938 [=================>............] - ETA: 2s - loss: 0.3216 - accuracy: 0.9165\n",
            "589/938 [=================>............] - ETA: 2s - loss: 0.3203 - accuracy: 0.9168\n",
            "603/938 [==================>...........] - ETA: 2s - loss: 0.3208 - accuracy: 0.9164\n",
            "617/938 [==================>...........] - ETA: 2s - loss: 0.3217 - accuracy: 0.9165\n",
            "631/938 [===================>..........] - ETA: 2s - loss: 0.3210 - accuracy: 0.9167\n",
            "645/938 [===================>..........] - ETA: 2s - loss: 0.3214 - accuracy: 0.9168\n",
            "659/938 [====================>.........] - ETA: 2s - loss: 0.3236 - accuracy: 0.9163\n",
            "673/938 [====================>.........] - ETA: 1s - loss: 0.3237 - accuracy: 0.9164\n",
            "687/938 [====================>.........] - ETA: 1s - loss: 0.3246 - accuracy: 0.9162\n",
            "701/938 [=====================>........] - ETA: 1s - loss: 0.3245 - accuracy: 0.9163\n",
            "715/938 [=====================>........] - ETA: 1s - loss: 0.3281 - accuracy: 0.9160\n",
            "729/938 [======================>.......] - ETA: 1s - loss: 0.3301 - accuracy: 0.9155\n",
            "743/938 [======================>.......] - ETA: 1s - loss: 0.3312 - accuracy: 0.9156\n",
            "757/938 [=======================>......] - ETA: 1s - loss: 0.3320 - accuracy: 0.9155\n",
            "771/938 [=======================>......] - ETA: 1s - loss: 0.3352 - accuracy: 0.9151\n",
            "784/938 [========================>.....] - ETA: 1s - loss: 0.3365 - accuracy: 0.9145\n",
            "791/938 [========================>.....] - ETA: 1s - loss: 0.3384 - accuracy: 0.9142\n",
            "805/938 [========================>.....] - ETA: 0s - loss: 0.3407 - accuracy: 0.9138\n",
            "819/938 [=========================>....] - ETA: 0s - loss: 0.3415 - accuracy: 0.9135\n",
            "833/938 [=========================>....] - ETA: 0s - loss: 0.3437 - accuracy: 0.9131\n",
            "847/938 [==========================>...] - ETA: 0s - loss: 0.3457 - accuracy: 0.9128\n",
            "861/938 [==========================>...] - ETA: 0s - loss: 0.3465 - accuracy: 0.9125\n",
            "875/938 [==========================>...] - ETA: 0s - loss: 0.3478 - accuracy: 0.9123\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 0.3497 - accuracy: 0.9120\n",
            "903/938 [===========================>..] - ETA: 0s - loss: 0.3512 - accuracy: 0.9118\n",
            "917/938 [============================>.] - ETA: 0s - loss: 0.3525 - accuracy: 0.9115\n",
            "931/938 [============================>.] - ETA: 0s - loss: 0.3525 - accuracy: 0.9113\n",
            "938/938 [==============================] - ETA: 0s - loss: 0.3526 - accuracy: 0.9112\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.3526 - accuracy: 0.9112 - val_loss: 0.2830 - val_accuracy: 0.9357\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 3/12\n",
            "  1/938 [..............................] - ETA: 8s - loss: 0.3781 - accuracy: 0.9062\n",
            " 15/938 [..............................] - ETA: 6s - loss: 0.4016 - accuracy: 0.9052\n",
            " 23/938 [..............................] - ETA: 6s - loss: 0.4156 - accuracy: 0.9035\n",
            " 30/938 [..............................] - ETA: 6s - loss: 0.4117 - accuracy: 0.9068\n",
            " 44/938 [>.............................] - ETA: 6s - loss: 0.4002 - accuracy: 0.9052\n",
            " 59/938 [>.............................] - ETA: 6s - loss: 0.3723 - accuracy: 0.9094\n",
            " 73/938 [=>............................] - ETA: 6s - loss: 0.3685 - accuracy: 0.9105\n",
            " 87/938 [=>............................] - ETA: 6s - loss: 0.3685 - accuracy: 0.9109\n",
            "101/938 [==>...........................] - ETA: 6s - loss: 0.3622 - accuracy: 0.9115\n",
            "115/938 [==>...........................] - ETA: 6s - loss: 0.3609 - accuracy: 0.9115\n",
            "129/938 [===>..........................] - ETA: 5s - loss: 0.3553 - accuracy: 0.9138\n",
            "143/938 [===>..........................] - ETA: 5s - loss: 0.3562 - accuracy: 0.9131\n",
            "157/938 [====>.........................] - ETA: 5s - loss: 0.3544 - accuracy: 0.9120\n",
            "171/938 [====>.........................] - ETA: 5s - loss: 0.3527 - accuracy: 0.9115\n",
            "185/938 [====>.........................] - ETA: 5s - loss: 0.3504 - accuracy: 0.9126\n",
            "199/938 [=====>........................] - ETA: 5s - loss: 0.3451 - accuracy: 0.9139\n",
            "213/938 [=====>........................] - ETA: 5s - loss: 0.3478 - accuracy: 0.9128\n",
            "227/938 [======>.......................] - ETA: 5s - loss: 0.3505 - accuracy: 0.9129\n",
            "241/938 [======>.......................] - ETA: 5s - loss: 0.3525 - accuracy: 0.9121\n",
            "255/938 [=======>......................] - ETA: 5s - loss: 0.3544 - accuracy: 0.9118\n",
            "262/938 [=======>......................] - ETA: 4s - loss: 0.3578 - accuracy: 0.9115\n",
            "276/938 [=======>......................] - ETA: 4s - loss: 0.3691 - accuracy: 0.9111\n",
            "290/938 [========>.....................] - ETA: 4s - loss: 0.3748 - accuracy: 0.9101\n",
            "304/938 [========>.....................] - ETA: 4s - loss: 0.3791 - accuracy: 0.9096\n",
            "318/938 [=========>....................] - ETA: 4s - loss: 0.3799 - accuracy: 0.9093\n",
            "332/938 [=========>....................] - ETA: 4s - loss: 0.3819 - accuracy: 0.9085\n",
            "346/938 [==========>...................] - ETA: 4s - loss: 0.3847 - accuracy: 0.9085\n",
            "360/938 [==========>...................] - ETA: 4s - loss: 0.3869 - accuracy: 0.9074\n",
            "374/938 [==========>...................] - ETA: 4s - loss: 0.3933 - accuracy: 0.9075\n",
            "388/938 [===========>..................] - ETA: 4s - loss: 0.3901 - accuracy: 0.9080\n",
            "402/938 [===========>..................] - ETA: 3s - loss: 0.3942 - accuracy: 0.9073\n",
            "416/938 [============>.................] - ETA: 3s - loss: 0.3931 - accuracy: 0.9076\n",
            "430/938 [============>.................] - ETA: 3s - loss: 0.3951 - accuracy: 0.9076\n",
            "444/938 [=============>................] - ETA: 3s - loss: 0.3969 - accuracy: 0.9071\n",
            "458/938 [=============>................] - ETA: 3s - loss: 0.4072 - accuracy: 0.9067\n",
            "472/938 [==============>...............] - ETA: 3s - loss: 0.4123 - accuracy: 0.9053\n",
            "486/938 [==============>...............] - ETA: 3s - loss: 0.4186 - accuracy: 0.9050\n",
            "500/938 [==============>...............] - ETA: 3s - loss: 0.4260 - accuracy: 0.9042\n",
            "514/938 [===============>..............] - ETA: 3s - loss: 0.4479 - accuracy: 0.9006\n",
            "521/938 [===============>..............] - ETA: 3s - loss: 0.4539 - accuracy: 0.8992\n",
            "535/938 [================>.............] - ETA: 2s - loss: 0.4660 - accuracy: 0.8962\n",
            "549/938 [================>.............] - ETA: 2s - loss: 0.4713 - accuracy: 0.8946\n",
            "563/938 [=================>............] - ETA: 2s - loss: 0.4733 - accuracy: 0.8935\n",
            "577/938 [=================>............] - ETA: 2s - loss: 0.4749 - accuracy: 0.8928\n",
            "591/938 [=================>............] - ETA: 2s - loss: 0.4783 - accuracy: 0.8918\n",
            "605/938 [==================>...........] - ETA: 2s - loss: 0.4821 - accuracy: 0.8904\n",
            "619/938 [==================>...........] - ETA: 2s - loss: 0.4850 - accuracy: 0.8900\n",
            "633/938 [===================>..........] - ETA: 2s - loss: 0.4862 - accuracy: 0.8888\n",
            "647/938 [===================>..........] - ETA: 2s - loss: 0.4870 - accuracy: 0.8883\n",
            "661/938 [====================>.........] - ETA: 2s - loss: 0.4878 - accuracy: 0.8878\n",
            "675/938 [====================>.........] - ETA: 1s - loss: 0.4867 - accuracy: 0.8876\n",
            "689/938 [=====================>........] - ETA: 1s - loss: 0.4845 - accuracy: 0.8878\n",
            "703/938 [=====================>........] - ETA: 1s - loss: 0.4835 - accuracy: 0.8877\n",
            "717/938 [=====================>........] - ETA: 1s - loss: 0.4828 - accuracy: 0.8876\n",
            "731/938 [======================>.......] - ETA: 1s - loss: 0.4825 - accuracy: 0.8875\n",
            "745/938 [======================>.......] - ETA: 1s - loss: 0.4837 - accuracy: 0.8870\n",
            "752/938 [=======================>......] - ETA: 1s - loss: 0.4865 - accuracy: 0.8861\n",
            "766/938 [=======================>......] - ETA: 1s - loss: 0.4874 - accuracy: 0.8858\n",
            "780/938 [=======================>......] - ETA: 1s - loss: 0.4886 - accuracy: 0.8853\n",
            "794/938 [========================>.....] - ETA: 1s - loss: 0.4892 - accuracy: 0.8847\n",
            "808/938 [========================>.....] - ETA: 0s - loss: 0.4894 - accuracy: 0.8847\n",
            "822/938 [=========================>....] - ETA: 0s - loss: 0.4899 - accuracy: 0.8848\n",
            "836/938 [=========================>....] - ETA: 0s - loss: 0.4899 - accuracy: 0.8844\n",
            "850/938 [==========================>...] - ETA: 0s - loss: 0.4899 - accuracy: 0.8844\n",
            "864/938 [==========================>...] - ETA: 0s - loss: 0.4886 - accuracy: 0.8844\n",
            "878/938 [===========================>..] - ETA: 0s - loss: 0.4893 - accuracy: 0.8842\n",
            "892/938 [===========================>..] - ETA: 0s - loss: 0.4905 - accuracy: 0.8839\n",
            "906/938 [===========================>..] - ETA: 0s - loss: 0.4911 - accuracy: 0.8839\n",
            "920/938 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.8838\n",
            "934/938 [============================>.] - ETA: 0s - loss: 0.4910 - accuracy: 0.8839\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.4905 - accuracy: 0.8840 - val_loss: 0.4005 - val_accuracy: 0.9038\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 4/12\n",
            "  1/938 [..............................] - ETA: 7s - loss: 0.1573 - accuracy: 0.9531\n",
            " 15/938 [..............................] - ETA: 6s - loss: 0.4407 - accuracy: 0.8958\n",
            " 29/938 [..............................] - ETA: 6s - loss: 0.4320 - accuracy: 0.8847\n",
            " 43/938 [>.............................] - ETA: 6s - loss: 0.4426 - accuracy: 0.8848\n",
            " 57/938 [>.............................] - ETA: 6s - loss: 0.4340 - accuracy: 0.8873\n",
            " 71/938 [=>............................] - ETA: 6s - loss: 0.4334 - accuracy: 0.8849\n",
            " 85/938 [=>............................] - ETA: 6s - loss: 0.4321 - accuracy: 0.8862\n",
            " 99/938 [==>...........................] - ETA: 6s - loss: 0.4382 - accuracy: 0.8854\n",
            "113/938 [==>...........................] - ETA: 6s - loss: 0.4401 - accuracy: 0.8859\n",
            "127/938 [===>..........................] - ETA: 5s - loss: 0.7448 - accuracy: 0.8616\n",
            "141/938 [===>..........................] - ETA: 5s - loss: 0.8827 - accuracy: 0.8339\n",
            "148/938 [===>..........................] - ETA: 5s - loss: 1.0036 - accuracy: 0.8065\n",
            "162/938 [====>.........................] - ETA: 5s - loss: 1.1605 - accuracy: 0.7579\n",
            "176/938 [====>.........................] - ETA: 5s - loss: 1.2421 - accuracy: 0.7172\n",
            "190/938 [=====>........................] - ETA: 5s - loss: 1.3214 - accuracy: 0.6870\n",
            "204/938 [=====>........................] - ETA: 5s - loss: 1.3771 - accuracy: 0.6562\n",
            "218/938 [=====>........................] - ETA: 5s - loss: 1.4526 - accuracy: 0.6348\n",
            "232/938 [======>.......................] - ETA: 5s - loss: 1.4770 - accuracy: 0.6179\n",
            "246/938 [======>.......................] - ETA: 5s - loss: 1.5031 - accuracy: 0.6056\n",
            "260/938 [=======>......................] - ETA: 4s - loss: 1.5274 - accuracy: 0.5939\n",
            "274/938 [=======>......................] - ETA: 4s - loss: 1.5355 - accuracy: 0.5865\n",
            "288/938 [========>.....................] - ETA: 4s - loss: 1.5408 - accuracy: 0.5786\n",
            "302/938 [========>.....................] - ETA: 4s - loss: 1.5546 - accuracy: 0.5738\n",
            "316/938 [=========>....................] - ETA: 4s - loss: 1.5727 - accuracy: 0.5636\n",
            "330/938 [=========>....................] - ETA: 4s - loss: 1.5765 - accuracy: 0.5562\n",
            "344/938 [==========>...................] - ETA: 4s - loss: 1.5802 - accuracy: 0.5502\n",
            "358/938 [==========>...................] - ETA: 4s - loss: 1.5805 - accuracy: 0.5501\n",
            "372/938 [==========>...................] - ETA: 4s - loss: 1.5767 - accuracy: 0.5523\n",
            "379/938 [===========>..................] - ETA: 4s - loss: 1.5728 - accuracy: 0.5535\n",
            "393/938 [===========>..................] - ETA: 4s - loss: 1.5671 - accuracy: 0.5573\n",
            "407/938 [============>.................] - ETA: 3s - loss: 1.5610 - accuracy: 0.5596\n",
            "421/938 [============>.................] - ETA: 3s - loss: 1.5560 - accuracy: 0.5619\n",
            "435/938 [============>.................] - ETA: 3s - loss: 1.5476 - accuracy: 0.5653\n",
            "449/938 [=============>................] - ETA: 3s - loss: 1.5365 - accuracy: 0.5692\n",
            "463/938 [=============>................] - ETA: 3s - loss: 1.5228 - accuracy: 0.5723\n",
            "477/938 [==============>...............] - ETA: 3s - loss: 1.5122 - accuracy: 0.5757\n",
            "491/938 [==============>...............] - ETA: 3s - loss: 1.5008 - accuracy: 0.5778\n",
            "505/938 [===============>..............] - ETA: 3s - loss: 1.4869 - accuracy: 0.5805\n",
            "519/938 [===============>..............] - ETA: 3s - loss: 1.4752 - accuracy: 0.5836\n",
            "533/938 [================>.............] - ETA: 2s - loss: 1.4624 - accuracy: 0.5868\n",
            "547/938 [================>.............] - ETA: 2s - loss: 1.4491 - accuracy: 0.5905\n",
            "561/938 [================>.............] - ETA: 2s - loss: 1.4341 - accuracy: 0.5937\n",
            "575/938 [=================>............] - ETA: 2s - loss: 1.4214 - accuracy: 0.5975\n",
            "589/938 [=================>............] - ETA: 2s - loss: 1.4094 - accuracy: 0.6010\n",
            "603/938 [==================>...........] - ETA: 2s - loss: 1.3953 - accuracy: 0.6047\n",
            "618/938 [==================>...........] - ETA: 2s - loss: 1.3829 - accuracy: 0.6077\n",
            "632/938 [===================>..........] - ETA: 2s - loss: 1.3731 - accuracy: 0.6109\n",
            "646/938 [===================>..........] - ETA: 2s - loss: 1.3642 - accuracy: 0.6134\n",
            "653/938 [===================>..........] - ETA: 2s - loss: 1.3587 - accuracy: 0.6146\n",
            "667/938 [====================>.........] - ETA: 1s - loss: 1.3478 - accuracy: 0.6172\n",
            "681/938 [====================>.........] - ETA: 1s - loss: 1.3371 - accuracy: 0.6204\n",
            "695/938 [=====================>........] - ETA: 1s - loss: 1.3272 - accuracy: 0.6231\n",
            "709/938 [=====================>........] - ETA: 1s - loss: 1.3182 - accuracy: 0.6257\n",
            "723/938 [======================>.......] - ETA: 1s - loss: 1.3082 - accuracy: 0.6281\n",
            "737/938 [======================>.......] - ETA: 1s - loss: 1.2988 - accuracy: 0.6304\n",
            "751/938 [=======================>......] - ETA: 1s - loss: 1.2891 - accuracy: 0.6327\n",
            "765/938 [=======================>......] - ETA: 1s - loss: 1.2775 - accuracy: 0.6355\n",
            "779/938 [=======================>......] - ETA: 1s - loss: 1.2670 - accuracy: 0.6383\n",
            "793/938 [========================>.....] - ETA: 1s - loss: 1.2578 - accuracy: 0.6411\n",
            "807/938 [========================>.....] - ETA: 0s - loss: 1.2489 - accuracy: 0.6434\n",
            "821/938 [=========================>....] - ETA: 0s - loss: 1.2414 - accuracy: 0.6457\n",
            "835/938 [=========================>....] - ETA: 0s - loss: 1.2336 - accuracy: 0.6478\n",
            "849/938 [==========================>...] - ETA: 0s - loss: 1.2265 - accuracy: 0.6495\n",
            "863/938 [==========================>...] - ETA: 0s - loss: 1.2195 - accuracy: 0.6514\n",
            "877/938 [===========================>..] - ETA: 0s - loss: 1.2119 - accuracy: 0.6530\n",
            "891/938 [===========================>..] - ETA: 0s - loss: 1.2050 - accuracy: 0.6544\n",
            "905/938 [===========================>..] - ETA: 0s - loss: 1.1977 - accuracy: 0.6560\n",
            "919/938 [============================>.] - ETA: 0s - loss: 1.1904 - accuracy: 0.6580\n",
            "926/938 [============================>.] - ETA: 0s - loss: 1.1866 - accuracy: 0.6590\n",
            "933/938 [============================>.] - ETA: 0s - loss: 1.1834 - accuracy: 0.6596\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 1.1811 - accuracy: 0.6601 - val_loss: 0.6437 - val_accuracy: 0.8241\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 5/12\n",
            "  1/938 [..............................] - ETA: 7s - loss: 0.4745 - accuracy: 0.8750\n",
            " 15/938 [..............................] - ETA: 6s - loss: 0.7140 - accuracy: 0.7906\n",
            " 29/938 [..............................] - ETA: 6s - loss: 0.7189 - accuracy: 0.7802\n",
            " 43/938 [>.............................] - ETA: 6s - loss: 0.7114 - accuracy: 0.7747\n",
            " 57/938 [>.............................] - ETA: 6s - loss: 0.7099 - accuracy: 0.7771\n",
            " 71/938 [=>............................] - ETA: 6s - loss: 0.6905 - accuracy: 0.7786\n",
            " 85/938 [=>............................] - ETA: 6s - loss: 0.6953 - accuracy: 0.7790\n",
            " 99/938 [==>...........................] - ETA: 6s - loss: 0.6901 - accuracy: 0.7835\n",
            "113/938 [==>...........................] - ETA: 6s - loss: 0.6933 - accuracy: 0.7843\n",
            "120/938 [==>...........................] - ETA: 6s - loss: 0.6926 - accuracy: 0.7846\n",
            "134/938 [===>..........................] - ETA: 5s - loss: 0.6892 - accuracy: 0.7846\n",
            "148/938 [===>..........................] - ETA: 5s - loss: 0.6825 - accuracy: 0.7880\n",
            "162/938 [====>.........................] - ETA: 5s - loss: 0.6838 - accuracy: 0.7896\n",
            "176/938 [====>.........................] - ETA: 5s - loss: 0.6906 - accuracy: 0.7904\n",
            "190/938 [=====>........................] - ETA: 5s - loss: 0.6876 - accuracy: 0.7936\n",
            "204/938 [=====>........................] - ETA: 5s - loss: 0.6845 - accuracy: 0.7955\n",
            "218/938 [=====>........................] - ETA: 5s - loss: 0.6861 - accuracy: 0.7974\n",
            "232/938 [======>.......................] - ETA: 5s - loss: 0.6815 - accuracy: 0.7987\n",
            "246/938 [======>.......................] - ETA: 5s - loss: 0.6803 - accuracy: 0.7992\n",
            "260/938 [=======>......................] - ETA: 5s - loss: 0.6777 - accuracy: 0.8005\n",
            "274/938 [=======>......................] - ETA: 4s - loss: 0.6721 - accuracy: 0.8021\n",
            "288/938 [========>.....................] - ETA: 4s - loss: 0.6635 - accuracy: 0.8044\n",
            "302/938 [========>.....................] - ETA: 4s - loss: 0.6630 - accuracy: 0.8048\n",
            "316/938 [=========>....................] - ETA: 4s - loss: 0.6599 - accuracy: 0.8063\n",
            "330/938 [=========>....................] - ETA: 4s - loss: 0.6608 - accuracy: 0.8073\n",
            "344/938 [==========>...................] - ETA: 4s - loss: 0.6613 - accuracy: 0.8073\n",
            "358/938 [==========>...................] - ETA: 4s - loss: 0.6617 - accuracy: 0.8073\n",
            "365/938 [==========>...................] - ETA: 4s - loss: 0.6628 - accuracy: 0.8076\n",
            "379/938 [===========>..................] - ETA: 4s - loss: 0.6578 - accuracy: 0.8087\n",
            "393/938 [===========>..................] - ETA: 4s - loss: 0.6525 - accuracy: 0.8103\n",
            "407/938 [============>.................] - ETA: 3s - loss: 0.6516 - accuracy: 0.8112\n",
            "421/938 [============>.................] - ETA: 3s - loss: 0.6507 - accuracy: 0.8119\n",
            "434/938 [============>.................] - ETA: 3s - loss: 0.6535 - accuracy: 0.8120\n",
            "448/938 [=============>................] - ETA: 3s - loss: 0.6503 - accuracy: 0.8132\n",
            "462/938 [=============>................] - ETA: 3s - loss: 0.6474 - accuracy: 0.8147\n",
            "476/938 [==============>...............] - ETA: 3s - loss: 0.6470 - accuracy: 0.8156\n",
            "490/938 [==============>...............] - ETA: 3s - loss: 0.6441 - accuracy: 0.8167\n",
            "504/938 [===============>..............] - ETA: 3s - loss: 0.6408 - accuracy: 0.8178\n",
            "518/938 [===============>..............] - ETA: 3s - loss: 0.6425 - accuracy: 0.8176\n",
            "532/938 [================>.............] - ETA: 3s - loss: 0.6425 - accuracy: 0.8180\n",
            "546/938 [================>.............] - ETA: 2s - loss: 0.6411 - accuracy: 0.8183\n",
            "560/938 [================>.............] - ETA: 2s - loss: 0.6403 - accuracy: 0.8186\n",
            "574/938 [=================>............] - ETA: 2s - loss: 0.6371 - accuracy: 0.8198\n",
            "588/938 [=================>............] - ETA: 2s - loss: 0.6358 - accuracy: 0.8203\n",
            "602/938 [==================>...........] - ETA: 2s - loss: 0.6322 - accuracy: 0.8214\n",
            "609/938 [==================>...........] - ETA: 2s - loss: 0.6316 - accuracy: 0.8215\n",
            "623/938 [==================>...........] - ETA: 2s - loss: 0.6284 - accuracy: 0.8225\n",
            "637/938 [===================>..........] - ETA: 2s - loss: 0.6272 - accuracy: 0.8231\n",
            "651/938 [===================>..........] - ETA: 2s - loss: 0.6250 - accuracy: 0.8237\n",
            "665/938 [====================>.........] - ETA: 2s - loss: 0.6217 - accuracy: 0.8248\n",
            "679/938 [====================>.........] - ETA: 1s - loss: 0.6206 - accuracy: 0.8255\n",
            "693/938 [=====================>........] - ETA: 1s - loss: 0.6182 - accuracy: 0.8263\n",
            "707/938 [=====================>........] - ETA: 1s - loss: 0.6167 - accuracy: 0.8266\n",
            "721/938 [======================>.......] - ETA: 1s - loss: 0.6179 - accuracy: 0.8267\n",
            "735/938 [======================>.......] - ETA: 1s - loss: 0.6177 - accuracy: 0.8269\n",
            "749/938 [======================>.......] - ETA: 1s - loss: 0.6153 - accuracy: 0.8278\n",
            "763/938 [=======================>......] - ETA: 1s - loss: 0.6143 - accuracy: 0.8280\n",
            "777/938 [=======================>......] - ETA: 1s - loss: 0.6124 - accuracy: 0.8286\n",
            "791/938 [========================>.....] - ETA: 1s - loss: 0.6121 - accuracy: 0.8287\n",
            "805/938 [========================>.....] - ETA: 0s - loss: 0.6108 - accuracy: 0.8293\n",
            "819/938 [=========================>....] - ETA: 0s - loss: 0.6106 - accuracy: 0.8293\n",
            "833/938 [=========================>....] - ETA: 0s - loss: 0.6109 - accuracy: 0.8293\n",
            "840/938 [=========================>....] - ETA: 0s - loss: 0.6100 - accuracy: 0.8297\n",
            "854/938 [==========================>...] - ETA: 0s - loss: 0.6082 - accuracy: 0.8302\n",
            "868/938 [==========================>...] - ETA: 0s - loss: 0.6085 - accuracy: 0.8304\n",
            "882/938 [===========================>..] - ETA: 0s - loss: 0.6072 - accuracy: 0.8306\n",
            "896/938 [===========================>..] - ETA: 0s - loss: 0.6064 - accuracy: 0.8309\n",
            "910/938 [============================>.] - ETA: 0s - loss: 0.6066 - accuracy: 0.8315\n",
            "924/938 [============================>.] - ETA: 0s - loss: 0.6064 - accuracy: 0.8316\n",
            "931/938 [============================>.] - ETA: 0s - loss: 0.6048 - accuracy: 0.8321\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.6048 - accuracy: 0.8322 - val_loss: 0.5082 - val_accuracy: 0.8744\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 6/12\n",
            "  1/938 [..............................] - ETA: 7s - loss: 0.4988 - accuracy: 0.8906\n",
            " 15/938 [..............................] - ETA: 6s - loss: 0.5301 - accuracy: 0.8677\n",
            " 29/938 [..............................] - ETA: 6s - loss: 0.5274 - accuracy: 0.8642\n",
            " 43/938 [>.............................] - ETA: 6s - loss: 0.5211 - accuracy: 0.8626\n",
            " 57/938 [>.............................] - ETA: 6s - loss: 0.5254 - accuracy: 0.8635\n",
            " 64/938 [=>............................] - ETA: 6s - loss: 0.5315 - accuracy: 0.8596\n",
            " 78/938 [=>............................] - ETA: 6s - loss: 0.5294 - accuracy: 0.8600\n",
            " 92/938 [=>............................] - ETA: 6s - loss: 0.5288 - accuracy: 0.8599\n",
            "106/938 [==>...........................] - ETA: 6s - loss: 0.5232 - accuracy: 0.8625\n",
            "120/938 [==>...........................] - ETA: 6s - loss: 0.5095 - accuracy: 0.8634\n",
            "134/938 [===>..........................] - ETA: 5s - loss: 0.5078 - accuracy: 0.8652\n",
            "148/938 [===>..........................] - ETA: 5s - loss: 0.5102 - accuracy: 0.8632\n",
            "162/938 [====>.........................] - ETA: 5s - loss: 0.5097 - accuracy: 0.8632\n",
            "176/938 [====>.........................] - ETA: 5s - loss: 0.5111 - accuracy: 0.8639\n",
            "190/938 [=====>........................] - ETA: 5s - loss: 0.5082 - accuracy: 0.8638\n",
            "204/938 [=====>........................] - ETA: 5s - loss: 0.5058 - accuracy: 0.8631\n",
            "218/938 [=====>........................] - ETA: 5s - loss: 0.5039 - accuracy: 0.8630\n",
            "232/938 [======>.......................] - ETA: 5s - loss: 0.5026 - accuracy: 0.8634\n",
            "246/938 [======>.......................] - ETA: 5s - loss: 0.4994 - accuracy: 0.8632\n",
            "260/938 [=======>......................] - ETA: 4s - loss: 0.4974 - accuracy: 0.8635\n",
            "267/938 [=======>......................] - ETA: 4s - loss: 0.5023 - accuracy: 0.8631\n",
            "281/938 [=======>......................] - ETA: 4s - loss: 0.5028 - accuracy: 0.8633\n",
            "295/938 [========>.....................] - ETA: 4s - loss: 0.5098 - accuracy: 0.8624\n",
            "309/938 [========>.....................] - ETA: 4s - loss: 0.5113 - accuracy: 0.8622\n",
            "323/938 [=========>....................] - ETA: 4s - loss: 0.5165 - accuracy: 0.8615\n",
            "337/938 [=========>....................] - ETA: 4s - loss: 0.5183 - accuracy: 0.8610\n",
            "351/938 [==========>...................] - ETA: 4s - loss: 0.5191 - accuracy: 0.8608\n",
            "365/938 [==========>...................] - ETA: 4s - loss: 0.5268 - accuracy: 0.8602\n",
            "379/938 [===========>..................] - ETA: 4s - loss: 0.5302 - accuracy: 0.8598\n",
            "393/938 [===========>..................] - ETA: 4s - loss: 0.5341 - accuracy: 0.8594\n",
            "407/938 [============>.................] - ETA: 3s - loss: 0.5340 - accuracy: 0.8585\n",
            "421/938 [============>.................] - ETA: 3s - loss: 0.5351 - accuracy: 0.8583\n",
            "435/938 [============>.................] - ETA: 3s - loss: 0.5318 - accuracy: 0.8595\n",
            "449/938 [=============>................] - ETA: 3s - loss: 0.5318 - accuracy: 0.8594\n",
            "463/938 [=============>................] - ETA: 3s - loss: 0.5343 - accuracy: 0.8589\n",
            "477/938 [==============>...............] - ETA: 3s - loss: 0.5336 - accuracy: 0.8589\n",
            "491/938 [==============>...............] - ETA: 3s - loss: 0.5306 - accuracy: 0.8593\n",
            "498/938 [==============>...............] - ETA: 3s - loss: 0.5313 - accuracy: 0.8592\n",
            "512/938 [===============>..............] - ETA: 3s - loss: 0.5304 - accuracy: 0.8597\n",
            "526/938 [===============>..............] - ETA: 3s - loss: 0.5273 - accuracy: 0.8602\n",
            "540/938 [================>.............] - ETA: 2s - loss: 0.5308 - accuracy: 0.8601\n",
            "554/938 [================>.............] - ETA: 2s - loss: 0.5315 - accuracy: 0.8600\n",
            "568/938 [=================>............] - ETA: 2s - loss: 0.5318 - accuracy: 0.8597\n",
            "582/938 [=================>............] - ETA: 2s - loss: 0.5322 - accuracy: 0.8596\n",
            "596/938 [==================>...........] - ETA: 2s - loss: 0.5333 - accuracy: 0.8591\n",
            "610/938 [==================>...........] - ETA: 2s - loss: 0.5349 - accuracy: 0.8590\n",
            "624/938 [==================>...........] - ETA: 2s - loss: 0.5357 - accuracy: 0.8586\n",
            "638/938 [===================>..........] - ETA: 2s - loss: 0.5354 - accuracy: 0.8585\n",
            "652/938 [===================>..........] - ETA: 2s - loss: 0.5350 - accuracy: 0.8585\n",
            "666/938 [====================>.........] - ETA: 2s - loss: 0.5352 - accuracy: 0.8587\n",
            "680/938 [====================>.........] - ETA: 1s - loss: 0.5342 - accuracy: 0.8590\n",
            "694/938 [=====================>........] - ETA: 1s - loss: 0.5344 - accuracy: 0.8591\n",
            "708/938 [=====================>........] - ETA: 1s - loss: 0.5344 - accuracy: 0.8591\n",
            "722/938 [======================>.......] - ETA: 1s - loss: 0.5323 - accuracy: 0.8596\n",
            "736/938 [======================>.......] - ETA: 1s - loss: 0.5320 - accuracy: 0.8597\n",
            "750/938 [======================>.......] - ETA: 1s - loss: 0.5318 - accuracy: 0.8595\n",
            "757/938 [=======================>......] - ETA: 1s - loss: 0.5313 - accuracy: 0.8596\n",
            "771/938 [=======================>......] - ETA: 1s - loss: 0.5310 - accuracy: 0.8596\n",
            "785/938 [========================>.....] - ETA: 1s - loss: 0.5296 - accuracy: 0.8600\n",
            "799/938 [========================>.....] - ETA: 1s - loss: 0.5285 - accuracy: 0.8602\n",
            "813/938 [=========================>....] - ETA: 0s - loss: 0.5287 - accuracy: 0.8602\n",
            "827/938 [=========================>....] - ETA: 0s - loss: 0.5281 - accuracy: 0.8605\n",
            "841/938 [=========================>....] - ETA: 0s - loss: 0.5283 - accuracy: 0.8603\n",
            "855/938 [==========================>...] - ETA: 0s - loss: 0.5276 - accuracy: 0.8604\n",
            "869/938 [==========================>...] - ETA: 0s - loss: 0.5267 - accuracy: 0.8607\n",
            "883/938 [===========================>..] - ETA: 0s - loss: 0.5263 - accuracy: 0.8606\n",
            "897/938 [===========================>..] - ETA: 0s - loss: 0.5273 - accuracy: 0.8608\n",
            "911/938 [============================>.] - ETA: 0s - loss: 0.5270 - accuracy: 0.8607\n",
            "925/938 [============================>.] - ETA: 0s - loss: 0.5266 - accuracy: 0.8612\n",
            "932/938 [============================>.] - ETA: 0s - loss: 0.5261 - accuracy: 0.8613\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.5254 - accuracy: 0.8615 - val_loss: 0.4467 - val_accuracy: 0.9004\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 7/12\n",
            "  8/938 [..............................] - ETA: 7s - loss: 0.4339 - accuracy: 0.8770\n",
            " 22/938 [..............................] - ETA: 6s - loss: 0.4719 - accuracy: 0.8693\n",
            " 36/938 [>.............................] - ETA: 6s - loss: 0.5169 - accuracy: 0.8659\n",
            " 50/938 [>.............................] - ETA: 6s - loss: 0.5406 - accuracy: 0.8591\n",
            " 64/938 [=>............................] - ETA: 6s - loss: 0.5460 - accuracy: 0.8628\n",
            " 78/938 [=>............................] - ETA: 6s - loss: 0.5424 - accuracy: 0.8606\n",
            " 92/938 [=>............................] - ETA: 6s - loss: 0.5291 - accuracy: 0.8629\n",
            "106/938 [==>...........................] - ETA: 6s - loss: 0.5216 - accuracy: 0.8654\n",
            "120/938 [==>...........................] - ETA: 5s - loss: 0.5146 - accuracy: 0.8647\n",
            "134/938 [===>..........................] - ETA: 5s - loss: 0.5140 - accuracy: 0.8651\n",
            "141/938 [===>..........................] - ETA: 5s - loss: 0.5110 - accuracy: 0.8667\n",
            "155/938 [===>..........................] - ETA: 5s - loss: 0.5107 - accuracy: 0.8686\n",
            "169/938 [====>.........................] - ETA: 5s - loss: 0.5162 - accuracy: 0.8671\n",
            "183/938 [====>.........................] - ETA: 5s - loss: 0.5122 - accuracy: 0.8666\n",
            "197/938 [=====>........................] - ETA: 5s - loss: 0.5118 - accuracy: 0.8676\n",
            "211/938 [=====>........................] - ETA: 5s - loss: 0.5109 - accuracy: 0.8662\n",
            "225/938 [======>.......................] - ETA: 5s - loss: 0.5121 - accuracy: 0.8669\n",
            "239/938 [======>.......................] - ETA: 5s - loss: 0.5159 - accuracy: 0.8660\n",
            "253/938 [=======>......................] - ETA: 5s - loss: 0.5156 - accuracy: 0.8647\n",
            "267/938 [=======>......................] - ETA: 4s - loss: 0.5184 - accuracy: 0.8645\n",
            "281/938 [=======>......................] - ETA: 4s - loss: 0.5193 - accuracy: 0.8634\n",
            "295/938 [========>.....................] - ETA: 4s - loss: 0.5165 - accuracy: 0.8632\n",
            "309/938 [========>.....................] - ETA: 4s - loss: 0.5169 - accuracy: 0.8625\n",
            "323/938 [=========>....................] - ETA: 4s - loss: 0.5185 - accuracy: 0.8630\n",
            "337/938 [=========>....................] - ETA: 4s - loss: 0.5177 - accuracy: 0.8631\n",
            "351/938 [==========>...................] - ETA: 4s - loss: 0.5148 - accuracy: 0.8637\n",
            "365/938 [==========>...................] - ETA: 4s - loss: 0.5158 - accuracy: 0.8633\n",
            "379/938 [===========>..................] - ETA: 4s - loss: 0.5188 - accuracy: 0.8630\n",
            "393/938 [===========>..................] - ETA: 4s - loss: 0.5174 - accuracy: 0.8638\n",
            "407/938 [============>.................] - ETA: 3s - loss: 0.5158 - accuracy: 0.8639\n",
            "421/938 [============>.................] - ETA: 3s - loss: 0.5169 - accuracy: 0.8641\n",
            "435/938 [============>.................] - ETA: 3s - loss: 0.5165 - accuracy: 0.8642\n",
            "449/938 [=============>................] - ETA: 3s - loss: 0.5167 - accuracy: 0.8640\n",
            "463/938 [=============>................] - ETA: 3s - loss: 0.5176 - accuracy: 0.8639\n",
            "477/938 [==============>...............] - ETA: 3s - loss: 0.5155 - accuracy: 0.8642\n",
            "491/938 [==============>...............] - ETA: 3s - loss: 0.5206 - accuracy: 0.8632\n",
            "505/938 [===============>..............] - ETA: 3s - loss: 0.5204 - accuracy: 0.8637\n",
            "519/938 [===============>..............] - ETA: 3s - loss: 0.5198 - accuracy: 0.8638\n",
            "533/938 [================>.............] - ETA: 3s - loss: 0.5183 - accuracy: 0.8639\n",
            "547/938 [================>.............] - ETA: 2s - loss: 0.5181 - accuracy: 0.8638\n",
            "561/938 [================>.............] - ETA: 2s - loss: 0.5163 - accuracy: 0.8641\n",
            "575/938 [=================>............] - ETA: 2s - loss: 0.5156 - accuracy: 0.8642\n",
            "589/938 [=================>............] - ETA: 2s - loss: 0.5170 - accuracy: 0.8642\n",
            "603/938 [==================>...........] - ETA: 2s - loss: 0.5177 - accuracy: 0.8639\n",
            "617/938 [==================>...........] - ETA: 2s - loss: 0.5192 - accuracy: 0.8636\n",
            "631/938 [===================>..........] - ETA: 2s - loss: 0.5215 - accuracy: 0.8632\n",
            "645/938 [===================>..........] - ETA: 2s - loss: 0.5253 - accuracy: 0.8629\n",
            "659/938 [====================>.........] - ETA: 2s - loss: 0.5268 - accuracy: 0.8626\n",
            "674/938 [====================>.........] - ETA: 1s - loss: 0.5259 - accuracy: 0.8623\n",
            "689/938 [=====================>........] - ETA: 1s - loss: 0.5245 - accuracy: 0.8623\n",
            "703/938 [=====================>........] - ETA: 1s - loss: 0.5223 - accuracy: 0.8625\n",
            "718/938 [=====================>........] - ETA: 1s - loss: 0.5225 - accuracy: 0.8626\n",
            "732/938 [======================>.......] - ETA: 1s - loss: 0.5223 - accuracy: 0.8627\n",
            "747/938 [======================>.......] - ETA: 1s - loss: 0.5209 - accuracy: 0.8629\n",
            "761/938 [=======================>......] - ETA: 1s - loss: 0.5212 - accuracy: 0.8627\n",
            "776/938 [=======================>......] - ETA: 1s - loss: 0.5198 - accuracy: 0.8630\n",
            "790/938 [========================>.....] - ETA: 1s - loss: 0.5195 - accuracy: 0.8629\n",
            "797/938 [========================>.....] - ETA: 1s - loss: 0.5190 - accuracy: 0.8631\n",
            "811/938 [========================>.....] - ETA: 0s - loss: 0.5194 - accuracy: 0.8632\n",
            "825/938 [=========================>....] - ETA: 0s - loss: 0.5193 - accuracy: 0.8632\n",
            "839/938 [=========================>....] - ETA: 0s - loss: 0.5189 - accuracy: 0.8632\n",
            "853/938 [==========================>...] - ETA: 0s - loss: 0.5178 - accuracy: 0.8634\n",
            "868/938 [==========================>...] - ETA: 0s - loss: 0.5162 - accuracy: 0.8638\n",
            "884/938 [===========================>..] - ETA: 0s - loss: 0.5151 - accuracy: 0.8639\n",
            "900/938 [===========================>..] - ETA: 0s - loss: 0.5139 - accuracy: 0.8641\n",
            "907/938 [============================>.] - ETA: 0s - loss: 0.5138 - accuracy: 0.8643\n",
            "915/938 [============================>.] - ETA: 0s - loss: 0.5125 - accuracy: 0.8644\n",
            "930/938 [============================>.] - ETA: 0s - loss: 0.5116 - accuracy: 0.8646\n",
            "938/938 [==============================] - ETA: 0s - loss: 0.5123 - accuracy: 0.8647\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.5123 - accuracy: 0.8647 - val_loss: 0.4156 - val_accuracy: 0.8971\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 8/12\n",
            "  8/938 [..............................] - ETA: 6s - loss: 0.6799 - accuracy: 0.8457\n",
            " 23/938 [..............................] - ETA: 6s - loss: 0.6962 - accuracy: 0.8363\n",
            " 38/938 [>.............................] - ETA: 6s - loss: 0.6887 - accuracy: 0.8355\n",
            " 52/938 [>.............................] - ETA: 6s - loss: 0.6297 - accuracy: 0.8419\n",
            " 66/938 [=>............................] - ETA: 6s - loss: 0.6201 - accuracy: 0.8440\n",
            " 80/938 [=>............................] - ETA: 6s - loss: 0.6040 - accuracy: 0.8486\n",
            " 94/938 [==>...........................] - ETA: 6s - loss: 0.5991 - accuracy: 0.8466\n",
            "108/938 [==>...........................] - ETA: 6s - loss: 0.5924 - accuracy: 0.8471\n",
            "122/938 [==>...........................] - ETA: 5s - loss: 0.5847 - accuracy: 0.8485\n",
            "136/938 [===>..........................] - ETA: 5s - loss: 0.5767 - accuracy: 0.8518\n",
            "150/938 [===>..........................] - ETA: 5s - loss: 0.5532 - accuracy: 0.8570\n",
            "165/938 [====>.........................] - ETA: 5s - loss: 0.5456 - accuracy: 0.8591\n",
            "179/938 [====>.........................] - ETA: 5s - loss: 0.5361 - accuracy: 0.8614\n",
            "194/938 [=====>........................] - ETA: 5s - loss: 0.5396 - accuracy: 0.8623\n",
            "209/938 [=====>........................] - ETA: 5s - loss: 0.5356 - accuracy: 0.8620\n",
            "223/938 [======>.......................] - ETA: 5s - loss: 0.5364 - accuracy: 0.8624\n",
            "237/938 [======>.......................] - ETA: 5s - loss: 0.5351 - accuracy: 0.8629\n",
            "251/938 [=======>......................] - ETA: 5s - loss: 0.5317 - accuracy: 0.8635\n",
            "267/938 [=======>......................] - ETA: 4s - loss: 0.5254 - accuracy: 0.8651\n",
            "282/938 [========>.....................] - ETA: 4s - loss: 0.5209 - accuracy: 0.8652\n",
            "296/938 [========>.....................] - ETA: 4s - loss: 0.5185 - accuracy: 0.8652\n",
            "311/938 [========>.....................] - ETA: 4s - loss: 0.5146 - accuracy: 0.8658\n",
            "325/938 [=========>....................] - ETA: 4s - loss: 0.5152 - accuracy: 0.8661\n",
            "339/938 [=========>....................] - ETA: 4s - loss: 0.5152 - accuracy: 0.8656\n",
            "346/938 [==========>...................] - ETA: 4s - loss: 0.5143 - accuracy: 0.8657\n",
            "360/938 [==========>...................] - ETA: 4s - loss: 0.5153 - accuracy: 0.8657\n",
            "374/938 [==========>...................] - ETA: 4s - loss: 0.5135 - accuracy: 0.8661\n",
            "388/938 [===========>..................] - ETA: 4s - loss: 0.5150 - accuracy: 0.8659\n",
            "402/938 [===========>..................] - ETA: 3s - loss: 0.5116 - accuracy: 0.8666\n",
            "416/938 [============>.................] - ETA: 3s - loss: 0.5086 - accuracy: 0.8666\n",
            "430/938 [============>.................] - ETA: 3s - loss: 0.5078 - accuracy: 0.8668\n",
            "444/938 [=============>................] - ETA: 3s - loss: 0.5062 - accuracy: 0.8675\n",
            "458/938 [=============>................] - ETA: 3s - loss: 0.5057 - accuracy: 0.8681\n",
            "472/938 [==============>...............] - ETA: 3s - loss: 0.5057 - accuracy: 0.8684\n",
            "486/938 [==============>...............] - ETA: 3s - loss: 0.5059 - accuracy: 0.8684\n",
            "500/938 [==============>...............] - ETA: 3s - loss: 0.5061 - accuracy: 0.8684\n",
            "514/938 [===============>..............] - ETA: 3s - loss: 0.5061 - accuracy: 0.8690\n",
            "528/938 [===============>..............] - ETA: 3s - loss: 0.5071 - accuracy: 0.8681\n",
            "542/938 [================>.............] - ETA: 2s - loss: 0.5075 - accuracy: 0.8681\n",
            "556/938 [================>.............] - ETA: 2s - loss: 0.5087 - accuracy: 0.8678\n",
            "570/938 [=================>............] - ETA: 2s - loss: 0.5070 - accuracy: 0.8678\n",
            "585/938 [=================>............] - ETA: 2s - loss: 0.5046 - accuracy: 0.8681\n",
            "599/938 [==================>...........] - ETA: 2s - loss: 0.5028 - accuracy: 0.8684\n",
            "606/938 [==================>...........] - ETA: 2s - loss: 0.5029 - accuracy: 0.8685\n",
            "620/938 [==================>...........] - ETA: 2s - loss: 0.5026 - accuracy: 0.8685\n",
            "634/938 [===================>..........] - ETA: 2s - loss: 0.5007 - accuracy: 0.8688\n",
            "648/938 [===================>..........] - ETA: 2s - loss: 0.4996 - accuracy: 0.8692\n",
            "662/938 [====================>.........] - ETA: 2s - loss: 0.4979 - accuracy: 0.8698\n",
            "676/938 [====================>.........] - ETA: 1s - loss: 0.4962 - accuracy: 0.8698\n",
            "691/938 [=====================>........] - ETA: 1s - loss: 0.4964 - accuracy: 0.8700\n",
            "705/938 [=====================>........] - ETA: 1s - loss: 0.4960 - accuracy: 0.8699\n",
            "719/938 [=====================>........] - ETA: 1s - loss: 0.4942 - accuracy: 0.8702\n",
            "733/938 [======================>.......] - ETA: 1s - loss: 0.4933 - accuracy: 0.8703\n",
            "748/938 [======================>.......] - ETA: 1s - loss: 0.4935 - accuracy: 0.8704\n",
            "762/938 [=======================>......] - ETA: 1s - loss: 0.4945 - accuracy: 0.8706\n",
            "775/938 [=======================>......] - ETA: 1s - loss: 0.4959 - accuracy: 0.8703\n",
            "789/938 [========================>.....] - ETA: 1s - loss: 0.4972 - accuracy: 0.8699\n",
            "803/938 [========================>.....] - ETA: 0s - loss: 0.4972 - accuracy: 0.8700\n",
            "817/938 [=========================>....] - ETA: 0s - loss: 0.4958 - accuracy: 0.8705\n",
            "831/938 [=========================>....] - ETA: 0s - loss: 0.4960 - accuracy: 0.8708\n",
            "845/938 [==========================>...] - ETA: 0s - loss: 0.4978 - accuracy: 0.8706\n",
            "859/938 [==========================>...] - ETA: 0s - loss: 0.4981 - accuracy: 0.8703\n",
            "866/938 [==========================>...] - ETA: 0s - loss: 0.5004 - accuracy: 0.8698\n",
            "880/938 [===========================>..] - ETA: 0s - loss: 0.5059 - accuracy: 0.8687\n",
            "894/938 [===========================>..] - ETA: 0s - loss: 0.5112 - accuracy: 0.8675\n",
            "908/938 [============================>.] - ETA: 0s - loss: 0.5144 - accuracy: 0.8670\n",
            "922/938 [============================>.] - ETA: 0s - loss: 0.5173 - accuracy: 0.8663\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.5203 - accuracy: 0.8651\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.5202 - accuracy: 0.8651 - val_loss: 0.5450 - val_accuracy: 0.8483\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 9/12\n",
            "  1/938 [..............................] - ETA: 7s - loss: 0.5949 - accuracy: 0.8594\n",
            " 15/938 [..............................] - ETA: 6s - loss: 0.6475 - accuracy: 0.8323\n",
            " 30/938 [..............................] - ETA: 6s - loss: 0.6154 - accuracy: 0.8453\n",
            " 44/938 [>.............................] - ETA: 6s - loss: 0.6154 - accuracy: 0.8366\n",
            " 58/938 [>.............................] - ETA: 6s - loss: 0.5954 - accuracy: 0.8408\n",
            " 72/938 [=>............................] - ETA: 6s - loss: 0.5903 - accuracy: 0.8418\n",
            " 86/938 [=>............................] - ETA: 6s - loss: 0.5804 - accuracy: 0.8434\n",
            "100/938 [==>...........................] - ETA: 6s - loss: 0.5783 - accuracy: 0.8445\n",
            "115/938 [==>...........................] - ETA: 6s - loss: 0.5659 - accuracy: 0.8450\n",
            "129/938 [===>..........................] - ETA: 5s - loss: 0.5582 - accuracy: 0.8481\n",
            "143/938 [===>..........................] - ETA: 5s - loss: 0.5471 - accuracy: 0.8501\n",
            "157/938 [====>.........................] - ETA: 5s - loss: 0.5341 - accuracy: 0.8530\n",
            "178/938 [====>.........................] - ETA: 5s - loss: 0.5304 - accuracy: 0.8534\n",
            "192/938 [=====>........................] - ETA: 5s - loss: 0.5282 - accuracy: 0.8547\n",
            "206/938 [=====>........................] - ETA: 5s - loss: 0.5220 - accuracy: 0.8551\n",
            "221/938 [======>.......................] - ETA: 5s - loss: 0.5229 - accuracy: 0.8546\n",
            "235/938 [======>.......................] - ETA: 5s - loss: 0.5197 - accuracy: 0.8553\n",
            "249/938 [======>.......................] - ETA: 5s - loss: 0.5147 - accuracy: 0.8572\n",
            "263/938 [=======>......................] - ETA: 4s - loss: 0.5134 - accuracy: 0.8577\n",
            "277/938 [=======>......................] - ETA: 4s - loss: 0.5151 - accuracy: 0.8573\n",
            "291/938 [========>.....................] - ETA: 4s - loss: 0.5136 - accuracy: 0.8581\n",
            "305/938 [========>.....................] - ETA: 4s - loss: 0.5155 - accuracy: 0.8587\n",
            "319/938 [=========>....................] - ETA: 4s - loss: 0.5163 - accuracy: 0.8586\n",
            "333/938 [=========>....................] - ETA: 4s - loss: 0.5121 - accuracy: 0.8594\n",
            "347/938 [==========>...................] - ETA: 4s - loss: 0.5124 - accuracy: 0.8598\n",
            "362/938 [==========>...................] - ETA: 4s - loss: 0.5103 - accuracy: 0.8604\n",
            "376/938 [===========>..................] - ETA: 4s - loss: 0.5102 - accuracy: 0.8604\n",
            "390/938 [===========>..................] - ETA: 4s - loss: 0.5081 - accuracy: 0.8611\n",
            "404/938 [===========>..................] - ETA: 3s - loss: 0.5066 - accuracy: 0.8618\n",
            "419/938 [============>.................] - ETA: 3s - loss: 0.5047 - accuracy: 0.8625\n",
            "433/938 [============>.................] - ETA: 3s - loss: 0.5035 - accuracy: 0.8633\n",
            "447/938 [=============>................] - ETA: 3s - loss: 0.5043 - accuracy: 0.8637\n",
            "461/938 [=============>................] - ETA: 3s - loss: 0.5059 - accuracy: 0.8634\n",
            "475/938 [==============>...............] - ETA: 3s - loss: 0.5043 - accuracy: 0.8637\n",
            "490/938 [==============>...............] - ETA: 3s - loss: 0.5038 - accuracy: 0.8635\n",
            "504/938 [===============>..............] - ETA: 3s - loss: 0.5036 - accuracy: 0.8636\n",
            "518/938 [===============>..............] - ETA: 3s - loss: 0.5012 - accuracy: 0.8640\n",
            "525/938 [===============>..............] - ETA: 3s - loss: 0.4998 - accuracy: 0.8642\n",
            "539/938 [================>.............] - ETA: 2s - loss: 0.4989 - accuracy: 0.8642\n",
            "553/938 [================>.............] - ETA: 2s - loss: 0.4986 - accuracy: 0.8644\n",
            "567/938 [=================>............] - ETA: 2s - loss: 0.4973 - accuracy: 0.8645\n",
            "581/938 [=================>............] - ETA: 2s - loss: 0.4960 - accuracy: 0.8646\n",
            "595/938 [==================>...........] - ETA: 2s - loss: 0.4962 - accuracy: 0.8649\n",
            "609/938 [==================>...........] - ETA: 2s - loss: 0.4927 - accuracy: 0.8656\n",
            "623/938 [==================>...........] - ETA: 2s - loss: 0.4909 - accuracy: 0.8660\n",
            "637/938 [===================>..........] - ETA: 2s - loss: 0.4896 - accuracy: 0.8660\n",
            "651/938 [===================>..........] - ETA: 2s - loss: 0.4871 - accuracy: 0.8666\n",
            "665/938 [====================>.........] - ETA: 1s - loss: 0.4868 - accuracy: 0.8668\n",
            "679/938 [====================>.........] - ETA: 1s - loss: 0.4866 - accuracy: 0.8670\n",
            "686/938 [====================>.........] - ETA: 1s - loss: 0.4865 - accuracy: 0.8670\n",
            "700/938 [=====================>........] - ETA: 1s - loss: 0.4874 - accuracy: 0.8669\n",
            "714/938 [=====================>........] - ETA: 1s - loss: 0.4896 - accuracy: 0.8666\n",
            "728/938 [======================>.......] - ETA: 1s - loss: 0.4899 - accuracy: 0.8669\n",
            "742/938 [======================>.......] - ETA: 1s - loss: 0.4940 - accuracy: 0.8663\n",
            "756/938 [=======================>......] - ETA: 1s - loss: 0.4955 - accuracy: 0.8656\n",
            "770/938 [=======================>......] - ETA: 1s - loss: 0.4980 - accuracy: 0.8653\n",
            "784/938 [========================>.....] - ETA: 1s - loss: 0.4992 - accuracy: 0.8650\n",
            "798/938 [========================>.....] - ETA: 1s - loss: 0.4995 - accuracy: 0.8652\n",
            "812/938 [========================>.....] - ETA: 0s - loss: 0.4989 - accuracy: 0.8655\n",
            "826/938 [=========================>....] - ETA: 0s - loss: 0.4989 - accuracy: 0.8658\n",
            "840/938 [=========================>....] - ETA: 0s - loss: 0.4987 - accuracy: 0.8662\n",
            "854/938 [==========================>...] - ETA: 0s - loss: 0.4974 - accuracy: 0.8664\n",
            "868/938 [==========================>...] - ETA: 0s - loss: 0.4968 - accuracy: 0.8667\n",
            "882/938 [===========================>..] - ETA: 0s - loss: 0.4961 - accuracy: 0.8670\n",
            "896/938 [===========================>..] - ETA: 0s - loss: 0.4971 - accuracy: 0.8670\n",
            "903/938 [===========================>..] - ETA: 0s - loss: 0.4970 - accuracy: 0.8669\n",
            "917/938 [============================>.] - ETA: 0s - loss: 0.4966 - accuracy: 0.8669\n",
            "932/938 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.8668\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.4975 - accuracy: 0.8669 - val_loss: 0.3898 - val_accuracy: 0.9100\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 10/12\n",
            "  8/938 [..............................] - ETA: 7s - loss: 0.4355 - accuracy: 0.8711\n",
            " 22/938 [..............................] - ETA: 6s - loss: 0.5389 - accuracy: 0.8572\n",
            " 36/938 [>.............................] - ETA: 6s - loss: 0.5560 - accuracy: 0.8607\n",
            " 50/938 [>.............................] - ETA: 6s - loss: 0.5532 - accuracy: 0.8606\n",
            " 64/938 [=>............................] - ETA: 6s - loss: 0.5596 - accuracy: 0.8601\n",
            " 78/938 [=>............................] - ETA: 6s - loss: 0.5522 - accuracy: 0.8618\n",
            " 93/938 [=>............................] - ETA: 6s - loss: 0.5640 - accuracy: 0.8569\n",
            "107/938 [==>...........................] - ETA: 6s - loss: 0.5790 - accuracy: 0.8597\n",
            "122/938 [==>...........................] - ETA: 5s - loss: 0.5660 - accuracy: 0.8616\n",
            "136/938 [===>..........................] - ETA: 5s - loss: 0.5655 - accuracy: 0.8614\n",
            "151/938 [===>..........................] - ETA: 5s - loss: 0.5554 - accuracy: 0.8632\n",
            "165/938 [====>.........................] - ETA: 5s - loss: 0.5513 - accuracy: 0.8631\n",
            "179/938 [====>.........................] - ETA: 5s - loss: 0.5478 - accuracy: 0.8628\n",
            "193/938 [=====>........................] - ETA: 5s - loss: 0.5399 - accuracy: 0.8637\n",
            "207/938 [=====>........................] - ETA: 5s - loss: 0.5372 - accuracy: 0.8634\n",
            "222/938 [======>.......................] - ETA: 5s - loss: 0.5329 - accuracy: 0.8637\n",
            "236/938 [======>.......................] - ETA: 5s - loss: 0.5340 - accuracy: 0.8633\n",
            "250/938 [======>.......................] - ETA: 5s - loss: 0.5259 - accuracy: 0.8642\n",
            "265/938 [=======>......................] - ETA: 4s - loss: 0.5247 - accuracy: 0.8649\n",
            "272/938 [=======>......................] - ETA: 4s - loss: 0.5257 - accuracy: 0.8647\n",
            "287/938 [========>.....................] - ETA: 4s - loss: 0.5250 - accuracy: 0.8648\n",
            "301/938 [========>.....................] - ETA: 4s - loss: 0.5211 - accuracy: 0.8651\n",
            "315/938 [=========>....................] - ETA: 4s - loss: 0.5219 - accuracy: 0.8652\n",
            "329/938 [=========>....................] - ETA: 4s - loss: 0.5193 - accuracy: 0.8661\n",
            "343/938 [=========>....................] - ETA: 4s - loss: 0.5168 - accuracy: 0.8664\n",
            "357/938 [==========>...................] - ETA: 4s - loss: 0.5146 - accuracy: 0.8669\n",
            "371/938 [==========>...................] - ETA: 4s - loss: 0.5118 - accuracy: 0.8673\n",
            "386/938 [===========>..................] - ETA: 4s - loss: 0.5078 - accuracy: 0.8681\n",
            "401/938 [===========>..................] - ETA: 3s - loss: 0.5069 - accuracy: 0.8682\n",
            "415/938 [============>.................] - ETA: 3s - loss: 0.5063 - accuracy: 0.8686\n",
            "429/938 [============>.................] - ETA: 3s - loss: 0.5064 - accuracy: 0.8687\n",
            "443/938 [=============>................] - ETA: 3s - loss: 0.5063 - accuracy: 0.8689\n",
            "457/938 [=============>................] - ETA: 3s - loss: 0.5079 - accuracy: 0.8688\n",
            "471/938 [==============>...............] - ETA: 3s - loss: 0.5075 - accuracy: 0.8693\n",
            "485/938 [==============>...............] - ETA: 3s - loss: 0.5040 - accuracy: 0.8697\n",
            "499/938 [==============>...............] - ETA: 3s - loss: 0.5045 - accuracy: 0.8698\n",
            "506/938 [===============>..............] - ETA: 3s - loss: 0.5052 - accuracy: 0.8693\n",
            "520/938 [===============>..............] - ETA: 3s - loss: 0.5056 - accuracy: 0.8694\n",
            "534/938 [================>.............] - ETA: 2s - loss: 0.5060 - accuracy: 0.8691\n",
            "548/938 [================>.............] - ETA: 2s - loss: 0.5030 - accuracy: 0.8696\n",
            "562/938 [================>.............] - ETA: 2s - loss: 0.5043 - accuracy: 0.8694\n",
            "576/938 [=================>............] - ETA: 2s - loss: 0.5053 - accuracy: 0.8692\n",
            "590/938 [=================>............] - ETA: 2s - loss: 0.5092 - accuracy: 0.8690\n",
            "605/938 [==================>...........] - ETA: 2s - loss: 0.5174 - accuracy: 0.8686\n",
            "619/938 [==================>...........] - ETA: 2s - loss: 0.5214 - accuracy: 0.8678\n",
            "633/938 [===================>..........] - ETA: 2s - loss: 0.5290 - accuracy: 0.8662\n",
            "648/938 [===================>..........] - ETA: 2s - loss: 0.5315 - accuracy: 0.8654\n",
            "662/938 [====================>.........] - ETA: 2s - loss: 0.5363 - accuracy: 0.8643\n",
            "676/938 [====================>.........] - ETA: 1s - loss: 0.5406 - accuracy: 0.8635\n",
            "690/938 [=====================>........] - ETA: 1s - loss: 0.5405 - accuracy: 0.8636\n",
            "704/938 [=====================>........] - ETA: 1s - loss: 0.5447 - accuracy: 0.8630\n",
            "718/938 [=====================>........] - ETA: 1s - loss: 0.5466 - accuracy: 0.8622\n",
            "739/938 [======================>.......] - ETA: 1s - loss: 0.5463 - accuracy: 0.8618\n",
            "753/938 [=======================>......] - ETA: 1s - loss: 0.5471 - accuracy: 0.8616\n",
            "767/938 [=======================>......] - ETA: 1s - loss: 0.5481 - accuracy: 0.8611\n",
            "781/938 [=======================>......] - ETA: 1s - loss: 0.5472 - accuracy: 0.8609\n",
            "795/938 [========================>.....] - ETA: 1s - loss: 0.5494 - accuracy: 0.8605\n",
            "809/938 [========================>.....] - ETA: 0s - loss: 0.5508 - accuracy: 0.8605\n",
            "823/938 [=========================>....] - ETA: 0s - loss: 0.5514 - accuracy: 0.8606\n",
            "837/938 [=========================>....] - ETA: 0s - loss: 0.5527 - accuracy: 0.8606\n",
            "851/938 [==========================>...] - ETA: 0s - loss: 0.5522 - accuracy: 0.8606\n",
            "865/938 [==========================>...] - ETA: 0s - loss: 0.5511 - accuracy: 0.8609\n",
            "879/938 [===========================>..] - ETA: 0s - loss: 0.5512 - accuracy: 0.8608\n",
            "893/938 [===========================>..] - ETA: 0s - loss: 0.5510 - accuracy: 0.8609\n",
            "914/938 [============================>.] - ETA: 0s - loss: 0.5501 - accuracy: 0.8610\n",
            "928/938 [============================>.] - ETA: 0s - loss: 0.5502 - accuracy: 0.8611\n",
            "935/938 [============================>.] - ETA: 0s - loss: 0.5505 - accuracy: 0.8612\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.5511 - accuracy: 0.8611 - val_loss: 0.5284 - val_accuracy: 0.8588\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 11/12\n",
            "  1/938 [..............................] - ETA: 9s - loss: 0.7040 - accuracy: 0.7969\n",
            " 15/938 [..............................] - ETA: 6s - loss: 0.5600 - accuracy: 0.8594\n",
            " 29/938 [..............................] - ETA: 6s - loss: 0.5583 - accuracy: 0.8610\n",
            " 43/938 [>.............................] - ETA: 6s - loss: 0.5519 - accuracy: 0.8677\n",
            " 57/938 [>.............................] - ETA: 6s - loss: 0.5581 - accuracy: 0.8635\n",
            " 71/938 [=>............................] - ETA: 6s - loss: 0.5446 - accuracy: 0.8647\n",
            " 78/938 [=>............................] - ETA: 6s - loss: 0.5375 - accuracy: 0.8654\n",
            " 92/938 [=>............................] - ETA: 6s - loss: 0.5321 - accuracy: 0.8636\n",
            "106/938 [==>...........................] - ETA: 6s - loss: 0.5362 - accuracy: 0.8613\n",
            "120/938 [==>...........................] - ETA: 6s - loss: 0.5502 - accuracy: 0.8622\n",
            "134/938 [===>..........................] - ETA: 5s - loss: 0.5618 - accuracy: 0.8591\n",
            "148/938 [===>..........................] - ETA: 5s - loss: 0.5518 - accuracy: 0.8598\n",
            "162/938 [====>.........................] - ETA: 5s - loss: 0.5412 - accuracy: 0.8619\n",
            "176/938 [====>.........................] - ETA: 5s - loss: 0.5276 - accuracy: 0.8652\n",
            "190/938 [=====>........................] - ETA: 5s - loss: 0.5354 - accuracy: 0.8647\n",
            "204/938 [=====>........................] - ETA: 5s - loss: 0.5441 - accuracy: 0.8634\n",
            "220/938 [======>.......................] - ETA: 5s - loss: 0.5367 - accuracy: 0.8647\n",
            "234/938 [======>.......................] - ETA: 5s - loss: 0.5397 - accuracy: 0.8634\n",
            "248/938 [======>.......................] - ETA: 5s - loss: 0.5361 - accuracy: 0.8648\n",
            "262/938 [=======>......................] - ETA: 4s - loss: 0.5319 - accuracy: 0.8656\n",
            "276/938 [=======>......................] - ETA: 4s - loss: 0.5367 - accuracy: 0.8644\n",
            "290/938 [========>.....................] - ETA: 4s - loss: 0.5367 - accuracy: 0.8638\n",
            "297/938 [========>.....................] - ETA: 4s - loss: 0.5334 - accuracy: 0.8639\n",
            "311/938 [========>.....................] - ETA: 4s - loss: 0.5299 - accuracy: 0.8648\n",
            "325/938 [=========>....................] - ETA: 4s - loss: 0.5293 - accuracy: 0.8645\n",
            "339/938 [=========>....................] - ETA: 4s - loss: 0.5284 - accuracy: 0.8644\n",
            "354/938 [==========>...................] - ETA: 4s - loss: 0.5275 - accuracy: 0.8643\n",
            "368/938 [==========>...................] - ETA: 4s - loss: 0.5276 - accuracy: 0.8648\n",
            "382/938 [===========>..................] - ETA: 4s - loss: 0.5236 - accuracy: 0.8657\n",
            "396/938 [===========>..................] - ETA: 3s - loss: 0.5248 - accuracy: 0.8649\n",
            "410/938 [============>.................] - ETA: 3s - loss: 0.5242 - accuracy: 0.8650\n",
            "424/938 [============>.................] - ETA: 3s - loss: 0.5235 - accuracy: 0.8648\n",
            "438/938 [=============>................] - ETA: 3s - loss: 0.5233 - accuracy: 0.8644\n",
            "452/938 [=============>................] - ETA: 3s - loss: 0.5250 - accuracy: 0.8643\n",
            "467/938 [=============>................] - ETA: 3s - loss: 0.5309 - accuracy: 0.8629\n",
            "481/938 [==============>...............] - ETA: 3s - loss: 0.5304 - accuracy: 0.8634\n",
            "495/938 [==============>...............] - ETA: 3s - loss: 0.5295 - accuracy: 0.8635\n",
            "502/938 [===============>..............] - ETA: 3s - loss: 0.5292 - accuracy: 0.8631\n",
            "510/938 [===============>..............] - ETA: 3s - loss: 0.5278 - accuracy: 0.8634\n",
            "517/938 [===============>..............] - ETA: 3s - loss: 0.5275 - accuracy: 0.8635\n",
            "531/938 [===============>..............] - ETA: 2s - loss: 0.5253 - accuracy: 0.8639\n",
            "546/938 [================>.............] - ETA: 2s - loss: 0.5255 - accuracy: 0.8642\n",
            "560/938 [================>.............] - ETA: 2s - loss: 0.5251 - accuracy: 0.8643\n",
            "574/938 [=================>............] - ETA: 2s - loss: 0.5239 - accuracy: 0.8643\n",
            "588/938 [=================>............] - ETA: 2s - loss: 0.5232 - accuracy: 0.8647\n",
            "602/938 [==================>...........] - ETA: 2s - loss: 0.5234 - accuracy: 0.8649\n",
            "616/938 [==================>...........] - ETA: 2s - loss: 0.5259 - accuracy: 0.8643\n",
            "630/938 [===================>..........] - ETA: 2s - loss: 0.5308 - accuracy: 0.8635\n",
            "644/938 [===================>..........] - ETA: 2s - loss: 0.5326 - accuracy: 0.8630\n",
            "658/938 [====================>.........] - ETA: 2s - loss: 0.5346 - accuracy: 0.8627\n",
            "672/938 [====================>.........] - ETA: 1s - loss: 0.5340 - accuracy: 0.8627\n",
            "686/938 [====================>.........] - ETA: 1s - loss: 0.5324 - accuracy: 0.8628\n",
            "700/938 [=====================>........] - ETA: 1s - loss: 0.5302 - accuracy: 0.8634\n",
            "714/938 [=====================>........] - ETA: 1s - loss: 0.5309 - accuracy: 0.8632\n",
            "728/938 [======================>.......] - ETA: 1s - loss: 0.5297 - accuracy: 0.8633\n",
            "742/938 [======================>.......] - ETA: 1s - loss: 0.5294 - accuracy: 0.8630\n",
            "756/938 [=======================>......] - ETA: 1s - loss: 0.5288 - accuracy: 0.8632\n",
            "770/938 [=======================>......] - ETA: 1s - loss: 0.5271 - accuracy: 0.8638\n",
            "784/938 [========================>.....] - ETA: 1s - loss: 0.5260 - accuracy: 0.8642\n",
            "798/938 [========================>.....] - ETA: 1s - loss: 0.5249 - accuracy: 0.8644\n",
            "805/938 [========================>.....] - ETA: 0s - loss: 0.5260 - accuracy: 0.8642\n",
            "819/938 [=========================>....] - ETA: 0s - loss: 0.5248 - accuracy: 0.8644\n",
            "833/938 [=========================>....] - ETA: 0s - loss: 0.5265 - accuracy: 0.8641\n",
            "847/938 [==========================>...] - ETA: 0s - loss: 0.5291 - accuracy: 0.8637\n",
            "861/938 [==========================>...] - ETA: 0s - loss: 0.5285 - accuracy: 0.8640\n",
            "875/938 [==========================>...] - ETA: 0s - loss: 0.5279 - accuracy: 0.8641\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 0.5285 - accuracy: 0.8639\n",
            "903/938 [===========================>..] - ETA: 0s - loss: 0.5314 - accuracy: 0.8633\n",
            "918/938 [============================>.] - ETA: 0s - loss: 0.5336 - accuracy: 0.8631\n",
            "932/938 [============================>.] - ETA: 0s - loss: 0.5366 - accuracy: 0.8624\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.5363 - accuracy: 0.8623 - val_loss: 0.5743 - val_accuracy: 0.8779\n",
            "\u001b[36m(train_mnist pid=3296841)\u001b[0m Epoch 12/12\n",
            "  1/938 [..............................] - ETA: 8s - loss: 0.9481 - accuracy: 0.7656\n",
            " 15/938 [..............................] - ETA: 6s - loss: 0.6376 - accuracy: 0.8313\n",
            " 29/938 [..............................] - ETA: 6s - loss: 0.6210 - accuracy: 0.8384\n",
            " 43/938 [>.............................] - ETA: 6s - loss: 0.5974 - accuracy: 0.8438\n",
            " 57/938 [>.............................] - ETA: 6s - loss: 0.6166 - accuracy: 0.8462\n",
            " 71/938 [=>............................] - ETA: 6s - loss: 0.6174 - accuracy: 0.8411\n",
            " 85/938 [=>............................] - ETA: 6s - loss: 0.6012 - accuracy: 0.8443\n",
            " 99/938 [==>...........................] - ETA: 6s - loss: 0.5930 - accuracy: 0.8464\n",
            "113/938 [==>...........................] - ETA: 6s - loss: 0.5931 - accuracy: 0.8480\n",
            "127/938 [===>..........................] - ETA: 5s - loss: 0.5834 - accuracy: 0.8484\n",
            "141/938 [===>..........................] - ETA: 5s - loss: 0.5719 - accuracy: 0.8497\n",
            "155/938 [===>..........................] - ETA: 5s - loss: 0.5677 - accuracy: 0.8491\n",
            "169/938 [====>.........................] - ETA: 5s - loss: 0.5725 - accuracy: 0.8474\n",
            "183/938 [====>.........................] - ETA: 5s - loss: 0.5677 - accuracy: 0.8466\n",
            "197/938 [=====>........................] - ETA: 5s - loss: 0.5692 - accuracy: 0.8464\n",
            "211/938 [=====>........................] - ETA: 5s - loss: 0.5639 - accuracy: 0.8477\n",
            "218/938 [=====>........................] - ETA: 5s - loss: 0.5624 - accuracy: 0.8483\n",
            "232/938 [======>.......................] - ETA: 5s - loss: 0.5563 - accuracy: 0.8481\n",
            "246/938 [======>.......................] - ETA: 5s - loss: 0.5496 - accuracy: 0.8497\n",
            "260/938 [=======>......................] - ETA: 4s - loss: 0.5445 - accuracy: 0.8511\n",
            "274/938 [=======>......................] - ETA: 4s - loss: 0.5374 - accuracy: 0.8527\n",
            "288/938 [========>.....................] - ETA: 4s - loss: 0.5343 - accuracy: 0.8536\n",
            "302/938 [========>.....................] - ETA: 4s - loss: 0.5328 - accuracy: 0.8540\n",
            "316/938 [=========>....................] - ETA: 4s - loss: 0.5375 - accuracy: 0.8542\n",
            "330/938 [=========>....................] - ETA: 4s - loss: 0.5345 - accuracy: 0.8548\n",
            "344/938 [==========>...................] - ETA: 4s - loss: 0.5335 - accuracy: 0.8554\n",
            "358/938 [==========>...................] - ETA: 4s - loss: 0.5313 - accuracy: 0.8564\n",
            "372/938 [==========>...................] - ETA: 4s - loss: 0.5267 - accuracy: 0.8573\n",
            "386/938 [===========>..................] - ETA: 4s - loss: 0.5269 - accuracy: 0.8578\n",
            "400/938 [===========>..................] - ETA: 3s - loss: 0.5337 - accuracy: 0.8570\n",
            "414/938 [============>.................] - ETA: 3s - loss: 0.5362 - accuracy: 0.8567\n",
            "421/938 [============>.................] - ETA: 3s - loss: 0.5376 - accuracy: 0.8565\n",
            "435/938 [============>.................] - ETA: 3s - loss: 0.5393 - accuracy: 0.8567\n",
            "449/938 [=============>................] - ETA: 3s - loss: 0.5441 - accuracy: 0.8551\n",
            "463/938 [=============>................] - ETA: 3s - loss: 0.5423 - accuracy: 0.8555\n",
            "477/938 [==============>...............] - ETA: 3s - loss: 0.5422 - accuracy: 0.8557\n",
            "491/938 [==============>...............] - ETA: 3s - loss: 0.5410 - accuracy: 0.8558\n",
            "505/938 [===============>..............] - ETA: 3s - loss: 0.5391 - accuracy: 0.8562\n",
            "519/938 [===============>..............] - ETA: 3s - loss: 0.5390 - accuracy: 0.8564\n",
            "533/938 [================>.............] - ETA: 2s - loss: 0.5385 - accuracy: 0.8563\n",
            "547/938 [================>.............] - ETA: 2s - loss: 0.5383 - accuracy: 0.8559\n",
            "561/938 [================>.............] - ETA: 2s - loss: 0.5368 - accuracy: 0.8562\n",
            "575/938 [=================>............] - ETA: 2s - loss: 0.5336 - accuracy: 0.8568\n",
            "589/938 [=================>............] - ETA: 2s - loss: 0.5330 - accuracy: 0.8569\n",
            "603/938 [==================>...........] - ETA: 2s - loss: 0.5306 - accuracy: 0.8576\n",
            "617/938 [==================>...........] - ETA: 2s - loss: 0.5285 - accuracy: 0.8581\n",
            "631/938 [===================>..........] - ETA: 2s - loss: 0.5270 - accuracy: 0.8585\n",
            "645/938 [===================>..........] - ETA: 2s - loss: 0.5260 - accuracy: 0.8584\n",
            "652/938 [===================>..........] - ETA: 2s - loss: 0.5264 - accuracy: 0.8583\n",
            "666/938 [====================>.........] - ETA: 2s - loss: 0.5255 - accuracy: 0.8587\n",
            "680/938 [====================>.........] - ETA: 1s - loss: 0.5254 - accuracy: 0.8586\n",
            "694/938 [=====================>........] - ETA: 1s - loss: 0.5231 - accuracy: 0.8591\n",
            "708/938 [=====================>........] - ETA: 1s - loss: 0.5217 - accuracy: 0.8592\n",
            "722/938 [======================>.......] - ETA: 1s - loss: 0.5198 - accuracy: 0.8597\n",
            "736/938 [======================>.......] - ETA: 1s - loss: 0.5194 - accuracy: 0.8601\n",
            "750/938 [======================>.......] - ETA: 1s - loss: 0.5218 - accuracy: 0.8602\n",
            "764/938 [=======================>......] - ETA: 1s - loss: 0.5237 - accuracy: 0.8597\n",
            "778/938 [=======================>......] - ETA: 1s - loss: 0.5230 - accuracy: 0.8598\n",
            "792/938 [========================>.....] - ETA: 1s - loss: 0.5248 - accuracy: 0.8601\n",
            "806/938 [========================>.....] - ETA: 0s - loss: 0.5238 - accuracy: 0.8602\n",
            "820/938 [=========================>....] - ETA: 0s - loss: 0.5250 - accuracy: 0.8599\n",
            "834/938 [=========================>....] - ETA: 0s - loss: 0.5239 - accuracy: 0.8600\n",
            "848/938 [==========================>...] - ETA: 0s - loss: 0.5227 - accuracy: 0.8601\n",
            "862/938 [==========================>...] - ETA: 0s - loss: 0.5227 - accuracy: 0.8600\n",
            "876/938 [===========================>..] - ETA: 0s - loss: 0.5218 - accuracy: 0.8604\n",
            "883/938 [===========================>..] - ETA: 0s - loss: 0.5214 - accuracy: 0.8604\n",
            "897/938 [===========================>..] - ETA: 0s - loss: 0.5203 - accuracy: 0.8606\n",
            "911/938 [============================>.] - ETA: 0s - loss: 0.5202 - accuracy: 0.8607\n",
            "925/938 [============================>.] - ETA: 0s - loss: 0.5202 - accuracy: 0.8606\n",
            "932/938 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.8607\n",
            "938/938 [==============================] - 7s 8ms/step - loss: 0.5192 - accuracy: 0.8609 - val_loss: 0.4097 - val_accuracy: 0.9085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3297460)\u001b[0m 2023-12-05 01:44:13.943903: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3297460)\u001b[0m 2023-12-05 01:44:13.991935: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3297460)\u001b[0m 2023-12-05 01:44:13.992455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3297460)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3297460)\u001b[0m 2023-12-05 01:44:14.877033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3297460)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3297460)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3297460)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3297460)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3297460)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3297460)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3297460)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3297460)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m 2023-12-05 01:44:16.573294: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 1/12\n",
            "  3/469 [..............................] - ETA: 12s - loss: 194.5676 - accuracy: 0.1224\n",
            "  7/469 [..............................] - ETA: 12s - loss: 85.2098 - accuracy: 0.2009 \n",
            " 11/469 [..............................] - ETA: 12s - loss: 54.5836 - accuracy: 0.3764\n",
            " 15/469 [..............................] - ETA: 12s - loss: 40.2388 - accuracy: 0.4901\n",
            " 17/469 [>.............................] - ETA: 12s - loss: 35.5873 - accuracy: 0.5267\n",
            " 22/469 [>.............................] - ETA: 11s - loss: 27.6425 - accuracy: 0.5952\n",
            " 26/469 [>.............................] - ETA: 11s - loss: 23.4841 - accuracy: 0.6337\n",
            " 30/469 [>.............................] - ETA: 11s - loss: 20.4398 - accuracy: 0.6625\n",
            " 34/469 [=>............................] - ETA: 11s - loss: 18.0810 - accuracy: 0.6900\n",
            " 38/469 [=>............................] - ETA: 11s - loss: 16.2210 - accuracy: 0.7085\n",
            " 41/469 [=>............................] - ETA: 11s - loss: 15.0814 - accuracy: 0.7184\n",
            " 43/469 [=>............................] - ETA: 11s - loss: 14.4035 - accuracy: 0.7257\n",
            " 47/469 [==>...........................] - ETA: 11s - loss: 13.2090 - accuracy: 0.7414\n",
            " 49/469 [==>...........................] - ETA: 11s - loss: 12.6819 - accuracy: 0.7478\n",
            " 53/469 [==>...........................] - ETA: 11s - loss: 11.7498 - accuracy: 0.7600\n",
            " 57/469 [==>...........................] - ETA: 11s - loss: 10.9659 - accuracy: 0.7673\n",
            " 62/469 [==>...........................] - ETA: 10s - loss: 10.1125 - accuracy: 0.7775\n",
            " 66/469 [===>..........................] - ETA: 10s - loss: 9.5184 - accuracy: 0.7862\n",
            " 69/469 [===>..........................] - ETA: 10s - loss: 9.1142 - accuracy: 0.7925\n",
            " 74/469 [===>..........................] - ETA: 10s - loss: 8.5172 - accuracy: 0.8015\n",
            " 78/469 [===>..........................] - ETA: 10s - loss: 8.0975 - accuracy: 0.8066\n",
            " 82/469 [====>.........................] - ETA: 10s - loss: 7.7168 - accuracy: 0.8116\n",
            " 86/469 [====>.........................] - ETA: 10s - loss: 7.3731 - accuracy: 0.8164\n",
            " 89/469 [====>.........................] - ETA: 10s - loss: 7.1330 - accuracy: 0.8197\n",
            " 93/469 [====>.........................] - ETA: 10s - loss: 6.8400 - accuracy: 0.8242\n",
            " 97/469 [=====>........................] - ETA: 9s - loss: 6.5647 - accuracy: 0.8294 \n",
            "101/469 [=====>........................] - ETA: 9s - loss: 6.3139 - accuracy: 0.8341\n",
            "105/469 [=====>........................] - ETA: 9s - loss: 6.0820 - accuracy: 0.8385\n",
            "110/469 [======>.......................] - ETA: 9s - loss: 5.8188 - accuracy: 0.8426\n",
            "114/469 [======>.......................] - ETA: 9s - loss: 5.6222 - accuracy: 0.8462\n",
            "118/469 [======>.......................] - ETA: 9s - loss: 5.4416 - accuracy: 0.8483\n",
            "122/469 [======>.......................] - ETA: 9s - loss: 5.2738 - accuracy: 0.8496\n",
            "124/469 [======>.......................] - ETA: 9s - loss: 5.1942 - accuracy: 0.8507\n",
            "129/469 [=======>......................] - ETA: 9s - loss: 5.0030 - accuracy: 0.8537\n",
            "133/469 [=======>......................] - ETA: 8s - loss: 4.8619 - accuracy: 0.8556\n",
            "138/469 [=======>......................] - ETA: 8s - loss: 4.6971 - accuracy: 0.8577\n",
            "142/469 [========>.....................] - ETA: 8s - loss: 4.5713 - accuracy: 0.8598\n",
            "144/469 [========>.....................] - ETA: 8s - loss: 4.5111 - accuracy: 0.8607\n",
            "147/469 [========>.....................] - ETA: 8s - loss: 4.4254 - accuracy: 0.8620\n",
            "149/469 [========>.....................] - ETA: 8s - loss: 4.3704 - accuracy: 0.8625\n",
            "154/469 [========>.....................] - ETA: 8s - loss: 4.2368 - accuracy: 0.8649\n",
            "158/469 [=========>....................] - ETA: 8s - loss: 4.1345 - accuracy: 0.8670\n",
            "162/469 [=========>....................] - ETA: 8s - loss: 4.0384 - accuracy: 0.8687\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 3.9237 - accuracy: 0.8710\n",
            "171/469 [=========>....................] - ETA: 7s - loss: 3.8380 - accuracy: 0.8723\n",
            "175/469 [==========>...................] - ETA: 7s - loss: 3.7546 - accuracy: 0.8740\n",
            "179/469 [==========>...................] - ETA: 7s - loss: 3.6763 - accuracy: 0.8751\n",
            "183/469 [==========>...................] - ETA: 7s - loss: 3.6018 - accuracy: 0.8767\n",
            "187/469 [==========>...................] - ETA: 7s - loss: 3.5297 - accuracy: 0.8780\n",
            "191/469 [===========>..................] - ETA: 7s - loss: 3.4606 - accuracy: 0.8793\n",
            "193/469 [===========>..................] - ETA: 7s - loss: 3.4277 - accuracy: 0.8797\n",
            "195/469 [===========>..................] - ETA: 7s - loss: 3.3952 - accuracy: 0.8802\n",
            "197/469 [===========>..................] - ETA: 7s - loss: 3.3617 - accuracy: 0.8810\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 3.2991 - accuracy: 0.8822\n",
            "205/469 [============>.................] - ETA: 7s - loss: 3.2383 - accuracy: 0.8833\n",
            "209/469 [============>.................] - ETA: 6s - loss: 3.1809 - accuracy: 0.8842\n",
            "213/469 [============>.................] - ETA: 6s - loss: 3.1248 - accuracy: 0.8855\n",
            "217/469 [============>.................] - ETA: 6s - loss: 3.0727 - accuracy: 0.8863\n",
            "221/469 [=============>................] - ETA: 6s - loss: 3.0219 - accuracy: 0.8869\n",
            "226/469 [=============>................] - ETA: 6s - loss: 2.9599 - accuracy: 0.8879\n",
            "231/469 [=============>................] - ETA: 6s - loss: 2.9031 - accuracy: 0.8886\n",
            "233/469 [=============>................] - ETA: 6s - loss: 2.8801 - accuracy: 0.8890\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 2.8574 - accuracy: 0.8892\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 2.8123 - accuracy: 0.8903\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 2.7913 - accuracy: 0.8906\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 2.7701 - accuracy: 0.8909\n",
            "245/469 [==============>...............] - ETA: 5s - loss: 2.7494 - accuracy: 0.8912\n",
            "249/469 [==============>...............] - ETA: 5s - loss: 2.7079 - accuracy: 0.8920\n",
            "251/469 [===============>..............] - ETA: 5s - loss: 2.6884 - accuracy: 0.8925\n",
            "253/469 [===============>..............] - ETA: 5s - loss: 2.6681 - accuracy: 0.8931\n",
            "255/469 [===============>..............] - ETA: 5s - loss: 2.6490 - accuracy: 0.8935\n",
            "257/469 [===============>..............] - ETA: 5s - loss: 2.6294 - accuracy: 0.8940\n",
            "259/469 [===============>..............] - ETA: 5s - loss: 2.6110 - accuracy: 0.8944\n",
            "263/469 [===============>..............] - ETA: 5s - loss: 2.5741 - accuracy: 0.8951\n",
            "267/469 [================>.............] - ETA: 5s - loss: 2.5387 - accuracy: 0.8957\n",
            "270/469 [================>.............] - ETA: 5s - loss: 2.5133 - accuracy: 0.8959\n",
            "274/469 [================>.............] - ETA: 5s - loss: 2.4798 - accuracy: 0.8964\n",
            "278/469 [================>.............] - ETA: 5s - loss: 2.4465 - accuracy: 0.8972\n",
            "282/469 [=================>............] - ETA: 5s - loss: 2.4155 - accuracy: 0.8978\n",
            "286/469 [=================>............] - ETA: 4s - loss: 2.3838 - accuracy: 0.8987\n",
            "290/469 [=================>............] - ETA: 4s - loss: 2.3541 - accuracy: 0.8991\n",
            "294/469 [=================>............] - ETA: 4s - loss: 2.3250 - accuracy: 0.8997\n",
            "298/469 [==================>...........] - ETA: 4s - loss: 2.2971 - accuracy: 0.9001\n",
            "302/469 [==================>...........] - ETA: 4s - loss: 2.2703 - accuracy: 0.9004\n",
            "306/469 [==================>...........] - ETA: 4s - loss: 2.2429 - accuracy: 0.9010\n",
            "310/469 [==================>...........] - ETA: 4s - loss: 2.2158 - accuracy: 0.9019\n",
            "314/469 [===================>..........] - ETA: 4s - loss: 2.1886 - accuracy: 0.9028\n",
            "318/469 [===================>..........] - ETA: 4s - loss: 2.1632 - accuracy: 0.9035\n",
            "322/469 [===================>..........] - ETA: 3s - loss: 2.1393 - accuracy: 0.9038\n",
            "324/469 [===================>..........] - ETA: 3s - loss: 2.1276 - accuracy: 0.9040\n",
            "326/469 [===================>..........] - ETA: 3s - loss: 2.1157 - accuracy: 0.9042\n",
            "330/469 [====================>.........] - ETA: 3s - loss: 2.0933 - accuracy: 0.9048\n",
            "332/469 [====================>.........] - ETA: 3s - loss: 2.0825 - accuracy: 0.9049\n",
            "336/469 [====================>.........] - ETA: 3s - loss: 2.0603 - accuracy: 0.9053\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 2.0330 - accuracy: 0.9057\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 2.0124 - accuracy: 0.9063\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 1.9917 - accuracy: 0.9068\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 1.9725 - accuracy: 0.9072\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 1.9627 - accuracy: 0.9073\n",
            "360/469 [======================>.......] - ETA: 2s - loss: 1.9380 - accuracy: 0.9078\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 1.9194 - accuracy: 0.9080\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 1.9096 - accuracy: 0.9083\n",
            "368/469 [======================>.......] - ETA: 2s - loss: 1.9009 - accuracy: 0.9085\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 1.8785 - accuracy: 0.9090\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 1.8606 - accuracy: 0.9094\n",
            "382/469 [=======================>......] - ETA: 2s - loss: 1.8386 - accuracy: 0.9100\n",
            "386/469 [=======================>......] - ETA: 2s - loss: 1.8216 - accuracy: 0.9105\n",
            "390/469 [=======================>......] - ETA: 2s - loss: 1.8049 - accuracy: 0.9108\n",
            "394/469 [========================>.....] - ETA: 2s - loss: 1.7898 - accuracy: 0.9109\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 1.7734 - accuracy: 0.9114\n",
            "402/469 [========================>.....] - ETA: 1s - loss: 1.7575 - accuracy: 0.9117\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 1.7416 - accuracy: 0.9121\n",
            "410/469 [=========================>....] - ETA: 1s - loss: 1.7268 - accuracy: 0.9125\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 1.7165 - accuracy: 0.9126\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 1.6950 - accuracy: 0.9132\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 1.6882 - accuracy: 0.9132\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 1.6814 - accuracy: 0.9134\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 1.6742 - accuracy: 0.9136\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 1.6672 - accuracy: 0.9138\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 1.6603 - accuracy: 0.9140\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 1.6466 - accuracy: 0.9144\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 1.6397 - accuracy: 0.9145\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 1.6270 - accuracy: 0.9148\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 1.6138 - accuracy: 0.9151\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 1.6074 - accuracy: 0.9151\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 1.5886 - accuracy: 0.9156\n",
            "455/469 [============================>.] - ETA: 0s - loss: 1.5766 - accuracy: 0.9158\n",
            "459/469 [============================>.] - ETA: 0s - loss: 1.5647 - accuracy: 0.9161\n",
            "463/469 [============================>.] - ETA: 0s - loss: 1.5530 - accuracy: 0.9163\n",
            "467/469 [============================>.] - ETA: 0s - loss: 1.5411 - accuracy: 0.9166\n",
            "469/469 [==============================] - ETA: 0s - loss: 1.5359 - accuracy: 0.9168\n",
            "469/469 [==============================] - 14s 29ms/step - loss: 1.5359 - accuracy: 0.9168 - val_loss: 0.1626 - val_accuracy: 0.9554\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 2/12\n",
            "  1/469 [..............................] - ETA: 12s - loss: 0.1143 - accuracy: 0.9453\n",
            "  5/469 [..............................] - ETA: 12s - loss: 0.1528 - accuracy: 0.9453\n",
            " 10/469 [..............................] - ETA: 12s - loss: 0.1507 - accuracy: 0.9523\n",
            " 14/469 [..............................] - ETA: 11s - loss: 0.1517 - accuracy: 0.9554\n",
            " 18/469 [>.............................] - ETA: 11s - loss: 0.1657 - accuracy: 0.9544\n",
            " 22/469 [>.............................] - ETA: 11s - loss: 0.1653 - accuracy: 0.9538\n",
            " 24/469 [>.............................] - ETA: 11s - loss: 0.1654 - accuracy: 0.9538\n",
            " 27/469 [>.............................] - ETA: 11s - loss: 0.1746 - accuracy: 0.9537\n",
            " 29/469 [>.............................] - ETA: 11s - loss: 0.1770 - accuracy: 0.9531\n",
            " 31/469 [>.............................] - ETA: 11s - loss: 0.1817 - accuracy: 0.9526\n",
            " 33/469 [=>............................] - ETA: 11s - loss: 0.1792 - accuracy: 0.9529\n",
            " 35/469 [=>............................] - ETA: 11s - loss: 0.1782 - accuracy: 0.9529\n",
            " 39/469 [=>............................] - ETA: 11s - loss: 0.1772 - accuracy: 0.9533\n",
            " 42/469 [=>............................] - ETA: 11s - loss: 0.1734 - accuracy: 0.9535\n",
            " 46/469 [=>............................] - ETA: 11s - loss: 0.1742 - accuracy: 0.9524\n",
            " 50/469 [==>...........................] - ETA: 11s - loss: 0.1737 - accuracy: 0.9523\n",
            " 55/469 [==>...........................] - ETA: 11s - loss: 0.1727 - accuracy: 0.9518\n",
            " 59/469 [==>...........................] - ETA: 10s - loss: 0.1755 - accuracy: 0.9515\n",
            " 63/469 [===>..........................] - ETA: 10s - loss: 0.1736 - accuracy: 0.9518\n",
            " 67/469 [===>..........................] - ETA: 10s - loss: 0.1710 - accuracy: 0.9524\n",
            " 70/469 [===>..........................] - ETA: 10s - loss: 0.1683 - accuracy: 0.9528\n",
            " 75/469 [===>..........................] - ETA: 10s - loss: 0.1678 - accuracy: 0.9522\n",
            " 79/469 [====>.........................] - ETA: 10s - loss: 0.1683 - accuracy: 0.9520\n",
            " 83/469 [====>.........................] - ETA: 10s - loss: 0.1694 - accuracy: 0.9526\n",
            " 87/469 [====>.........................] - ETA: 10s - loss: 0.1686 - accuracy: 0.9528\n",
            " 90/469 [====>.........................] - ETA: 10s - loss: 0.1689 - accuracy: 0.9530\n",
            " 94/469 [=====>........................] - ETA: 9s - loss: 0.1668 - accuracy: 0.9535\n",
            " 98/469 [=====>........................] - ETA: 9s - loss: 0.1699 - accuracy: 0.9529\n",
            "103/469 [=====>........................] - ETA: 9s - loss: 0.1694 - accuracy: 0.9526\n",
            "105/469 [=====>........................] - ETA: 9s - loss: 0.1689 - accuracy: 0.9526\n",
            "111/469 [======>.......................] - ETA: 9s - loss: 0.1670 - accuracy: 0.9532\n",
            "114/469 [======>.......................] - ETA: 9s - loss: 0.1694 - accuracy: 0.9524\n",
            "118/469 [======>.......................] - ETA: 9s - loss: 0.1688 - accuracy: 0.9526\n",
            "122/469 [======>.......................] - ETA: 9s - loss: 0.1681 - accuracy: 0.9525\n",
            "126/469 [=======>......................] - ETA: 9s - loss: 0.1655 - accuracy: 0.9532\n",
            "130/469 [=======>......................] - ETA: 8s - loss: 0.1639 - accuracy: 0.9539\n",
            "134/469 [=======>......................] - ETA: 8s - loss: 0.1666 - accuracy: 0.9539\n",
            "139/469 [=======>......................] - ETA: 8s - loss: 0.1678 - accuracy: 0.9539\n",
            "143/469 [========>.....................] - ETA: 8s - loss: 0.1723 - accuracy: 0.9532\n",
            "148/469 [========>.....................] - ETA: 8s - loss: 0.1755 - accuracy: 0.9522\n",
            "152/469 [========>.....................] - ETA: 8s - loss: 0.1781 - accuracy: 0.9517\n",
            "155/469 [========>.....................] - ETA: 8s - loss: 0.1789 - accuracy: 0.9514\n",
            "160/469 [=========>....................] - ETA: 8s - loss: 0.1805 - accuracy: 0.9511\n",
            "164/469 [=========>....................] - ETA: 8s - loss: 0.1824 - accuracy: 0.9505\n",
            "168/469 [=========>....................] - ETA: 7s - loss: 0.1850 - accuracy: 0.9499\n",
            "170/469 [=========>....................] - ETA: 7s - loss: 0.1865 - accuracy: 0.9494\n",
            "175/469 [==========>...................] - ETA: 7s - loss: 0.1852 - accuracy: 0.9497\n",
            "179/469 [==========>...................] - ETA: 7s - loss: 0.1881 - accuracy: 0.9490\n",
            "183/469 [==========>...................] - ETA: 7s - loss: 0.1891 - accuracy: 0.9486\n",
            "188/469 [===========>..................] - ETA: 7s - loss: 0.1930 - accuracy: 0.9478\n",
            "192/469 [===========>..................] - ETA: 7s - loss: 0.1936 - accuracy: 0.9480\n",
            "196/469 [===========>..................] - ETA: 7s - loss: 0.1927 - accuracy: 0.9481\n",
            "200/469 [===========>..................] - ETA: 7s - loss: 0.1919 - accuracy: 0.9484\n",
            "204/469 [============>.................] - ETA: 7s - loss: 0.1909 - accuracy: 0.9486\n",
            "208/469 [============>.................] - ETA: 6s - loss: 0.1908 - accuracy: 0.9485\n",
            "212/469 [============>.................] - ETA: 6s - loss: 0.1900 - accuracy: 0.9486\n",
            "216/469 [============>.................] - ETA: 6s - loss: 0.1893 - accuracy: 0.9488\n",
            "221/469 [=============>................] - ETA: 6s - loss: 0.1879 - accuracy: 0.9491\n",
            "225/469 [=============>................] - ETA: 6s - loss: 0.1896 - accuracy: 0.9489\n",
            "229/469 [=============>................] - ETA: 6s - loss: 0.1889 - accuracy: 0.9490\n",
            "233/469 [=============>................] - ETA: 6s - loss: 0.1885 - accuracy: 0.9492\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1887 - accuracy: 0.9490\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1886 - accuracy: 0.9491\n",
            "245/469 [==============>...............] - ETA: 5s - loss: 0.1892 - accuracy: 0.9493\n",
            "249/469 [==============>...............] - ETA: 5s - loss: 0.1928 - accuracy: 0.9489\n",
            "253/469 [===============>..............] - ETA: 5s - loss: 0.1926 - accuracy: 0.9487\n",
            "257/469 [===============>..............] - ETA: 5s - loss: 0.1942 - accuracy: 0.9484\n",
            "261/469 [===============>..............] - ETA: 5s - loss: 0.1953 - accuracy: 0.9482\n",
            "265/469 [===============>..............] - ETA: 5s - loss: 0.1954 - accuracy: 0.9483\n",
            "267/469 [================>.............] - ETA: 5s - loss: 0.1948 - accuracy: 0.9485\n",
            "272/469 [================>.............] - ETA: 5s - loss: 0.1962 - accuracy: 0.9481\n",
            "276/469 [================>.............] - ETA: 5s - loss: 0.1965 - accuracy: 0.9480\n",
            "280/469 [================>.............] - ETA: 5s - loss: 0.1972 - accuracy: 0.9479\n",
            "285/469 [=================>............] - ETA: 4s - loss: 0.1972 - accuracy: 0.9480\n",
            "288/469 [=================>............] - ETA: 4s - loss: 0.1969 - accuracy: 0.9481\n",
            "290/469 [=================>............] - ETA: 4s - loss: 0.1972 - accuracy: 0.9481\n",
            "292/469 [=================>............] - ETA: 4s - loss: 0.1980 - accuracy: 0.9479\n",
            "294/469 [=================>............] - ETA: 4s - loss: 0.1988 - accuracy: 0.9477\n",
            "296/469 [=================>............] - ETA: 4s - loss: 0.1991 - accuracy: 0.9475\n",
            "298/469 [==================>...........] - ETA: 4s - loss: 0.1983 - accuracy: 0.9477\n",
            "302/469 [==================>...........] - ETA: 4s - loss: 0.1992 - accuracy: 0.9476\n",
            "304/469 [==================>...........] - ETA: 4s - loss: 0.1997 - accuracy: 0.9475\n",
            "306/469 [==================>...........] - ETA: 4s - loss: 0.1992 - accuracy: 0.9475\n",
            "308/469 [==================>...........] - ETA: 4s - loss: 0.1997 - accuracy: 0.9476\n",
            "310/469 [==================>...........] - ETA: 4s - loss: 0.1992 - accuracy: 0.9475\n",
            "312/469 [==================>...........] - ETA: 4s - loss: 0.1994 - accuracy: 0.9475\n",
            "314/469 [===================>..........] - ETA: 4s - loss: 0.1996 - accuracy: 0.9474\n",
            "318/469 [===================>..........] - ETA: 3s - loss: 0.1999 - accuracy: 0.9474\n",
            "322/469 [===================>..........] - ETA: 3s - loss: 0.2010 - accuracy: 0.9472\n",
            "324/469 [===================>..........] - ETA: 3s - loss: 0.2010 - accuracy: 0.9472\n",
            "326/469 [===================>..........] - ETA: 3s - loss: 0.2009 - accuracy: 0.9473\n",
            "328/469 [===================>..........] - ETA: 3s - loss: 0.2013 - accuracy: 0.9472\n",
            "330/469 [====================>.........] - ETA: 3s - loss: 0.2016 - accuracy: 0.9471\n",
            "332/469 [====================>.........] - ETA: 3s - loss: 0.2014 - accuracy: 0.9471\n",
            "334/469 [====================>.........] - ETA: 3s - loss: 0.2027 - accuracy: 0.9469\n",
            "336/469 [====================>.........] - ETA: 3s - loss: 0.2023 - accuracy: 0.9470\n",
            "340/469 [====================>.........] - ETA: 3s - loss: 0.2024 - accuracy: 0.9469\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.2021 - accuracy: 0.9471\n",
            "350/469 [=====================>........] - ETA: 3s - loss: 0.2025 - accuracy: 0.9469\n",
            "352/469 [=====================>........] - ETA: 3s - loss: 0.2022 - accuracy: 0.9470\n",
            "357/469 [=====================>........] - ETA: 2s - loss: 0.2025 - accuracy: 0.9469\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2025 - accuracy: 0.9468\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 0.2018 - accuracy: 0.9468\n",
            "370/469 [======================>.......] - ETA: 2s - loss: 0.2019 - accuracy: 0.9466\n",
            "374/469 [======================>.......] - ETA: 2s - loss: 0.2015 - accuracy: 0.9467\n",
            "376/469 [=======================>......] - ETA: 2s - loss: 0.2019 - accuracy: 0.9467\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 0.2016 - accuracy: 0.9466\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 0.2021 - accuracy: 0.9464\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.2018 - accuracy: 0.9465\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.2014 - accuracy: 0.9466\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2009 - accuracy: 0.9468\n",
            "401/469 [========================>.....] - ETA: 1s - loss: 0.2006 - accuracy: 0.9469\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.2008 - accuracy: 0.9469\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2014 - accuracy: 0.9468\n",
            "414/469 [=========================>....] - ETA: 1s - loss: 0.2019 - accuracy: 0.9467\n",
            "418/469 [=========================>....] - ETA: 1s - loss: 0.2022 - accuracy: 0.9466\n",
            "420/469 [=========================>....] - ETA: 1s - loss: 0.2029 - accuracy: 0.9464\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.2032 - accuracy: 0.9462\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.2030 - accuracy: 0.9463\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.2032 - accuracy: 0.9462\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.2033 - accuracy: 0.9463\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 0.2031 - accuracy: 0.9464\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.2034 - accuracy: 0.9462\n",
            "438/469 [===========================>..] - ETA: 0s - loss: 0.2037 - accuracy: 0.9461\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2036 - accuracy: 0.9460\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2035 - accuracy: 0.9460\n",
            "450/469 [===========================>..] - ETA: 0s - loss: 0.2037 - accuracy: 0.9460\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9461\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9462\n",
            "462/469 [============================>.] - ETA: 0s - loss: 0.2037 - accuracy: 0.9462\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.2040 - accuracy: 0.9462\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.2034 - accuracy: 0.9463\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.2033 - accuracy: 0.9463 - val_loss: 0.1652 - val_accuracy: 0.9574\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 3/12\n",
            "  3/469 [..............................] - ETA: 12s - loss: 0.1738 - accuracy: 0.9557\n",
            "  7/469 [..............................] - ETA: 12s - loss: 0.1700 - accuracy: 0.9587\n",
            " 11/469 [..............................] - ETA: 12s - loss: 0.1802 - accuracy: 0.9581\n",
            " 15/469 [..............................] - ETA: 12s - loss: 0.1910 - accuracy: 0.9516\n",
            " 19/469 [>.............................] - ETA: 11s - loss: 0.1986 - accuracy: 0.9490\n",
            " 23/469 [>.............................] - ETA: 11s - loss: 0.2023 - accuracy: 0.9490\n",
            " 27/469 [>.............................] - ETA: 11s - loss: 0.1891 - accuracy: 0.9517\n",
            " 31/469 [>.............................] - ETA: 11s - loss: 0.1900 - accuracy: 0.9526\n",
            " 35/469 [=>............................] - ETA: 11s - loss: 0.1925 - accuracy: 0.9525\n",
            " 39/469 [=>............................] - ETA: 11s - loss: 0.1937 - accuracy: 0.9531\n",
            " 43/469 [=>............................] - ETA: 11s - loss: 0.1993 - accuracy: 0.9517\n",
            " 47/469 [==>...........................] - ETA: 11s - loss: 0.2033 - accuracy: 0.9508\n",
            " 51/469 [==>...........................] - ETA: 11s - loss: 0.2063 - accuracy: 0.9488\n",
            " 55/469 [==>...........................] - ETA: 11s - loss: 0.2038 - accuracy: 0.9490\n",
            " 59/469 [==>...........................] - ETA: 10s - loss: 0.2022 - accuracy: 0.9497\n",
            " 63/469 [===>..........................] - ETA: 10s - loss: 0.2017 - accuracy: 0.9498\n",
            " 67/469 [===>..........................] - ETA: 10s - loss: 0.1999 - accuracy: 0.9502\n",
            " 69/469 [===>..........................] - ETA: 10s - loss: 0.2004 - accuracy: 0.9503\n",
            " 74/469 [===>..........................] - ETA: 10s - loss: 0.2023 - accuracy: 0.9493\n",
            " 78/469 [===>..........................] - ETA: 10s - loss: 0.1995 - accuracy: 0.9498\n",
            " 82/469 [====>.........................] - ETA: 10s - loss: 0.1974 - accuracy: 0.9504\n",
            " 86/469 [====>.........................] - ETA: 10s - loss: 0.1945 - accuracy: 0.9509\n",
            " 90/469 [====>.........................] - ETA: 10s - loss: 0.1922 - accuracy: 0.9516\n",
            " 94/469 [=====>........................] - ETA: 10s - loss: 0.1919 - accuracy: 0.9510\n",
            " 98/469 [=====>........................] - ETA: 9s - loss: 0.1946 - accuracy: 0.9503\n",
            "102/469 [=====>........................] - ETA: 9s - loss: 0.1943 - accuracy: 0.9506\n",
            "106/469 [=====>........................] - ETA: 9s - loss: 0.1958 - accuracy: 0.9500\n",
            "111/469 [======>.......................] - ETA: 9s - loss: 0.1941 - accuracy: 0.9502\n",
            "115/469 [======>.......................] - ETA: 9s - loss: 0.1929 - accuracy: 0.9501\n",
            "117/469 [======>.......................] - ETA: 9s - loss: 0.1938 - accuracy: 0.9500\n",
            "120/469 [======>.......................] - ETA: 9s - loss: 0.1932 - accuracy: 0.9499\n",
            "122/469 [======>.......................] - ETA: 9s - loss: 0.1926 - accuracy: 0.9500\n",
            "124/469 [======>.......................] - ETA: 9s - loss: 0.1908 - accuracy: 0.9504\n",
            "126/469 [=======>......................] - ETA: 9s - loss: 0.1910 - accuracy: 0.9504\n",
            "128/469 [=======>......................] - ETA: 9s - loss: 0.1901 - accuracy: 0.9505\n",
            "132/469 [=======>......................] - ETA: 8s - loss: 0.1899 - accuracy: 0.9501\n",
            "134/469 [=======>......................] - ETA: 8s - loss: 0.1900 - accuracy: 0.9502\n",
            "136/469 [=======>......................] - ETA: 8s - loss: 0.1888 - accuracy: 0.9505\n",
            "138/469 [=======>......................] - ETA: 8s - loss: 0.1896 - accuracy: 0.9504\n",
            "140/469 [=======>......................] - ETA: 8s - loss: 0.1901 - accuracy: 0.9502\n",
            "142/469 [========>.....................] - ETA: 8s - loss: 0.1881 - accuracy: 0.9506\n",
            "147/469 [========>.....................] - ETA: 8s - loss: 0.1880 - accuracy: 0.9509\n",
            "152/469 [========>.....................] - ETA: 8s - loss: 0.1868 - accuracy: 0.9513\n",
            "156/469 [========>.....................] - ETA: 8s - loss: 0.1851 - accuracy: 0.9518\n",
            "160/469 [=========>....................] - ETA: 8s - loss: 0.1852 - accuracy: 0.9519\n",
            "164/469 [=========>....................] - ETA: 8s - loss: 0.1843 - accuracy: 0.9522\n",
            "168/469 [=========>....................] - ETA: 8s - loss: 0.1829 - accuracy: 0.9521\n",
            "172/469 [==========>...................] - ETA: 7s - loss: 0.1819 - accuracy: 0.9520\n",
            "176/469 [==========>...................] - ETA: 7s - loss: 0.1801 - accuracy: 0.9524\n",
            "180/469 [==========>...................] - ETA: 7s - loss: 0.1794 - accuracy: 0.9526\n",
            "184/469 [==========>...................] - ETA: 7s - loss: 0.1801 - accuracy: 0.9527\n",
            "188/469 [===========>..................] - ETA: 7s - loss: 0.1805 - accuracy: 0.9528\n",
            "192/469 [===========>..................] - ETA: 7s - loss: 0.1803 - accuracy: 0.9529\n",
            "196/469 [===========>..................] - ETA: 7s - loss: 0.1811 - accuracy: 0.9524\n",
            "200/469 [===========>..................] - ETA: 7s - loss: 0.1805 - accuracy: 0.9522\n",
            "204/469 [============>.................] - ETA: 7s - loss: 0.1807 - accuracy: 0.9521\n",
            "208/469 [============>.................] - ETA: 6s - loss: 0.1797 - accuracy: 0.9523\n",
            "212/469 [============>.................] - ETA: 6s - loss: 0.1790 - accuracy: 0.9526\n",
            "216/469 [============>.................] - ETA: 6s - loss: 0.1793 - accuracy: 0.9525\n",
            "220/469 [=============>................] - ETA: 6s - loss: 0.1793 - accuracy: 0.9522\n",
            "224/469 [=============>................] - ETA: 6s - loss: 0.1794 - accuracy: 0.9521\n",
            "226/469 [=============>................] - ETA: 6s - loss: 0.1789 - accuracy: 0.9521\n",
            "232/469 [=============>................] - ETA: 6s - loss: 0.1803 - accuracy: 0.9521\n",
            "236/469 [==============>...............] - ETA: 6s - loss: 0.1802 - accuracy: 0.9522\n",
            "240/469 [==============>...............] - ETA: 6s - loss: 0.1796 - accuracy: 0.9524\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 0.1822 - accuracy: 0.9524\n",
            "245/469 [==============>...............] - ETA: 5s - loss: 0.1819 - accuracy: 0.9524\n",
            "249/469 [==============>...............] - ETA: 5s - loss: 0.1815 - accuracy: 0.9526\n",
            "253/469 [===============>..............] - ETA: 5s - loss: 0.1816 - accuracy: 0.9524\n",
            "257/469 [===============>..............] - ETA: 5s - loss: 0.1816 - accuracy: 0.9525\n",
            "259/469 [===============>..............] - ETA: 5s - loss: 0.1820 - accuracy: 0.9525\n",
            "261/469 [===============>..............] - ETA: 5s - loss: 0.1824 - accuracy: 0.9524\n",
            "263/469 [===============>..............] - ETA: 5s - loss: 0.1837 - accuracy: 0.9523\n",
            "265/469 [===============>..............] - ETA: 5s - loss: 0.1834 - accuracy: 0.9524\n",
            "269/469 [================>.............] - ETA: 5s - loss: 0.1843 - accuracy: 0.9523\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1843 - accuracy: 0.9523\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1845 - accuracy: 0.9522\n",
            "275/469 [================>.............] - ETA: 5s - loss: 0.1841 - accuracy: 0.9523\n",
            "279/469 [================>.............] - ETA: 5s - loss: 0.1853 - accuracy: 0.9520\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1857 - accuracy: 0.9518\n",
            "285/469 [=================>............] - ETA: 4s - loss: 0.1866 - accuracy: 0.9515\n",
            "288/469 [=================>............] - ETA: 4s - loss: 0.1869 - accuracy: 0.9516\n",
            "292/469 [=================>............] - ETA: 4s - loss: 0.1864 - accuracy: 0.9515\n",
            "296/469 [=================>............] - ETA: 4s - loss: 0.1858 - accuracy: 0.9514\n",
            "300/469 [==================>...........] - ETA: 4s - loss: 0.1858 - accuracy: 0.9514\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 0.1864 - accuracy: 0.9514\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.1876 - accuracy: 0.9514\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1878 - accuracy: 0.9514\n",
            "314/469 [===================>..........] - ETA: 4s - loss: 0.1874 - accuracy: 0.9517\n",
            "318/469 [===================>..........] - ETA: 4s - loss: 0.1883 - accuracy: 0.9515\n",
            "320/469 [===================>..........] - ETA: 3s - loss: 0.1878 - accuracy: 0.9516\n",
            "325/469 [===================>..........] - ETA: 3s - loss: 0.1877 - accuracy: 0.9516\n",
            "329/469 [====================>.........] - ETA: 3s - loss: 0.1920 - accuracy: 0.9511\n",
            "333/469 [====================>.........] - ETA: 3s - loss: 0.1963 - accuracy: 0.9508\n",
            "338/469 [====================>.........] - ETA: 3s - loss: 0.1988 - accuracy: 0.9506\n",
            "342/469 [====================>.........] - ETA: 3s - loss: 0.1997 - accuracy: 0.9506\n",
            "346/469 [=====================>........] - ETA: 3s - loss: 0.2009 - accuracy: 0.9503\n",
            "350/469 [=====================>........] - ETA: 3s - loss: 0.2021 - accuracy: 0.9500\n",
            "354/469 [=====================>........] - ETA: 3s - loss: 0.2022 - accuracy: 0.9498\n",
            "359/469 [=====================>........] - ETA: 2s - loss: 0.2038 - accuracy: 0.9498\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2041 - accuracy: 0.9497\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 0.2062 - accuracy: 0.9494\n",
            "370/469 [======================>.......] - ETA: 2s - loss: 0.2073 - accuracy: 0.9493\n",
            "374/469 [======================>.......] - ETA: 2s - loss: 0.2078 - accuracy: 0.9490\n",
            "378/469 [=======================>......] - ETA: 2s - loss: 0.2088 - accuracy: 0.9487\n",
            "382/469 [=======================>......] - ETA: 2s - loss: 0.2097 - accuracy: 0.9487\n",
            "386/469 [=======================>......] - ETA: 2s - loss: 0.2104 - accuracy: 0.9484\n",
            "390/469 [=======================>......] - ETA: 2s - loss: 0.2120 - accuracy: 0.9480\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.2126 - accuracy: 0.9479\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 0.2129 - accuracy: 0.9478\n",
            "402/469 [========================>.....] - ETA: 1s - loss: 0.2126 - accuracy: 0.9479\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2126 - accuracy: 0.9479\n",
            "410/469 [=========================>....] - ETA: 1s - loss: 0.2132 - accuracy: 0.9477\n",
            "414/469 [=========================>....] - ETA: 1s - loss: 0.2127 - accuracy: 0.9479\n",
            "418/469 [=========================>....] - ETA: 1s - loss: 0.2124 - accuracy: 0.9479\n",
            "422/469 [=========================>....] - ETA: 1s - loss: 0.2132 - accuracy: 0.9479\n",
            "426/469 [==========================>...] - ETA: 1s - loss: 0.2144 - accuracy: 0.9477\n",
            "430/469 [==========================>...] - ETA: 1s - loss: 0.2145 - accuracy: 0.9476\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.2149 - accuracy: 0.9474\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.2159 - accuracy: 0.9473\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.2169 - accuracy: 0.9471\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.2185 - accuracy: 0.9469\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.2189 - accuracy: 0.9469\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9468\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.2191 - accuracy: 0.9467\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9466\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.2195 - accuracy: 0.9466\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.2198 - accuracy: 0.9465 - val_loss: 0.2765 - val_accuracy: 0.9411\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 4/12\n",
            "  3/469 [..............................] - ETA: 12s - loss: 0.1952 - accuracy: 0.9583\n",
            "  7/469 [..............................] - ETA: 12s - loss: 0.1937 - accuracy: 0.9587\n",
            " 11/469 [..............................] - ETA: 12s - loss: 0.2107 - accuracy: 0.9574\n",
            " 15/469 [..............................] - ETA: 12s - loss: 0.2077 - accuracy: 0.9526\n",
            " 17/469 [>.............................] - ETA: 12s - loss: 0.2223 - accuracy: 0.9499\n",
            " 23/469 [>.............................] - ETA: 11s - loss: 0.2154 - accuracy: 0.9480\n",
            " 27/469 [>.............................] - ETA: 11s - loss: 0.2279 - accuracy: 0.9462\n",
            " 31/469 [>.............................] - ETA: 11s - loss: 0.2242 - accuracy: 0.9463\n",
            " 35/469 [=>............................] - ETA: 11s - loss: 0.2279 - accuracy: 0.9449\n",
            " 37/469 [=>............................] - ETA: 11s - loss: 0.2251 - accuracy: 0.9455\n",
            " 40/469 [=>............................] - ETA: 11s - loss: 0.2266 - accuracy: 0.9449\n",
            " 43/469 [=>............................] - ETA: 11s - loss: 0.2335 - accuracy: 0.9446\n",
            " 47/469 [==>...........................] - ETA: 11s - loss: 0.2307 - accuracy: 0.9455\n",
            " 51/469 [==>...........................] - ETA: 11s - loss: 0.2308 - accuracy: 0.9453\n",
            " 55/469 [==>...........................] - ETA: 10s - loss: 0.2309 - accuracy: 0.9447\n",
            " 59/469 [==>...........................] - ETA: 10s - loss: 0.2252 - accuracy: 0.9464\n",
            " 63/469 [===>..........................] - ETA: 10s - loss: 0.2300 - accuracy: 0.9463\n",
            " 68/469 [===>..........................] - ETA: 10s - loss: 0.2359 - accuracy: 0.9458\n",
            " 72/469 [===>..........................] - ETA: 10s - loss: 0.2485 - accuracy: 0.9429\n",
            " 76/469 [===>..........................] - ETA: 10s - loss: 0.2533 - accuracy: 0.9411\n",
            " 79/469 [====>.........................] - ETA: 10s - loss: 0.2619 - accuracy: 0.9398\n",
            " 84/469 [====>.........................] - ETA: 10s - loss: 0.2666 - accuracy: 0.9391\n",
            " 86/469 [====>.........................] - ETA: 10s - loss: 0.2649 - accuracy: 0.9394\n",
            " 89/469 [====>.........................] - ETA: 10s - loss: 0.2686 - accuracy: 0.9390\n",
            " 91/469 [====>.........................] - ETA: 10s - loss: 0.2692 - accuracy: 0.9389\n",
            " 95/469 [=====>........................] - ETA: 9s - loss: 0.2738 - accuracy: 0.9387\n",
            " 97/469 [=====>........................] - ETA: 9s - loss: 0.2738 - accuracy: 0.9382\n",
            " 99/469 [=====>........................] - ETA: 9s - loss: 0.2731 - accuracy: 0.9380\n",
            "104/469 [=====>........................] - ETA: 9s - loss: 0.2724 - accuracy: 0.9385\n",
            "109/469 [=====>........................] - ETA: 9s - loss: 0.2707 - accuracy: 0.9383\n",
            "113/469 [======>.......................] - ETA: 9s - loss: 0.2704 - accuracy: 0.9381\n",
            "115/469 [======>.......................] - ETA: 9s - loss: 0.2706 - accuracy: 0.9382\n",
            "120/469 [======>.......................] - ETA: 9s - loss: 0.2738 - accuracy: 0.9376\n",
            "124/469 [======>.......................] - ETA: 9s - loss: 0.2715 - accuracy: 0.9380\n",
            "128/469 [=======>......................] - ETA: 9s - loss: 0.2727 - accuracy: 0.9379\n",
            "132/469 [=======>......................] - ETA: 8s - loss: 0.2684 - accuracy: 0.9383\n",
            "136/469 [=======>......................] - ETA: 8s - loss: 0.2673 - accuracy: 0.9385\n",
            "140/469 [=======>......................] - ETA: 8s - loss: 0.2666 - accuracy: 0.9387\n",
            "144/469 [========>.....................] - ETA: 8s - loss: 0.2646 - accuracy: 0.9389\n",
            "148/469 [========>.....................] - ETA: 8s - loss: 0.2634 - accuracy: 0.9391\n",
            "152/469 [========>.....................] - ETA: 8s - loss: 0.2607 - accuracy: 0.9395\n",
            "157/469 [=========>....................] - ETA: 8s - loss: 0.2575 - accuracy: 0.9400\n",
            "161/469 [=========>....................] - ETA: 8s - loss: 0.2544 - accuracy: 0.9406\n",
            "165/469 [=========>....................] - ETA: 8s - loss: 0.2529 - accuracy: 0.9411\n",
            "169/469 [=========>....................] - ETA: 7s - loss: 0.2522 - accuracy: 0.9412\n",
            "174/469 [==========>...................] - ETA: 7s - loss: 0.2502 - accuracy: 0.9414\n",
            "178/469 [==========>...................] - ETA: 7s - loss: 0.2507 - accuracy: 0.9411\n",
            "182/469 [==========>...................] - ETA: 7s - loss: 0.2521 - accuracy: 0.9409\n",
            "186/469 [==========>...................] - ETA: 7s - loss: 0.2514 - accuracy: 0.9409\n",
            "190/469 [===========>..................] - ETA: 7s - loss: 0.2510 - accuracy: 0.9410\n",
            "192/469 [===========>..................] - ETA: 7s - loss: 0.2507 - accuracy: 0.9410\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 0.2507 - accuracy: 0.9407\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 0.2497 - accuracy: 0.9408\n",
            "207/469 [============>.................] - ETA: 6s - loss: 0.2488 - accuracy: 0.9408\n",
            "209/469 [============>.................] - ETA: 6s - loss: 0.2484 - accuracy: 0.9409\n",
            "214/469 [============>.................] - ETA: 6s - loss: 0.2493 - accuracy: 0.9411\n",
            "218/469 [============>.................] - ETA: 6s - loss: 0.2497 - accuracy: 0.9408\n",
            "222/469 [=============>................] - ETA: 6s - loss: 0.2505 - accuracy: 0.9407\n",
            "226/469 [=============>................] - ETA: 6s - loss: 0.2497 - accuracy: 0.9410\n",
            "230/469 [=============>................] - ETA: 6s - loss: 0.2483 - accuracy: 0.9412\n",
            "234/469 [=============>................] - ETA: 6s - loss: 0.2480 - accuracy: 0.9411\n",
            "238/469 [==============>...............] - ETA: 6s - loss: 0.2470 - accuracy: 0.9413\n",
            "242/469 [==============>...............] - ETA: 5s - loss: 0.2463 - accuracy: 0.9413\n",
            "246/469 [==============>...............] - ETA: 5s - loss: 0.2456 - accuracy: 0.9414\n",
            "251/469 [===============>..............] - ETA: 5s - loss: 0.2461 - accuracy: 0.9415\n",
            "255/469 [===============>..............] - ETA: 5s - loss: 0.2465 - accuracy: 0.9414\n",
            "257/469 [===============>..............] - ETA: 5s - loss: 0.2465 - accuracy: 0.9415\n",
            "260/469 [===============>..............] - ETA: 5s - loss: 0.2466 - accuracy: 0.9413\n",
            "263/469 [===============>..............] - ETA: 5s - loss: 0.2472 - accuracy: 0.9410\n",
            "267/469 [================>.............] - ETA: 5s - loss: 0.2470 - accuracy: 0.9410\n",
            "272/469 [================>.............] - ETA: 5s - loss: 0.2469 - accuracy: 0.9410\n",
            "276/469 [================>.............] - ETA: 5s - loss: 0.2471 - accuracy: 0.9412\n",
            "280/469 [================>.............] - ETA: 4s - loss: 0.2488 - accuracy: 0.9407\n",
            "284/469 [=================>............] - ETA: 4s - loss: 0.2483 - accuracy: 0.9408\n",
            "288/469 [=================>............] - ETA: 4s - loss: 0.2478 - accuracy: 0.9408\n",
            "292/469 [=================>............] - ETA: 4s - loss: 0.2464 - accuracy: 0.9410\n",
            "296/469 [=================>............] - ETA: 4s - loss: 0.2467 - accuracy: 0.9410\n",
            "300/469 [==================>...........] - ETA: 4s - loss: 0.2466 - accuracy: 0.9412\n",
            "304/469 [==================>...........] - ETA: 4s - loss: 0.2462 - accuracy: 0.9414\n",
            "308/469 [==================>...........] - ETA: 4s - loss: 0.2447 - accuracy: 0.9416\n",
            "312/469 [==================>...........] - ETA: 4s - loss: 0.2450 - accuracy: 0.9417\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.2438 - accuracy: 0.9419\n",
            "319/469 [===================>..........] - ETA: 3s - loss: 0.2431 - accuracy: 0.9420\n",
            "321/469 [===================>..........] - ETA: 3s - loss: 0.2427 - accuracy: 0.9421\n",
            "324/469 [===================>..........] - ETA: 3s - loss: 0.2416 - accuracy: 0.9423\n",
            "328/469 [===================>..........] - ETA: 3s - loss: 0.2409 - accuracy: 0.9425\n",
            "332/469 [====================>.........] - ETA: 3s - loss: 0.2411 - accuracy: 0.9424\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.2415 - accuracy: 0.9424\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 0.2418 - accuracy: 0.9425\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.2415 - accuracy: 0.9425\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 0.2413 - accuracy: 0.9425\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 0.2402 - accuracy: 0.9427\n",
            "356/469 [=====================>........] - ETA: 2s - loss: 0.2403 - accuracy: 0.9427\n",
            "360/469 [======================>.......] - ETA: 2s - loss: 0.2404 - accuracy: 0.9429\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2399 - accuracy: 0.9429\n",
            "368/469 [======================>.......] - ETA: 2s - loss: 0.2413 - accuracy: 0.9428\n",
            "372/469 [======================>.......] - ETA: 2s - loss: 0.2412 - accuracy: 0.9428\n",
            "376/469 [=======================>......] - ETA: 2s - loss: 0.2408 - accuracy: 0.9430\n",
            "380/469 [=======================>......] - ETA: 2s - loss: 0.2406 - accuracy: 0.9430\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 0.2409 - accuracy: 0.9430\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.2406 - accuracy: 0.9430\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.2402 - accuracy: 0.9431\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.2395 - accuracy: 0.9432\n",
            "396/469 [========================>.....] - ETA: 1s - loss: 0.2390 - accuracy: 0.9433\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 0.2394 - accuracy: 0.9433\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2391 - accuracy: 0.9433\n",
            "402/469 [========================>.....] - ETA: 1s - loss: 0.2388 - accuracy: 0.9433\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2383 - accuracy: 0.9434\n",
            "410/469 [=========================>....] - ETA: 1s - loss: 0.2385 - accuracy: 0.9434\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2390 - accuracy: 0.9434\n",
            "414/469 [=========================>....] - ETA: 1s - loss: 0.2392 - accuracy: 0.9434\n",
            "418/469 [=========================>....] - ETA: 1s - loss: 0.2395 - accuracy: 0.9431\n",
            "422/469 [=========================>....] - ETA: 1s - loss: 0.2389 - accuracy: 0.9432\n",
            "424/469 [==========================>...] - ETA: 1s - loss: 0.2387 - accuracy: 0.9432\n",
            "430/469 [==========================>...] - ETA: 1s - loss: 0.2380 - accuracy: 0.9433\n",
            "434/469 [==========================>...] - ETA: 0s - loss: 0.2383 - accuracy: 0.9433\n",
            "438/469 [===========================>..] - ETA: 0s - loss: 0.2389 - accuracy: 0.9433\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2398 - accuracy: 0.9432\n",
            "444/469 [===========================>..] - ETA: 0s - loss: 0.2394 - accuracy: 0.9433\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.2392 - accuracy: 0.9432\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.2394 - accuracy: 0.9432\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9432\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9433\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9433\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9433\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9434\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.2373 - accuracy: 0.9434 - val_loss: 0.2225 - val_accuracy: 0.9539\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 5/12\n",
            "  3/469 [..............................] - ETA: 12s - loss: 0.1786 - accuracy: 0.9557\n",
            "  7/469 [..............................] - ETA: 12s - loss: 0.2400 - accuracy: 0.9408\n",
            " 12/469 [..............................] - ETA: 11s - loss: 0.2210 - accuracy: 0.9479\n",
            " 14/469 [..............................] - ETA: 11s - loss: 0.2242 - accuracy: 0.9481\n",
            " 16/469 [>.............................] - ETA: 11s - loss: 0.2096 - accuracy: 0.9507\n",
            " 18/469 [>.............................] - ETA: 11s - loss: 0.2078 - accuracy: 0.9514\n",
            " 23/469 [>.............................] - ETA: 11s - loss: 0.2029 - accuracy: 0.9538\n",
            " 27/469 [>.............................] - ETA: 11s - loss: 0.2070 - accuracy: 0.9528\n",
            " 31/469 [>.............................] - ETA: 11s - loss: 0.2084 - accuracy: 0.9506\n",
            " 35/469 [=>............................] - ETA: 11s - loss: 0.2067 - accuracy: 0.9504\n",
            " 39/469 [=>............................] - ETA: 11s - loss: 0.2101 - accuracy: 0.9511\n",
            " 43/469 [=>............................] - ETA: 11s - loss: 0.2090 - accuracy: 0.9508\n",
            " 48/469 [==>...........................] - ETA: 11s - loss: 0.2059 - accuracy: 0.9512\n",
            " 52/469 [==>...........................] - ETA: 11s - loss: 0.2022 - accuracy: 0.9524\n",
            " 55/469 [==>...........................] - ETA: 10s - loss: 0.2021 - accuracy: 0.9526\n",
            " 59/469 [==>...........................] - ETA: 10s - loss: 0.2020 - accuracy: 0.9522\n",
            " 63/469 [===>..........................] - ETA: 10s - loss: 0.2079 - accuracy: 0.9509\n",
            " 67/469 [===>..........................] - ETA: 10s - loss: 0.2040 - accuracy: 0.9515\n",
            " 72/469 [===>..........................] - ETA: 10s - loss: 0.2083 - accuracy: 0.9497\n",
            " 74/469 [===>..........................] - ETA: 10s - loss: 0.2101 - accuracy: 0.9495\n",
            " 76/469 [===>..........................] - ETA: 10s - loss: 0.2151 - accuracy: 0.9494\n",
            " 78/469 [===>..........................] - ETA: 10s - loss: 0.2148 - accuracy: 0.9499\n",
            " 82/469 [====>.........................] - ETA: 10s - loss: 0.2151 - accuracy: 0.9500\n",
            " 87/469 [====>.........................] - ETA: 10s - loss: 0.2208 - accuracy: 0.9493\n",
            " 91/469 [====>.........................] - ETA: 10s - loss: 0.2179 - accuracy: 0.9492\n",
            " 95/469 [=====>........................] - ETA: 9s - loss: 0.2177 - accuracy: 0.9488 \n",
            " 99/469 [=====>........................] - ETA: 9s - loss: 0.2203 - accuracy: 0.9486\n",
            "103/469 [=====>........................] - ETA: 9s - loss: 0.2164 - accuracy: 0.9492\n",
            "107/469 [=====>........................] - ETA: 9s - loss: 0.2154 - accuracy: 0.9495\n",
            "111/469 [======>.......................] - ETA: 9s - loss: 0.2112 - accuracy: 0.9502\n",
            "115/469 [======>.......................] - ETA: 9s - loss: 0.2135 - accuracy: 0.9497\n",
            "119/469 [======>.......................] - ETA: 9s - loss: 0.2124 - accuracy: 0.9502\n",
            "123/469 [======>.......................] - ETA: 9s - loss: 0.2103 - accuracy: 0.9501\n",
            "128/469 [=======>......................] - ETA: 9s - loss: 0.2112 - accuracy: 0.9503\n",
            "132/469 [=======>......................] - ETA: 8s - loss: 0.2113 - accuracy: 0.9499\n",
            "136/469 [=======>......................] - ETA: 8s - loss: 0.2117 - accuracy: 0.9493\n",
            "138/469 [=======>......................] - ETA: 8s - loss: 0.2113 - accuracy: 0.9493\n",
            "143/469 [========>.....................] - ETA: 8s - loss: 0.2118 - accuracy: 0.9496\n",
            "147/469 [========>.....................] - ETA: 8s - loss: 0.2104 - accuracy: 0.9497\n",
            "151/469 [========>.....................] - ETA: 8s - loss: 0.2096 - accuracy: 0.9496\n",
            "155/469 [========>.....................] - ETA: 8s - loss: 0.2096 - accuracy: 0.9497\n",
            "159/469 [=========>....................] - ETA: 8s - loss: 0.2101 - accuracy: 0.9495\n",
            "163/469 [=========>....................] - ETA: 8s - loss: 0.2123 - accuracy: 0.9496\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 0.2126 - accuracy: 0.9494\n",
            "171/469 [=========>....................] - ETA: 7s - loss: 0.2139 - accuracy: 0.9492\n",
            "173/469 [==========>...................] - ETA: 7s - loss: 0.2143 - accuracy: 0.9492\n",
            "176/469 [==========>...................] - ETA: 7s - loss: 0.2158 - accuracy: 0.9492\n",
            "178/469 [==========>...................] - ETA: 7s - loss: 0.2161 - accuracy: 0.9491\n",
            "180/469 [==========>...................] - ETA: 7s - loss: 0.2173 - accuracy: 0.9488\n",
            "184/469 [==========>...................] - ETA: 7s - loss: 0.2179 - accuracy: 0.9487\n",
            "186/469 [==========>...................] - ETA: 7s - loss: 0.2175 - accuracy: 0.9485\n",
            "188/469 [===========>..................] - ETA: 7s - loss: 0.2167 - accuracy: 0.9487\n",
            "190/469 [===========>..................] - ETA: 7s - loss: 0.2180 - accuracy: 0.9486\n",
            "192/469 [===========>..................] - ETA: 7s - loss: 0.2183 - accuracy: 0.9483\n",
            "194/469 [===========>..................] - ETA: 7s - loss: 0.2181 - accuracy: 0.9484\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 0.2188 - accuracy: 0.9480\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 0.2182 - accuracy: 0.9480\n",
            "207/469 [============>.................] - ETA: 6s - loss: 0.2184 - accuracy: 0.9479\n",
            "211/469 [============>.................] - ETA: 6s - loss: 0.2189 - accuracy: 0.9480\n",
            "215/469 [============>.................] - ETA: 6s - loss: 0.2197 - accuracy: 0.9479\n",
            "219/469 [=============>................] - ETA: 6s - loss: 0.2189 - accuracy: 0.9480\n",
            "223/469 [=============>................] - ETA: 6s - loss: 0.2187 - accuracy: 0.9479\n",
            "225/469 [=============>................] - ETA: 6s - loss: 0.2184 - accuracy: 0.9481\n",
            "230/469 [=============>................] - ETA: 6s - loss: 0.2188 - accuracy: 0.9480\n",
            "234/469 [=============>................] - ETA: 6s - loss: 0.2184 - accuracy: 0.9480\n",
            "238/469 [==============>...............] - ETA: 6s - loss: 0.2191 - accuracy: 0.9478\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 0.2176 - accuracy: 0.9481\n",
            "247/469 [==============>...............] - ETA: 5s - loss: 0.2172 - accuracy: 0.9481\n",
            "249/469 [==============>...............] - ETA: 5s - loss: 0.2173 - accuracy: 0.9480\n",
            "255/469 [===============>..............] - ETA: 5s - loss: 0.2188 - accuracy: 0.9476\n",
            "259/469 [===============>..............] - ETA: 5s - loss: 0.2182 - accuracy: 0.9478\n",
            "263/469 [===============>..............] - ETA: 5s - loss: 0.2171 - accuracy: 0.9480\n",
            "267/469 [================>.............] - ETA: 5s - loss: 0.2175 - accuracy: 0.9478\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.2184 - accuracy: 0.9475\n",
            "275/469 [================>.............] - ETA: 5s - loss: 0.2185 - accuracy: 0.9475\n",
            "279/469 [================>.............] - ETA: 5s - loss: 0.2201 - accuracy: 0.9474\n",
            "283/469 [=================>............] - ETA: 4s - loss: 0.2227 - accuracy: 0.9472\n",
            "285/469 [=================>............] - ETA: 4s - loss: 0.2233 - accuracy: 0.9473\n",
            "291/469 [=================>............] - ETA: 4s - loss: 0.2235 - accuracy: 0.9472\n",
            "295/469 [=================>............] - ETA: 4s - loss: 0.2237 - accuracy: 0.9472\n",
            "297/469 [=================>............] - ETA: 4s - loss: 0.2237 - accuracy: 0.9471\n",
            "299/469 [==================>...........] - ETA: 4s - loss: 0.2246 - accuracy: 0.9468\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 0.2243 - accuracy: 0.9467\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.2247 - accuracy: 0.9467\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.2247 - accuracy: 0.9465\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.2249 - accuracy: 0.9465\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.2248 - accuracy: 0.9465\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.2243 - accuracy: 0.9465\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.2243 - accuracy: 0.9465\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.2248 - accuracy: 0.9463\n",
            "321/469 [===================>..........] - ETA: 3s - loss: 0.2254 - accuracy: 0.9463\n",
            "325/469 [===================>..........] - ETA: 3s - loss: 0.2241 - accuracy: 0.9466\n",
            "330/469 [====================>.........] - ETA: 3s - loss: 0.2249 - accuracy: 0.9468\n",
            "334/469 [====================>.........] - ETA: 3s - loss: 0.2241 - accuracy: 0.9469\n",
            "338/469 [====================>.........] - ETA: 3s - loss: 0.2228 - accuracy: 0.9470\n",
            "342/469 [====================>.........] - ETA: 3s - loss: 0.2238 - accuracy: 0.9471\n",
            "344/469 [=====================>........] - ETA: 3s - loss: 0.2240 - accuracy: 0.9472\n",
            "348/469 [=====================>........] - ETA: 3s - loss: 0.2242 - accuracy: 0.9473\n",
            "350/469 [=====================>........] - ETA: 3s - loss: 0.2236 - accuracy: 0.9474\n",
            "354/469 [=====================>........] - ETA: 3s - loss: 0.2240 - accuracy: 0.9475\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2262 - accuracy: 0.9474\n",
            "360/469 [======================>.......] - ETA: 2s - loss: 0.2264 - accuracy: 0.9473\n",
            "362/469 [======================>.......] - ETA: 2s - loss: 0.2263 - accuracy: 0.9472\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 0.2273 - accuracy: 0.9473\n",
            "370/469 [======================>.......] - ETA: 2s - loss: 0.2279 - accuracy: 0.9473\n",
            "372/469 [======================>.......] - ETA: 2s - loss: 0.2290 - accuracy: 0.9472\n",
            "374/469 [======================>.......] - ETA: 2s - loss: 0.2287 - accuracy: 0.9473\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 0.2283 - accuracy: 0.9474\n",
            "382/469 [=======================>......] - ETA: 2s - loss: 0.2277 - accuracy: 0.9473\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 0.2282 - accuracy: 0.9472\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.2298 - accuracy: 0.9469\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.2316 - accuracy: 0.9466\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 0.2326 - accuracy: 0.9464\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2325 - accuracy: 0.9465\n",
            "404/469 [========================>.....] - ETA: 1s - loss: 0.2342 - accuracy: 0.9463\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2343 - accuracy: 0.9462\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 0.2349 - accuracy: 0.9462\n",
            "418/469 [=========================>....] - ETA: 1s - loss: 0.2359 - accuracy: 0.9461\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 0.2356 - accuracy: 0.9461\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.2353 - accuracy: 0.9461\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.2355 - accuracy: 0.9461\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 0.2366 - accuracy: 0.9459\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.2363 - accuracy: 0.9461\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.2365 - accuracy: 0.9460\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2369 - accuracy: 0.9459\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.2363 - accuracy: 0.9460\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2359 - accuracy: 0.9461\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.2361 - accuracy: 0.9461\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9461\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9461\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9461\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9462\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.2362 - accuracy: 0.9462\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9462\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.2360 - accuracy: 0.9463 - val_loss: 0.2296 - val_accuracy: 0.9479\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 6/12\n",
            "  3/469 [..............................] - ETA: 12s - loss: 0.1208 - accuracy: 0.9635\n",
            "  5/469 [..............................] - ETA: 12s - loss: 0.1964 - accuracy: 0.9469\n",
            "  7/469 [..............................] - ETA: 12s - loss: 0.1940 - accuracy: 0.9509\n",
            "  9/469 [..............................] - ETA: 12s - loss: 0.2026 - accuracy: 0.9531\n",
            " 13/469 [..............................] - ETA: 12s - loss: 0.1993 - accuracy: 0.9543\n",
            " 15/469 [..............................] - ETA: 12s - loss: 0.1982 - accuracy: 0.9536\n",
            " 19/469 [>.............................] - ETA: 12s - loss: 0.2183 - accuracy: 0.9511\n",
            " 21/469 [>.............................] - ETA: 12s - loss: 0.2303 - accuracy: 0.9483\n",
            " 23/469 [>.............................] - ETA: 11s - loss: 0.2216 - accuracy: 0.9507\n",
            " 25/469 [>.............................] - ETA: 11s - loss: 0.2134 - accuracy: 0.9509\n",
            " 27/469 [>.............................] - ETA: 11s - loss: 0.2094 - accuracy: 0.9511\n",
            " 31/469 [>.............................] - ETA: 11s - loss: 0.2126 - accuracy: 0.9498\n",
            " 35/469 [=>............................] - ETA: 11s - loss: 0.2065 - accuracy: 0.9511\n",
            " 39/469 [=>............................] - ETA: 11s - loss: 0.2115 - accuracy: 0.9515\n",
            " 43/469 [=>............................] - ETA: 11s - loss: 0.2004 - accuracy: 0.9540\n",
            " 47/469 [==>...........................] - ETA: 11s - loss: 0.1982 - accuracy: 0.9546\n",
            " 52/469 [==>...........................] - ETA: 11s - loss: 0.1934 - accuracy: 0.9555\n",
            " 56/469 [==>...........................] - ETA: 11s - loss: 0.1997 - accuracy: 0.9542\n",
            " 59/469 [==>...........................] - ETA: 11s - loss: 0.2057 - accuracy: 0.9527\n",
            " 63/469 [===>..........................] - ETA: 10s - loss: 0.2090 - accuracy: 0.9520\n",
            " 67/469 [===>..........................] - ETA: 10s - loss: 0.2054 - accuracy: 0.9524\n",
            " 71/469 [===>..........................] - ETA: 10s - loss: 0.2032 - accuracy: 0.9531\n",
            " 75/469 [===>..........................] - ETA: 10s - loss: 0.2022 - accuracy: 0.9535\n",
            " 79/469 [====>.........................] - ETA: 10s - loss: 0.2009 - accuracy: 0.9540\n",
            " 83/469 [====>.........................] - ETA: 10s - loss: 0.2036 - accuracy: 0.9544\n",
            " 85/469 [====>.........................] - ETA: 10s - loss: 0.2053 - accuracy: 0.9546\n",
            " 89/469 [====>.........................] - ETA: 10s - loss: 0.2062 - accuracy: 0.9543\n",
            " 93/469 [====>.........................] - ETA: 10s - loss: 0.2053 - accuracy: 0.9541\n",
            " 97/469 [=====>........................] - ETA: 10s - loss: 0.2059 - accuracy: 0.9538\n",
            "100/469 [=====>........................] - ETA: 9s - loss: 0.2048 - accuracy: 0.9538 \n",
            "105/469 [=====>........................] - ETA: 9s - loss: 0.2053 - accuracy: 0.9536\n",
            "109/469 [=====>........................] - ETA: 9s - loss: 0.2077 - accuracy: 0.9535\n",
            "111/469 [======>.......................] - ETA: 9s - loss: 0.2084 - accuracy: 0.9533\n",
            "115/469 [======>.......................] - ETA: 9s - loss: 0.2278 - accuracy: 0.9489\n",
            "119/469 [======>.......................] - ETA: 9s - loss: 0.2285 - accuracy: 0.9491\n",
            "124/469 [======>.......................] - ETA: 9s - loss: 0.2331 - accuracy: 0.9487\n",
            "128/469 [=======>......................] - ETA: 9s - loss: 0.2346 - accuracy: 0.9489\n",
            "132/469 [=======>......................] - ETA: 9s - loss: 0.2384 - accuracy: 0.9487\n",
            "137/469 [=======>......................] - ETA: 8s - loss: 0.2409 - accuracy: 0.9484\n",
            "141/469 [========>.....................] - ETA: 8s - loss: 0.2429 - accuracy: 0.9486\n",
            "143/469 [========>.....................] - ETA: 8s - loss: 0.2432 - accuracy: 0.9486\n",
            "148/469 [========>.....................] - ETA: 8s - loss: 0.2478 - accuracy: 0.9478\n",
            "151/469 [========>.....................] - ETA: 8s - loss: 0.2494 - accuracy: 0.9477\n",
            "155/469 [========>.....................] - ETA: 8s - loss: 0.2508 - accuracy: 0.9473\n",
            "159/469 [=========>....................] - ETA: 8s - loss: 0.2515 - accuracy: 0.9469\n",
            "163/469 [=========>....................] - ETA: 8s - loss: 0.2536 - accuracy: 0.9468\n",
            "165/469 [=========>....................] - ETA: 8s - loss: 0.2525 - accuracy: 0.9471\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 0.2517 - accuracy: 0.9473\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 0.2543 - accuracy: 0.9471\n",
            "173/469 [==========>...................] - ETA: 7s - loss: 0.2553 - accuracy: 0.9464\n",
            "177/469 [==========>...................] - ETA: 7s - loss: 0.2560 - accuracy: 0.9458\n",
            "181/469 [==========>...................] - ETA: 7s - loss: 0.2562 - accuracy: 0.9458\n",
            "183/469 [==========>...................] - ETA: 7s - loss: 0.2580 - accuracy: 0.9456\n",
            "185/469 [==========>...................] - ETA: 7s - loss: 0.2581 - accuracy: 0.9455\n",
            "187/469 [==========>...................] - ETA: 7s - loss: 0.2619 - accuracy: 0.9452\n",
            "193/469 [===========>..................] - ETA: 7s - loss: 0.2642 - accuracy: 0.9449\n",
            "195/469 [===========>..................] - ETA: 7s - loss: 0.2642 - accuracy: 0.9447\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 0.2644 - accuracy: 0.9448\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.2682 - accuracy: 0.9448\n",
            "209/469 [============>.................] - ETA: 6s - loss: 0.2684 - accuracy: 0.9445\n",
            "213/469 [============>.................] - ETA: 6s - loss: 0.2680 - accuracy: 0.9444\n",
            "216/469 [============>.................] - ETA: 6s - loss: 0.2677 - accuracy: 0.9444\n",
            "220/469 [=============>................] - ETA: 6s - loss: 0.2729 - accuracy: 0.9441\n",
            "226/469 [=============>................] - ETA: 6s - loss: 0.2733 - accuracy: 0.9439\n",
            "228/469 [=============>................] - ETA: 6s - loss: 0.2741 - accuracy: 0.9436\n",
            "230/469 [=============>................] - ETA: 6s - loss: 0.2739 - accuracy: 0.9434\n",
            "234/469 [=============>................] - ETA: 6s - loss: 0.2743 - accuracy: 0.9431\n",
            "238/469 [==============>...............] - ETA: 6s - loss: 0.2728 - accuracy: 0.9432\n",
            "240/469 [==============>...............] - ETA: 6s - loss: 0.2726 - accuracy: 0.9432\n",
            "242/469 [==============>...............] - ETA: 6s - loss: 0.2718 - accuracy: 0.9434\n",
            "246/469 [==============>...............] - ETA: 5s - loss: 0.2725 - accuracy: 0.9434\n",
            "250/469 [==============>...............] - ETA: 5s - loss: 0.2732 - accuracy: 0.9436\n",
            "252/469 [===============>..............] - ETA: 5s - loss: 0.2734 - accuracy: 0.9435\n",
            "254/469 [===============>..............] - ETA: 5s - loss: 0.2741 - accuracy: 0.9435\n",
            "258/469 [===============>..............] - ETA: 5s - loss: 0.2743 - accuracy: 0.9434\n",
            "260/469 [===============>..............] - ETA: 5s - loss: 0.2748 - accuracy: 0.9433\n",
            "264/469 [===============>..............] - ETA: 5s - loss: 0.2735 - accuracy: 0.9434\n",
            "269/469 [================>.............] - ETA: 5s - loss: 0.2732 - accuracy: 0.9435\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.2736 - accuracy: 0.9436\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.2722 - accuracy: 0.9438\n",
            "282/469 [=================>............] - ETA: 4s - loss: 0.2721 - accuracy: 0.9440\n",
            "284/469 [=================>............] - ETA: 4s - loss: 0.2724 - accuracy: 0.9440\n",
            "286/469 [=================>............] - ETA: 4s - loss: 0.2722 - accuracy: 0.9439\n",
            "288/469 [=================>............] - ETA: 4s - loss: 0.2718 - accuracy: 0.9440\n",
            "290/469 [=================>............] - ETA: 4s - loss: 0.2720 - accuracy: 0.9438\n",
            "292/469 [=================>............] - ETA: 4s - loss: 0.2716 - accuracy: 0.9438\n",
            "294/469 [=================>............] - ETA: 4s - loss: 0.2712 - accuracy: 0.9439\n",
            "296/469 [=================>............] - ETA: 4s - loss: 0.2716 - accuracy: 0.9437\n",
            "298/469 [==================>...........] - ETA: 4s - loss: 0.2708 - accuracy: 0.9438\n",
            "302/469 [==================>...........] - ETA: 4s - loss: 0.2696 - accuracy: 0.9437\n",
            "304/469 [==================>...........] - ETA: 4s - loss: 0.2686 - accuracy: 0.9438\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.2671 - accuracy: 0.9442\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.2663 - accuracy: 0.9444\n",
            "314/469 [===================>..........] - ETA: 4s - loss: 0.2649 - accuracy: 0.9446\n",
            "316/469 [===================>..........] - ETA: 4s - loss: 0.2645 - accuracy: 0.9446\n",
            "320/469 [===================>..........] - ETA: 3s - loss: 0.2649 - accuracy: 0.9446\n",
            "322/469 [===================>..........] - ETA: 3s - loss: 0.2654 - accuracy: 0.9445\n",
            "326/469 [===================>..........] - ETA: 3s - loss: 0.2657 - accuracy: 0.9446\n",
            "328/469 [===================>..........] - ETA: 3s - loss: 0.2652 - accuracy: 0.9446\n",
            "333/469 [====================>.........] - ETA: 3s - loss: 0.2642 - accuracy: 0.9448\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.2659 - accuracy: 0.9445\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.2658 - accuracy: 0.9446\n",
            "342/469 [====================>.........] - ETA: 3s - loss: 0.2657 - accuracy: 0.9445\n",
            "344/469 [=====================>........] - ETA: 3s - loss: 0.2653 - accuracy: 0.9444\n",
            "346/469 [=====================>........] - ETA: 3s - loss: 0.2647 - accuracy: 0.9445\n",
            "350/469 [=====================>........] - ETA: 3s - loss: 0.2635 - accuracy: 0.9447\n",
            "352/469 [=====================>........] - ETA: 3s - loss: 0.2628 - accuracy: 0.9448\n",
            "354/469 [=====================>........] - ETA: 3s - loss: 0.2633 - accuracy: 0.9448\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2626 - accuracy: 0.9448\n",
            "362/469 [======================>.......] - ETA: 2s - loss: 0.2622 - accuracy: 0.9447\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2619 - accuracy: 0.9448\n",
            "368/469 [======================>.......] - ETA: 2s - loss: 0.2617 - accuracy: 0.9447\n",
            "372/469 [======================>.......] - ETA: 2s - loss: 0.2627 - accuracy: 0.9445\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 0.2618 - accuracy: 0.9445\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.2616 - accuracy: 0.9445\n",
            "384/469 [=======================>......] - ETA: 2s - loss: 0.2614 - accuracy: 0.9446\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.2594 - accuracy: 0.9450\n",
            "394/469 [========================>.....] - ETA: 2s - loss: 0.2588 - accuracy: 0.9451\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 0.2574 - accuracy: 0.9453\n",
            "402/469 [========================>.....] - ETA: 1s - loss: 0.2570 - accuracy: 0.9455\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2563 - accuracy: 0.9455\n",
            "408/469 [=========================>....] - ETA: 1s - loss: 0.2563 - accuracy: 0.9456\n",
            "410/469 [=========================>....] - ETA: 1s - loss: 0.2566 - accuracy: 0.9455\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2564 - accuracy: 0.9455\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 0.2563 - accuracy: 0.9455\n",
            "422/469 [=========================>....] - ETA: 1s - loss: 0.2563 - accuracy: 0.9455\n",
            "426/469 [==========================>...] - ETA: 1s - loss: 0.2563 - accuracy: 0.9456\n",
            "430/469 [==========================>...] - ETA: 1s - loss: 0.2562 - accuracy: 0.9457\n",
            "432/469 [==========================>...] - ETA: 0s - loss: 0.2557 - accuracy: 0.9457\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.2557 - accuracy: 0.9456\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2551 - accuracy: 0.9457\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2553 - accuracy: 0.9457\n",
            "450/469 [===========================>..] - ETA: 0s - loss: 0.2553 - accuracy: 0.9456\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2560 - accuracy: 0.9455\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9455\n",
            "462/469 [============================>.] - ETA: 0s - loss: 0.2571 - accuracy: 0.9450\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9450\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.2568 - accuracy: 0.9449\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.2566 - accuracy: 0.9450 - val_loss: 0.3725 - val_accuracy: 0.9422\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 7/12\n",
            "  1/469 [..............................] - ETA: 12s - loss: 0.3971 - accuracy: 0.9219\n",
            "  5/469 [..............................] - ETA: 12s - loss: 0.2701 - accuracy: 0.9531\n",
            "  9/469 [..............................] - ETA: 12s - loss: 0.3369 - accuracy: 0.9453\n",
            " 14/469 [..............................] - ETA: 12s - loss: 0.3183 - accuracy: 0.9414\n",
            " 18/469 [>.............................] - ETA: 12s - loss: 0.3074 - accuracy: 0.9453\n",
            " 20/469 [>.............................] - ETA: 12s - loss: 0.3234 - accuracy: 0.9426\n",
            " 25/469 [>.............................] - ETA: 11s - loss: 0.3013 - accuracy: 0.9441\n",
            " 29/469 [>.............................] - ETA: 11s - loss: 0.2988 - accuracy: 0.9434\n",
            " 35/469 [=>............................] - ETA: 11s - loss: 0.3107 - accuracy: 0.9406\n",
            " 37/469 [=>............................] - ETA: 11s - loss: 0.3036 - accuracy: 0.9409\n",
            " 39/469 [=>............................] - ETA: 11s - loss: 0.3119 - accuracy: 0.9413\n",
            " 42/469 [=>............................] - ETA: 11s - loss: 0.3215 - accuracy: 0.9416\n",
            " 46/469 [=>............................] - ETA: 11s - loss: 0.3060 - accuracy: 0.9441\n",
            " 50/469 [==>...........................] - ETA: 11s - loss: 0.2994 - accuracy: 0.9450\n",
            " 54/469 [==>...........................] - ETA: 10s - loss: 0.3073 - accuracy: 0.9449\n",
            " 58/469 [==>...........................] - ETA: 10s - loss: 0.3156 - accuracy: 0.9433\n",
            " 62/469 [==>...........................] - ETA: 10s - loss: 0.3211 - accuracy: 0.9427\n",
            " 66/469 [===>..........................] - ETA: 10s - loss: 0.3383 - accuracy: 0.9405\n",
            " 70/469 [===>..........................] - ETA: 10s - loss: 0.3441 - accuracy: 0.9401\n",
            " 74/469 [===>..........................] - ETA: 10s - loss: 0.3421 - accuracy: 0.9394\n",
            " 79/469 [====>.........................] - ETA: 10s - loss: 0.3427 - accuracy: 0.9388\n",
            " 83/469 [====>.........................] - ETA: 10s - loss: 0.3375 - accuracy: 0.9395\n",
            " 88/469 [====>.........................] - ETA: 10s - loss: 0.3372 - accuracy: 0.9396\n",
            " 92/469 [====>.........................] - ETA: 9s - loss: 0.3313 - accuracy: 0.9400\n",
            " 96/469 [=====>........................] - ETA: 9s - loss: 0.3275 - accuracy: 0.9404\n",
            " 99/469 [=====>........................] - ETA: 9s - loss: 0.3270 - accuracy: 0.9404\n",
            "103/469 [=====>........................] - ETA: 9s - loss: 0.3236 - accuracy: 0.9405\n",
            "107/469 [=====>........................] - ETA: 9s - loss: 0.3217 - accuracy: 0.9403\n",
            "112/469 [======>.......................] - ETA: 9s - loss: 0.3179 - accuracy: 0.9404\n",
            "116/469 [======>.......................] - ETA: 9s - loss: 0.3171 - accuracy: 0.9405\n",
            "118/469 [======>.......................] - ETA: 9s - loss: 0.3164 - accuracy: 0.9401\n",
            "124/469 [======>.......................] - ETA: 9s - loss: 0.3159 - accuracy: 0.9407\n",
            "127/469 [=======>......................] - ETA: 8s - loss: 0.3145 - accuracy: 0.9406\n",
            "132/469 [=======>......................] - ETA: 8s - loss: 0.3152 - accuracy: 0.9406\n",
            "136/469 [=======>......................] - ETA: 8s - loss: 0.3135 - accuracy: 0.9404\n",
            "140/469 [=======>......................] - ETA: 8s - loss: 0.3139 - accuracy: 0.9402\n",
            "142/469 [========>.....................] - ETA: 8s - loss: 0.3120 - accuracy: 0.9404\n",
            "144/469 [========>.....................] - ETA: 8s - loss: 0.3103 - accuracy: 0.9404\n",
            "146/469 [========>.....................] - ETA: 8s - loss: 0.3076 - accuracy: 0.9409\n",
            "150/469 [========>.....................] - ETA: 8s - loss: 0.3066 - accuracy: 0.9411\n",
            "155/469 [========>.....................] - ETA: 8s - loss: 0.3023 - accuracy: 0.9417\n",
            "159/469 [=========>....................] - ETA: 8s - loss: 0.3037 - accuracy: 0.9422\n",
            "163/469 [=========>....................] - ETA: 8s - loss: 0.3007 - accuracy: 0.9427\n",
            "167/469 [=========>....................] - ETA: 7s - loss: 0.3010 - accuracy: 0.9426\n",
            "171/469 [=========>....................] - ETA: 7s - loss: 0.2981 - accuracy: 0.9426\n",
            "175/469 [==========>...................] - ETA: 7s - loss: 0.2969 - accuracy: 0.9427\n",
            "179/469 [==========>...................] - ETA: 7s - loss: 0.2951 - accuracy: 0.9428\n",
            "183/469 [==========>...................] - ETA: 7s - loss: 0.2923 - accuracy: 0.9431\n",
            "187/469 [==========>...................] - ETA: 7s - loss: 0.2914 - accuracy: 0.9429\n",
            "191/469 [===========>..................] - ETA: 7s - loss: 0.2889 - accuracy: 0.9432\n",
            "195/469 [===========>..................] - ETA: 7s - loss: 0.2873 - accuracy: 0.9431\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 0.2860 - accuracy: 0.9434\n",
            "204/469 [============>.................] - ETA: 7s - loss: 0.2847 - accuracy: 0.9436\n",
            "208/469 [============>.................] - ETA: 6s - loss: 0.2861 - accuracy: 0.9433\n",
            "210/469 [============>.................] - ETA: 6s - loss: 0.2870 - accuracy: 0.9431\n",
            "212/469 [============>.................] - ETA: 6s - loss: 0.2868 - accuracy: 0.9431\n",
            "215/469 [============>.................] - ETA: 6s - loss: 0.2872 - accuracy: 0.9429\n",
            "220/469 [=============>................] - ETA: 6s - loss: 0.2863 - accuracy: 0.9428\n",
            "223/469 [=============>................] - ETA: 6s - loss: 0.2846 - accuracy: 0.9430\n",
            "228/469 [=============>................] - ETA: 6s - loss: 0.2822 - accuracy: 0.9434\n",
            "232/469 [=============>................] - ETA: 6s - loss: 0.2799 - accuracy: 0.9436\n",
            "236/469 [==============>...............] - ETA: 6s - loss: 0.2799 - accuracy: 0.9435\n",
            "240/469 [==============>...............] - ETA: 6s - loss: 0.2803 - accuracy: 0.9438\n",
            "244/469 [==============>...............] - ETA: 5s - loss: 0.2793 - accuracy: 0.9439\n",
            "246/469 [==============>...............] - ETA: 5s - loss: 0.2786 - accuracy: 0.9440\n",
            "252/469 [===============>..............] - ETA: 5s - loss: 0.2776 - accuracy: 0.9443\n",
            "256/469 [===============>..............] - ETA: 5s - loss: 0.2770 - accuracy: 0.9442\n",
            "260/469 [===============>..............] - ETA: 5s - loss: 0.2755 - accuracy: 0.9442\n",
            "262/469 [===============>..............] - ETA: 5s - loss: 0.2755 - accuracy: 0.9443\n",
            "265/469 [===============>..............] - ETA: 5s - loss: 0.2747 - accuracy: 0.9443\n",
            "269/469 [================>.............] - ETA: 5s - loss: 0.2733 - accuracy: 0.9446\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.2728 - accuracy: 0.9445\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.2717 - accuracy: 0.9445\n",
            "279/469 [================>.............] - ETA: 5s - loss: 0.2709 - accuracy: 0.9446\n",
            "281/469 [================>.............] - ETA: 4s - loss: 0.2710 - accuracy: 0.9446\n",
            "283/469 [=================>............] - ETA: 4s - loss: 0.2704 - accuracy: 0.9446\n",
            "285/469 [=================>............] - ETA: 4s - loss: 0.2708 - accuracy: 0.9447\n",
            "287/469 [=================>............] - ETA: 4s - loss: 0.2703 - accuracy: 0.9447\n",
            "291/469 [=================>............] - ETA: 4s - loss: 0.2688 - accuracy: 0.9449\n",
            "295/469 [=================>............] - ETA: 4s - loss: 0.2685 - accuracy: 0.9449\n",
            "299/469 [==================>...........] - ETA: 4s - loss: 0.2678 - accuracy: 0.9449\n",
            "304/469 [==================>...........] - ETA: 4s - loss: 0.2679 - accuracy: 0.9450\n",
            "308/469 [==================>...........] - ETA: 4s - loss: 0.2667 - accuracy: 0.9451\n",
            "312/469 [==================>...........] - ETA: 4s - loss: 0.2669 - accuracy: 0.9451\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.2659 - accuracy: 0.9452\n",
            "321/469 [===================>..........] - ETA: 3s - loss: 0.2662 - accuracy: 0.9449\n",
            "324/469 [===================>..........] - ETA: 3s - loss: 0.2652 - accuracy: 0.9448\n",
            "328/469 [===================>..........] - ETA: 3s - loss: 0.2640 - accuracy: 0.9449\n",
            "332/469 [====================>.........] - ETA: 3s - loss: 0.2632 - accuracy: 0.9451\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 0.2633 - accuracy: 0.9451\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 0.2624 - accuracy: 0.9453\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.2613 - accuracy: 0.9456\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 0.2605 - accuracy: 0.9456\n",
            "352/469 [=====================>........] - ETA: 3s - loss: 0.2605 - accuracy: 0.9457\n",
            "354/469 [=====================>........] - ETA: 3s - loss: 0.2601 - accuracy: 0.9458\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2598 - accuracy: 0.9459\n",
            "362/469 [======================>.......] - ETA: 2s - loss: 0.2590 - accuracy: 0.9461\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 0.2601 - accuracy: 0.9459\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.2597 - accuracy: 0.9458\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 0.2591 - accuracy: 0.9458\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 0.2580 - accuracy: 0.9459\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 0.2585 - accuracy: 0.9459\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 0.2582 - accuracy: 0.9461\n",
            "390/469 [=======================>......] - ETA: 2s - loss: 0.2584 - accuracy: 0.9462\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.2570 - accuracy: 0.9463\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 0.2559 - accuracy: 0.9465\n",
            "402/469 [========================>.....] - ETA: 1s - loss: 0.2555 - accuracy: 0.9465\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2556 - accuracy: 0.9465\n",
            "410/469 [=========================>....] - ETA: 1s - loss: 0.2550 - accuracy: 0.9467\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2550 - accuracy: 0.9467\n",
            "414/469 [=========================>....] - ETA: 1s - loss: 0.2544 - accuracy: 0.9468\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 0.2533 - accuracy: 0.9469\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 0.2520 - accuracy: 0.9471\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.2506 - accuracy: 0.9472\n",
            "430/469 [==========================>...] - ETA: 1s - loss: 0.2511 - accuracy: 0.9473\n",
            "434/469 [==========================>...] - ETA: 0s - loss: 0.2511 - accuracy: 0.9472\n",
            "438/469 [===========================>..] - ETA: 0s - loss: 0.2507 - accuracy: 0.9473\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2492 - accuracy: 0.9475\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2481 - accuracy: 0.9477\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.2486 - accuracy: 0.9477\n",
            "450/469 [===========================>..] - ETA: 0s - loss: 0.2486 - accuracy: 0.9477\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.9477\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9476\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.2496 - accuracy: 0.9474\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9475\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9472\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.2506 - accuracy: 0.9470\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.2510 - accuracy: 0.9469 - val_loss: 0.4988 - val_accuracy: 0.9216\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 8/12\n",
            "  1/469 [..............................] - ETA: 12s - loss: 0.5121 - accuracy: 0.8984\n",
            "  5/469 [..............................] - ETA: 12s - loss: 0.5613 - accuracy: 0.8813\n",
            "  9/469 [..............................] - ETA: 12s - loss: 0.4409 - accuracy: 0.9141\n",
            " 13/469 [..............................] - ETA: 12s - loss: 0.4035 - accuracy: 0.9231\n",
            " 17/469 [>.............................] - ETA: 12s - loss: 0.4064 - accuracy: 0.9246\n",
            " 19/469 [>.............................] - ETA: 12s - loss: 0.3958 - accuracy: 0.9276\n",
            " 22/469 [>.............................] - ETA: 11s - loss: 0.3751 - accuracy: 0.9304\n",
            " 24/469 [>.............................] - ETA: 11s - loss: 0.3632 - accuracy: 0.9316\n",
            " 26/469 [>.............................] - ETA: 11s - loss: 0.3570 - accuracy: 0.9315\n",
            " 30/469 [>.............................] - ETA: 11s - loss: 0.3461 - accuracy: 0.9328\n",
            " 33/469 [=>............................] - ETA: 11s - loss: 0.3323 - accuracy: 0.9347\n",
            " 37/469 [=>............................] - ETA: 11s - loss: 0.3352 - accuracy: 0.9354\n",
            " 41/469 [=>............................] - ETA: 11s - loss: 0.3317 - accuracy: 0.9362\n",
            " 45/469 [=>............................] - ETA: 11s - loss: 0.3206 - accuracy: 0.9387\n",
            " 49/469 [==>...........................] - ETA: 11s - loss: 0.3131 - accuracy: 0.9402\n",
            " 54/469 [==>...........................] - ETA: 10s - loss: 0.3041 - accuracy: 0.9416\n",
            " 56/469 [==>...........................] - ETA: 10s - loss: 0.3036 - accuracy: 0.9415\n",
            " 61/469 [==>...........................] - ETA: 10s - loss: 0.2964 - accuracy: 0.9428\n",
            " 65/469 [===>..........................] - ETA: 10s - loss: 0.3020 - accuracy: 0.9424\n",
            " 69/469 [===>..........................] - ETA: 10s - loss: 0.3008 - accuracy: 0.9420\n",
            " 74/469 [===>..........................] - ETA: 10s - loss: 0.2959 - accuracy: 0.9423\n",
            " 78/469 [===>..........................] - ETA: 10s - loss: 0.2907 - accuracy: 0.9427\n",
            " 82/469 [====>.........................] - ETA: 10s - loss: 0.2913 - accuracy: 0.9429\n",
            " 87/469 [====>.........................] - ETA: 10s - loss: 0.2853 - accuracy: 0.9432\n",
            " 89/469 [====>.........................] - ETA: 10s - loss: 0.2848 - accuracy: 0.9430\n",
            " 93/469 [====>.........................] - ETA: 9s - loss: 0.2815 - accuracy: 0.9436\n",
            " 95/469 [=====>........................] - ETA: 9s - loss: 0.2776 - accuracy: 0.9442\n",
            " 97/469 [=====>........................] - ETA: 9s - loss: 0.2760 - accuracy: 0.9442\n",
            " 99/469 [=====>........................] - ETA: 9s - loss: 0.2719 - accuracy: 0.9450\n",
            "101/469 [=====>........................] - ETA: 9s - loss: 0.2696 - accuracy: 0.9452\n",
            "103/469 [=====>........................] - ETA: 9s - loss: 0.2688 - accuracy: 0.9454\n",
            "107/469 [=====>........................] - ETA: 9s - loss: 0.2650 - accuracy: 0.9459\n",
            "111/469 [======>.......................] - ETA: 9s - loss: 0.2608 - accuracy: 0.9467\n",
            "115/469 [======>.......................] - ETA: 9s - loss: 0.2562 - accuracy: 0.9475\n",
            "118/469 [======>.......................] - ETA: 9s - loss: 0.2557 - accuracy: 0.9477\n",
            "122/469 [======>.......................] - ETA: 9s - loss: 0.2529 - accuracy: 0.9479\n",
            "126/469 [=======>......................] - ETA: 9s - loss: 0.2513 - accuracy: 0.9482\n",
            "130/469 [=======>......................] - ETA: 8s - loss: 0.2480 - accuracy: 0.9489\n",
            "135/469 [=======>......................] - ETA: 8s - loss: 0.2442 - accuracy: 0.9496\n",
            "139/469 [=======>......................] - ETA: 8s - loss: 0.2436 - accuracy: 0.9497\n",
            "141/469 [========>.....................] - ETA: 8s - loss: 0.2422 - accuracy: 0.9497\n",
            "146/469 [========>.....................] - ETA: 8s - loss: 0.2447 - accuracy: 0.9493\n",
            "150/469 [========>.....................] - ETA: 8s - loss: 0.2440 - accuracy: 0.9494\n",
            "154/469 [========>.....................] - ETA: 8s - loss: 0.2451 - accuracy: 0.9495\n",
            "158/469 [=========>....................] - ETA: 8s - loss: 0.2452 - accuracy: 0.9493\n",
            "162/469 [=========>....................] - ETA: 8s - loss: 0.2457 - accuracy: 0.9493\n",
            "166/469 [=========>....................] - ETA: 8s - loss: 0.2439 - accuracy: 0.9495\n",
            "170/469 [=========>....................] - ETA: 7s - loss: 0.2422 - accuracy: 0.9501\n",
            "174/469 [==========>...................] - ETA: 7s - loss: 0.2384 - accuracy: 0.9507\n",
            "178/469 [==========>...................] - ETA: 7s - loss: 0.2373 - accuracy: 0.9511\n",
            "182/469 [==========>...................] - ETA: 7s - loss: 0.2366 - accuracy: 0.9513\n",
            "186/469 [==========>...................] - ETA: 7s - loss: 0.2374 - accuracy: 0.9514\n",
            "190/469 [===========>..................] - ETA: 7s - loss: 0.2384 - accuracy: 0.9512\n",
            "194/469 [===========>..................] - ETA: 7s - loss: 0.2405 - accuracy: 0.9512\n",
            "198/469 [===========>..................] - ETA: 7s - loss: 0.2390 - accuracy: 0.9513\n",
            "202/469 [===========>..................] - ETA: 7s - loss: 0.2390 - accuracy: 0.9513\n",
            "206/469 [============>.................] - ETA: 7s - loss: 0.2399 - accuracy: 0.9513\n",
            "210/469 [============>.................] - ETA: 6s - loss: 0.2407 - accuracy: 0.9510\n",
            "214/469 [============>.................] - ETA: 6s - loss: 0.2425 - accuracy: 0.9507\n",
            "217/469 [============>.................] - ETA: 6s - loss: 0.2444 - accuracy: 0.9508\n",
            "221/469 [=============>................] - ETA: 6s - loss: 0.2440 - accuracy: 0.9507\n",
            "226/469 [=============>................] - ETA: 6s - loss: 0.2422 - accuracy: 0.9509\n",
            "228/469 [=============>................] - ETA: 6s - loss: 0.2423 - accuracy: 0.9509\n",
            "231/469 [=============>................] - ETA: 6s - loss: 0.2424 - accuracy: 0.9508\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 0.2421 - accuracy: 0.9508\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 0.2436 - accuracy: 0.9503\n",
            "242/469 [==============>...............] - ETA: 6s - loss: 0.2438 - accuracy: 0.9500\n",
            "246/469 [==============>...............] - ETA: 5s - loss: 0.2445 - accuracy: 0.9498\n",
            "250/469 [==============>...............] - ETA: 5s - loss: 0.2440 - accuracy: 0.9496\n",
            "254/469 [===============>..............] - ETA: 5s - loss: 0.2439 - accuracy: 0.9496\n",
            "258/469 [===============>..............] - ETA: 5s - loss: 0.2424 - accuracy: 0.9499\n",
            "262/469 [===============>..............] - ETA: 5s - loss: 0.2424 - accuracy: 0.9501\n",
            "266/469 [================>.............] - ETA: 5s - loss: 0.2414 - accuracy: 0.9501\n",
            "270/469 [================>.............] - ETA: 5s - loss: 0.2409 - accuracy: 0.9502\n",
            "274/469 [================>.............] - ETA: 5s - loss: 0.2400 - accuracy: 0.9502\n",
            "278/469 [================>.............] - ETA: 5s - loss: 0.2400 - accuracy: 0.9502\n",
            "282/469 [=================>............] - ETA: 4s - loss: 0.2395 - accuracy: 0.9503\n",
            "286/469 [=================>............] - ETA: 4s - loss: 0.2396 - accuracy: 0.9504\n",
            "290/469 [=================>............] - ETA: 4s - loss: 0.2403 - accuracy: 0.9503\n",
            "294/469 [=================>............] - ETA: 4s - loss: 0.2410 - accuracy: 0.9504\n",
            "298/469 [==================>...........] - ETA: 4s - loss: 0.2419 - accuracy: 0.9503\n",
            "302/469 [==================>...........] - ETA: 4s - loss: 0.2421 - accuracy: 0.9502\n",
            "304/469 [==================>...........] - ETA: 4s - loss: 0.2423 - accuracy: 0.9501\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.2457 - accuracy: 0.9496\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.2471 - accuracy: 0.9493\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.2486 - accuracy: 0.9492\n",
            "322/469 [===================>..........] - ETA: 3s - loss: 0.2483 - accuracy: 0.9494\n",
            "326/469 [===================>..........] - ETA: 3s - loss: 0.2485 - accuracy: 0.9491\n",
            "329/469 [====================>.........] - ETA: 3s - loss: 0.2499 - accuracy: 0.9490\n",
            "333/469 [====================>.........] - ETA: 3s - loss: 0.2497 - accuracy: 0.9491\n",
            "338/469 [====================>.........] - ETA: 3s - loss: 0.2516 - accuracy: 0.9490\n",
            "340/469 [====================>.........] - ETA: 3s - loss: 0.2522 - accuracy: 0.9490\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.2521 - accuracy: 0.9491\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.2522 - accuracy: 0.9491\n",
            "350/469 [=====================>........] - ETA: 3s - loss: 0.2519 - accuracy: 0.9490\n",
            "354/469 [=====================>........] - ETA: 3s - loss: 0.2527 - accuracy: 0.9488\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2531 - accuracy: 0.9487\n",
            "362/469 [======================>.......] - ETA: 2s - loss: 0.2550 - accuracy: 0.9485\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 0.2561 - accuracy: 0.9484\n",
            "370/469 [======================>.......] - ETA: 2s - loss: 0.2572 - accuracy: 0.9482\n",
            "372/469 [======================>.......] - ETA: 2s - loss: 0.2587 - accuracy: 0.9481\n",
            "378/469 [=======================>......] - ETA: 2s - loss: 0.2644 - accuracy: 0.9478\n",
            "382/469 [=======================>......] - ETA: 2s - loss: 0.2645 - accuracy: 0.9478\n",
            "386/469 [=======================>......] - ETA: 2s - loss: 0.2662 - accuracy: 0.9475\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.2685 - accuracy: 0.9472\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.2696 - accuracy: 0.9471\n",
            "399/469 [========================>.....] - ETA: 1s - loss: 0.2731 - accuracy: 0.9466\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.2735 - accuracy: 0.9464\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 0.2764 - accuracy: 0.9460\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 0.2777 - accuracy: 0.9458\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.2786 - accuracy: 0.9456\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.2782 - accuracy: 0.9455\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.2787 - accuracy: 0.9454\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.2794 - accuracy: 0.9454\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 0.2798 - accuracy: 0.9455\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.2805 - accuracy: 0.9454\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.2804 - accuracy: 0.9454\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.2809 - accuracy: 0.9454\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.2816 - accuracy: 0.9454\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2835 - accuracy: 0.9451\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9448\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9445\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.9444\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2881 - accuracy: 0.9441\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9441\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.2881 - accuracy: 0.9441 - val_loss: 0.3143 - val_accuracy: 0.9436\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 9/12\n",
            "  3/469 [..............................] - ETA: 12s - loss: 0.2637 - accuracy: 0.9349\n",
            "  7/469 [..............................] - ETA: 12s - loss: 0.2327 - accuracy: 0.9408\n",
            " 11/469 [..............................] - ETA: 12s - loss: 0.2191 - accuracy: 0.9439\n",
            " 15/469 [..............................] - ETA: 12s - loss: 0.2480 - accuracy: 0.9422\n",
            " 19/469 [>.............................] - ETA: 12s - loss: 0.2701 - accuracy: 0.9437\n",
            " 24/469 [>.............................] - ETA: 11s - loss: 0.3223 - accuracy: 0.9424\n",
            " 28/469 [>.............................] - ETA: 11s - loss: 0.3151 - accuracy: 0.9414\n",
            " 30/469 [>.............................] - ETA: 11s - loss: 0.3140 - accuracy: 0.9409\n",
            " 32/469 [=>............................] - ETA: 11s - loss: 0.3140 - accuracy: 0.9409\n",
            " 34/469 [=>............................] - ETA: 11s - loss: 0.3112 - accuracy: 0.9414\n",
            " 36/469 [=>............................] - ETA: 11s - loss: 0.3150 - accuracy: 0.9421\n",
            " 38/469 [=>............................] - ETA: 11s - loss: 0.3130 - accuracy: 0.9416\n",
            " 40/469 [=>............................] - ETA: 11s - loss: 0.3136 - accuracy: 0.9424\n",
            " 42/469 [=>............................] - ETA: 11s - loss: 0.3066 - accuracy: 0.9442\n",
            " 46/469 [=>............................] - ETA: 11s - loss: 0.3028 - accuracy: 0.9443\n",
            " 50/469 [==>...........................] - ETA: 11s - loss: 0.3133 - accuracy: 0.9442\n",
            " 54/469 [==>...........................] - ETA: 11s - loss: 0.3241 - accuracy: 0.9427\n",
            " 56/469 [==>...........................] - ETA: 11s - loss: 0.3228 - accuracy: 0.9421\n",
            " 58/469 [==>...........................] - ETA: 11s - loss: 0.3252 - accuracy: 0.9417\n",
            " 62/469 [==>...........................] - ETA: 10s - loss: 0.3265 - accuracy: 0.9415\n",
            " 66/469 [===>..........................] - ETA: 10s - loss: 0.3240 - accuracy: 0.9415\n",
            " 71/469 [===>..........................] - ETA: 10s - loss: 0.3327 - accuracy: 0.9397\n",
            " 75/469 [===>..........................] - ETA: 10s - loss: 0.3248 - accuracy: 0.9394\n",
            " 79/469 [====>.........................] - ETA: 10s - loss: 0.3308 - accuracy: 0.9370\n",
            " 84/469 [====>.........................] - ETA: 10s - loss: 0.3405 - accuracy: 0.9368\n",
            " 88/469 [====>.........................] - ETA: 10s - loss: 0.3382 - accuracy: 0.9370\n",
            " 90/469 [====>.........................] - ETA: 10s - loss: 0.3394 - accuracy: 0.9363\n",
            " 95/469 [=====>........................] - ETA: 9s - loss: 0.3419 - accuracy: 0.9351 \n",
            " 98/469 [=====>........................] - ETA: 9s - loss: 0.3388 - accuracy: 0.9352\n",
            "100/469 [=====>........................] - ETA: 9s - loss: 0.3356 - accuracy: 0.9356\n",
            "102/469 [=====>........................] - ETA: 9s - loss: 0.3348 - accuracy: 0.9357\n",
            "104/469 [=====>........................] - ETA: 9s - loss: 0.3394 - accuracy: 0.9355\n",
            "108/469 [=====>........................] - ETA: 9s - loss: 0.3392 - accuracy: 0.9358\n",
            "110/469 [======>.......................] - ETA: 9s - loss: 0.3352 - accuracy: 0.9364\n",
            "112/469 [======>.......................] - ETA: 9s - loss: 0.3331 - accuracy: 0.9369\n",
            "116/469 [======>.......................] - ETA: 9s - loss: 0.3377 - accuracy: 0.9368\n",
            "118/469 [======>.......................] - ETA: 9s - loss: 0.3365 - accuracy: 0.9368\n",
            "120/469 [======>.......................] - ETA: 9s - loss: 0.3377 - accuracy: 0.9368\n",
            "122/469 [======>.......................] - ETA: 9s - loss: 0.3359 - accuracy: 0.9371\n",
            "127/469 [=======>......................] - ETA: 9s - loss: 0.3355 - accuracy: 0.9371\n",
            "131/469 [=======>......................] - ETA: 9s - loss: 0.3355 - accuracy: 0.9365\n",
            "135/469 [=======>......................] - ETA: 8s - loss: 0.3364 - accuracy: 0.9368\n",
            "139/469 [=======>......................] - ETA: 8s - loss: 0.3328 - accuracy: 0.9371\n",
            "143/469 [========>.....................] - ETA: 8s - loss: 0.3306 - accuracy: 0.9374\n",
            "147/469 [========>.....................] - ETA: 8s - loss: 0.3272 - accuracy: 0.9380\n",
            "151/469 [========>.....................] - ETA: 8s - loss: 0.3254 - accuracy: 0.9382\n",
            "155/469 [========>.....................] - ETA: 8s - loss: 0.3246 - accuracy: 0.9380\n",
            "159/469 [=========>....................] - ETA: 8s - loss: 0.3235 - accuracy: 0.9379\n",
            "163/469 [=========>....................] - ETA: 8s - loss: 0.3225 - accuracy: 0.9381\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 0.3247 - accuracy: 0.9381\n",
            "171/469 [=========>....................] - ETA: 7s - loss: 0.3253 - accuracy: 0.9381\n",
            "175/469 [==========>...................] - ETA: 7s - loss: 0.3249 - accuracy: 0.9385\n",
            "180/469 [==========>...................] - ETA: 7s - loss: 0.3286 - accuracy: 0.9380\n",
            "184/469 [==========>...................] - ETA: 7s - loss: 0.3279 - accuracy: 0.9378\n",
            "187/469 [==========>...................] - ETA: 7s - loss: 0.3271 - accuracy: 0.9375\n",
            "191/469 [===========>..................] - ETA: 7s - loss: 0.3268 - accuracy: 0.9376\n",
            "195/469 [===========>..................] - ETA: 7s - loss: 0.3251 - accuracy: 0.9377\n",
            "200/469 [===========>..................] - ETA: 7s - loss: 0.3242 - accuracy: 0.9380\n",
            "204/469 [============>.................] - ETA: 7s - loss: 0.3241 - accuracy: 0.9380\n",
            "208/469 [============>.................] - ETA: 6s - loss: 0.3228 - accuracy: 0.9380\n",
            "212/469 [============>.................] - ETA: 6s - loss: 0.3209 - accuracy: 0.9383\n",
            "214/469 [============>.................] - ETA: 6s - loss: 0.3202 - accuracy: 0.9383\n",
            "219/469 [=============>................] - ETA: 6s - loss: 0.3189 - accuracy: 0.9387\n",
            "223/469 [=============>................] - ETA: 6s - loss: 0.3186 - accuracy: 0.9386\n",
            "228/469 [=============>................] - ETA: 6s - loss: 0.3175 - accuracy: 0.9387\n",
            "231/469 [=============>................] - ETA: 6s - loss: 0.3167 - accuracy: 0.9388\n",
            "236/469 [==============>...............] - ETA: 6s - loss: 0.3165 - accuracy: 0.9388\n",
            "240/469 [==============>...............] - ETA: 6s - loss: 0.3149 - accuracy: 0.9390\n",
            "244/469 [==============>...............] - ETA: 5s - loss: 0.3137 - accuracy: 0.9391\n",
            "248/469 [==============>...............] - ETA: 5s - loss: 0.3135 - accuracy: 0.9392\n",
            "253/469 [===============>..............] - ETA: 5s - loss: 0.3126 - accuracy: 0.9391\n",
            "257/469 [===============>..............] - ETA: 5s - loss: 0.3121 - accuracy: 0.9391\n",
            "261/469 [===============>..............] - ETA: 5s - loss: 0.3134 - accuracy: 0.9391\n",
            "266/469 [================>.............] - ETA: 5s - loss: 0.3124 - accuracy: 0.9392\n",
            "270/469 [================>.............] - ETA: 5s - loss: 0.3114 - accuracy: 0.9392\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.3098 - accuracy: 0.9395\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.3097 - accuracy: 0.9396\n",
            "280/469 [================>.............] - ETA: 5s - loss: 0.3094 - accuracy: 0.9398\n",
            "282/469 [=================>............] - ETA: 4s - loss: 0.3107 - accuracy: 0.9396\n",
            "286/469 [=================>............] - ETA: 4s - loss: 0.3106 - accuracy: 0.9397\n",
            "290/469 [=================>............] - ETA: 4s - loss: 0.3098 - accuracy: 0.9399\n",
            "292/469 [=================>............] - ETA: 4s - loss: 0.3096 - accuracy: 0.9400\n",
            "296/469 [=================>............] - ETA: 4s - loss: 0.3095 - accuracy: 0.9400\n",
            "298/469 [==================>...........] - ETA: 4s - loss: 0.3095 - accuracy: 0.9401\n",
            "300/469 [==================>...........] - ETA: 4s - loss: 0.3104 - accuracy: 0.9401\n",
            "302/469 [==================>...........] - ETA: 4s - loss: 0.3103 - accuracy: 0.9401\n",
            "306/469 [==================>...........] - ETA: 4s - loss: 0.3103 - accuracy: 0.9401\n",
            "310/469 [==================>...........] - ETA: 4s - loss: 0.3106 - accuracy: 0.9401\n",
            "314/469 [===================>..........] - ETA: 4s - loss: 0.3120 - accuracy: 0.9400\n",
            "316/469 [===================>..........] - ETA: 4s - loss: 0.3122 - accuracy: 0.9400\n",
            "318/469 [===================>..........] - ETA: 4s - loss: 0.3124 - accuracy: 0.9400\n",
            "320/469 [===================>..........] - ETA: 3s - loss: 0.3127 - accuracy: 0.9400\n",
            "325/469 [===================>..........] - ETA: 3s - loss: 0.3111 - accuracy: 0.9401\n",
            "329/469 [====================>.........] - ETA: 3s - loss: 0.3129 - accuracy: 0.9400\n",
            "333/469 [====================>.........] - ETA: 3s - loss: 0.3108 - accuracy: 0.9402\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.3102 - accuracy: 0.9404\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 0.3089 - accuracy: 0.9405\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.3084 - accuracy: 0.9405\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 0.3077 - accuracy: 0.9403\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.3072 - accuracy: 0.9404\n",
            "356/469 [=====================>........] - ETA: 3s - loss: 0.3052 - accuracy: 0.9406\n",
            "360/469 [======================>.......] - ETA: 2s - loss: 0.3060 - accuracy: 0.9405\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.3079 - accuracy: 0.9406\n",
            "368/469 [======================>.......] - ETA: 2s - loss: 0.3071 - accuracy: 0.9407\n",
            "372/469 [======================>.......] - ETA: 2s - loss: 0.3078 - accuracy: 0.9407\n",
            "376/469 [=======================>......] - ETA: 2s - loss: 0.3094 - accuracy: 0.9404\n",
            "380/469 [=======================>......] - ETA: 2s - loss: 0.3115 - accuracy: 0.9401\n",
            "382/469 [=======================>......] - ETA: 2s - loss: 0.3116 - accuracy: 0.9401\n",
            "386/469 [=======================>......] - ETA: 2s - loss: 0.3123 - accuracy: 0.9399\n",
            "388/469 [=======================>......] - ETA: 2s - loss: 0.3120 - accuracy: 0.9399\n",
            "390/469 [=======================>......] - ETA: 2s - loss: 0.3138 - accuracy: 0.9398\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.3154 - accuracy: 0.9393\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 0.3193 - accuracy: 0.9392\n",
            "402/469 [========================>.....] - ETA: 1s - loss: 0.3199 - accuracy: 0.9390\n",
            "404/469 [========================>.....] - ETA: 1s - loss: 0.3194 - accuracy: 0.9392\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.3191 - accuracy: 0.9393\n",
            "408/469 [=========================>....] - ETA: 1s - loss: 0.3190 - accuracy: 0.9393\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.3181 - accuracy: 0.9393\n",
            "416/469 [=========================>....] - ETA: 1s - loss: 0.3172 - accuracy: 0.9394\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 0.3168 - accuracy: 0.9396\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.3179 - accuracy: 0.9395\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.3176 - accuracy: 0.9395\n",
            "432/469 [==========================>...] - ETA: 0s - loss: 0.3179 - accuracy: 0.9392\n",
            "434/469 [==========================>...] - ETA: 0s - loss: 0.3171 - accuracy: 0.9393\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.3172 - accuracy: 0.9393\n",
            "440/469 [===========================>..] - ETA: 0s - loss: 0.3183 - accuracy: 0.9393\n",
            "444/469 [===========================>..] - ETA: 0s - loss: 0.3174 - accuracy: 0.9395\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.3174 - accuracy: 0.9395\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.3225 - accuracy: 0.9393\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.9393\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.3228 - accuracy: 0.9393\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.9391\n",
            "462/469 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.9390\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.9388\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.9388\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.3248 - accuracy: 0.9388 - val_loss: 0.4435 - val_accuracy: 0.9388\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 10/12\n",
            "  1/469 [..............................] - ETA: 13s - loss: 0.2147 - accuracy: 0.9609\n",
            "  5/469 [..............................] - ETA: 12s - loss: 0.3048 - accuracy: 0.9453\n",
            "  9/469 [..............................] - ETA: 12s - loss: 0.2976 - accuracy: 0.9366\n",
            " 13/469 [..............................] - ETA: 12s - loss: 0.2463 - accuracy: 0.9429\n",
            " 17/469 [>.............................] - ETA: 12s - loss: 0.2592 - accuracy: 0.9407\n",
            " 21/469 [>.............................] - ETA: 12s - loss: 0.3210 - accuracy: 0.9416\n",
            " 25/469 [>.............................] - ETA: 11s - loss: 0.3301 - accuracy: 0.9406\n",
            " 27/469 [>.............................] - ETA: 11s - loss: 0.3235 - accuracy: 0.9407\n",
            " 32/469 [=>............................] - ETA: 11s - loss: 0.3460 - accuracy: 0.9385\n",
            " 36/469 [=>............................] - ETA: 11s - loss: 0.3471 - accuracy: 0.9355\n",
            " 41/469 [=>............................] - ETA: 11s - loss: 0.3419 - accuracy: 0.9343\n",
            " 43/469 [=>............................] - ETA: 11s - loss: 0.3372 - accuracy: 0.9331\n",
            " 48/469 [==>...........................] - ETA: 11s - loss: 0.3310 - accuracy: 0.9331\n",
            " 52/469 [==>...........................] - ETA: 11s - loss: 0.3366 - accuracy: 0.9327\n",
            " 56/469 [==>...........................] - ETA: 11s - loss: 0.3453 - accuracy: 0.9321\n",
            " 60/469 [==>...........................] - ETA: 11s - loss: 0.3453 - accuracy: 0.9335\n",
            " 64/469 [===>..........................] - ETA: 10s - loss: 0.3474 - accuracy: 0.9337\n",
            " 69/469 [===>..........................] - ETA: 10s - loss: 0.3573 - accuracy: 0.9334\n",
            " 71/469 [===>..........................] - ETA: 10s - loss: 0.3577 - accuracy: 0.9328\n",
            " 76/469 [===>..........................] - ETA: 10s - loss: 0.3550 - accuracy: 0.9327\n",
            " 80/469 [====>.........................] - ETA: 10s - loss: 0.3568 - accuracy: 0.9319\n",
            " 84/469 [====>.........................] - ETA: 10s - loss: 0.3554 - accuracy: 0.9317\n",
            " 88/469 [====>.........................] - ETA: 10s - loss: 0.3571 - accuracy: 0.9324\n",
            " 92/469 [====>.........................] - ETA: 10s - loss: 0.3637 - accuracy: 0.9328\n",
            " 96/469 [=====>........................] - ETA: 10s - loss: 0.3592 - accuracy: 0.9333\n",
            "100/469 [=====>........................] - ETA: 9s - loss: 0.3590 - accuracy: 0.9336\n",
            "104/469 [=====>........................] - ETA: 9s - loss: 0.3677 - accuracy: 0.9322\n",
            "108/469 [=====>........................] - ETA: 9s - loss: 0.3646 - accuracy: 0.9314\n",
            "113/469 [======>.......................] - ETA: 9s - loss: 0.3714 - accuracy: 0.9307\n",
            "116/469 [======>.......................] - ETA: 9s - loss: 0.3741 - accuracy: 0.9304\n",
            "120/469 [======>.......................] - ETA: 9s - loss: 0.3837 - accuracy: 0.9298\n",
            "124/469 [======>.......................] - ETA: 9s - loss: 0.4058 - accuracy: 0.9287\n",
            "128/469 [=======>......................] - ETA: 9s - loss: 0.4060 - accuracy: 0.9285\n",
            "132/469 [=======>......................] - ETA: 9s - loss: 0.4178 - accuracy: 0.9283\n",
            "136/469 [=======>......................] - ETA: 8s - loss: 0.4167 - accuracy: 0.9283\n",
            "138/469 [=======>......................] - ETA: 8s - loss: 0.4211 - accuracy: 0.9279\n",
            "142/469 [========>.....................] - ETA: 8s - loss: 0.4176 - accuracy: 0.9280\n",
            "146/469 [========>.....................] - ETA: 8s - loss: 0.4168 - accuracy: 0.9274\n",
            "150/469 [========>.....................] - ETA: 8s - loss: 0.4234 - accuracy: 0.9274\n",
            "155/469 [========>.....................] - ETA: 8s - loss: 0.4298 - accuracy: 0.9274\n",
            "159/469 [=========>....................] - ETA: 8s - loss: 0.4297 - accuracy: 0.9270\n",
            "163/469 [=========>....................] - ETA: 8s - loss: 0.4296 - accuracy: 0.9266\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 0.4277 - accuracy: 0.9266\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 0.4256 - accuracy: 0.9270\n",
            "175/469 [==========>...................] - ETA: 7s - loss: 0.4268 - accuracy: 0.9270\n",
            "179/469 [==========>...................] - ETA: 7s - loss: 0.4227 - accuracy: 0.9276\n",
            "183/469 [==========>...................] - ETA: 7s - loss: 0.4241 - accuracy: 0.9276\n",
            "188/469 [===========>..................] - ETA: 7s - loss: 0.4225 - accuracy: 0.9272\n",
            "190/469 [===========>..................] - ETA: 7s - loss: 0.4216 - accuracy: 0.9271\n",
            "195/469 [===========>..................] - ETA: 7s - loss: 0.4181 - accuracy: 0.9276\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 0.4164 - accuracy: 0.9277\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 0.4162 - accuracy: 0.9275\n",
            "207/469 [============>.................] - ETA: 7s - loss: 0.4168 - accuracy: 0.9274\n",
            "211/469 [============>.................] - ETA: 6s - loss: 0.4156 - accuracy: 0.9277\n",
            "215/469 [============>.................] - ETA: 6s - loss: 0.4172 - accuracy: 0.9273\n",
            "220/469 [=============>................] - ETA: 6s - loss: 0.4158 - accuracy: 0.9271\n",
            "225/469 [=============>................] - ETA: 6s - loss: 0.4162 - accuracy: 0.9266\n",
            "229/469 [=============>................] - ETA: 6s - loss: 0.4174 - accuracy: 0.9263\n",
            "233/469 [=============>................] - ETA: 6s - loss: 0.4170 - accuracy: 0.9259\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.4140 - accuracy: 0.9261\n",
            "240/469 [==============>...............] - ETA: 6s - loss: 0.4116 - accuracy: 0.9261\n",
            "244/469 [==============>...............] - ETA: 6s - loss: 0.4110 - accuracy: 0.9262\n",
            "248/469 [==============>...............] - ETA: 5s - loss: 0.4085 - accuracy: 0.9263\n",
            "252/469 [===============>..............] - ETA: 5s - loss: 0.4070 - accuracy: 0.9266\n",
            "256/469 [===============>..............] - ETA: 5s - loss: 0.4055 - accuracy: 0.9267\n",
            "261/469 [===============>..............] - ETA: 5s - loss: 0.4053 - accuracy: 0.9266\n",
            "265/469 [===============>..............] - ETA: 5s - loss: 0.4051 - accuracy: 0.9267\n",
            "269/469 [================>.............] - ETA: 5s - loss: 0.4036 - accuracy: 0.9267\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.4072 - accuracy: 0.9265\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.4070 - accuracy: 0.9266\n",
            "279/469 [================>.............] - ETA: 5s - loss: 0.4070 - accuracy: 0.9266\n",
            "282/469 [=================>............] - ETA: 4s - loss: 0.4089 - accuracy: 0.9266\n",
            "286/469 [=================>............] - ETA: 4s - loss: 0.4088 - accuracy: 0.9266\n",
            "290/469 [=================>............] - ETA: 4s - loss: 0.4070 - accuracy: 0.9268\n",
            "294/469 [=================>............] - ETA: 4s - loss: 0.4063 - accuracy: 0.9269\n",
            "298/469 [==================>...........] - ETA: 4s - loss: 0.4076 - accuracy: 0.9268\n",
            "302/469 [==================>...........] - ETA: 4s - loss: 0.4064 - accuracy: 0.9269\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 0.4045 - accuracy: 0.9269\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.4052 - accuracy: 0.9269\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.4038 - accuracy: 0.9270\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.4025 - accuracy: 0.9269\n",
            "321/469 [===================>..........] - ETA: 3s - loss: 0.4011 - accuracy: 0.9267\n",
            "325/469 [===================>..........] - ETA: 3s - loss: 0.3987 - accuracy: 0.9269\n",
            "329/469 [====================>.........] - ETA: 3s - loss: 0.3987 - accuracy: 0.9271\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 0.3962 - accuracy: 0.9273\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.3962 - accuracy: 0.9274\n",
            "342/469 [====================>.........] - ETA: 3s - loss: 0.3962 - accuracy: 0.9275\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.3965 - accuracy: 0.9276\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.3959 - accuracy: 0.9277\n",
            "354/469 [=====================>........] - ETA: 3s - loss: 0.3973 - accuracy: 0.9275\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.3981 - accuracy: 0.9273\n",
            "362/469 [======================>.......] - ETA: 2s - loss: 0.3974 - accuracy: 0.9273\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 0.3971 - accuracy: 0.9272\n",
            "370/469 [======================>.......] - ETA: 2s - loss: 0.3958 - accuracy: 0.9273\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.3962 - accuracy: 0.9273\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.3935 - accuracy: 0.9276\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.3939 - accuracy: 0.9276\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.3922 - accuracy: 0.9277\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.3929 - accuracy: 0.9276\n",
            "395/469 [========================>.....] - ETA: 1s - loss: 0.3933 - accuracy: 0.9277\n",
            "399/469 [========================>.....] - ETA: 1s - loss: 0.3926 - accuracy: 0.9276\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.3923 - accuracy: 0.9277\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 0.3908 - accuracy: 0.9277\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 0.3903 - accuracy: 0.9276\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.3928 - accuracy: 0.9275\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.3924 - accuracy: 0.9274\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.3938 - accuracy: 0.9272\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.3937 - accuracy: 0.9269\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 0.3925 - accuracy: 0.9269\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.3920 - accuracy: 0.9269\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.3913 - accuracy: 0.9269\n",
            "444/469 [===========================>..] - ETA: 0s - loss: 0.3888 - accuracy: 0.9271\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.3883 - accuracy: 0.9271\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.3875 - accuracy: 0.9271\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.9272\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.3884 - accuracy: 0.9274\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.3867 - accuracy: 0.9275\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.3853 - accuracy: 0.9277\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.3847 - accuracy: 0.9277 - val_loss: 0.4870 - val_accuracy: 0.9294\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 11/12\n",
            "  3/469 [..............................] - ETA: 12s - loss: 0.2121 - accuracy: 0.9557\n",
            "  7/469 [..............................] - ETA: 12s - loss: 0.2438 - accuracy: 0.9453\n",
            " 11/469 [..............................] - ETA: 12s - loss: 0.2430 - accuracy: 0.9432\n",
            " 16/469 [>.............................] - ETA: 11s - loss: 0.2982 - accuracy: 0.9360\n",
            " 20/469 [>.............................] - ETA: 11s - loss: 0.3214 - accuracy: 0.9332\n",
            " 22/469 [>.............................] - ETA: 11s - loss: 0.3230 - accuracy: 0.9315\n",
            " 27/469 [>.............................] - ETA: 11s - loss: 0.3014 - accuracy: 0.9329\n",
            " 31/469 [>.............................] - ETA: 11s - loss: 0.3164 - accuracy: 0.9325\n",
            " 35/469 [=>............................] - ETA: 11s - loss: 0.3161 - accuracy: 0.9337\n",
            " 39/469 [=>............................] - ETA: 11s - loss: 0.3097 - accuracy: 0.9319\n",
            " 43/469 [=>............................] - ETA: 11s - loss: 0.3353 - accuracy: 0.9310\n",
            " 48/469 [==>...........................] - ETA: 11s - loss: 0.3405 - accuracy: 0.9307\n",
            " 50/469 [==>...........................] - ETA: 11s - loss: 0.3365 - accuracy: 0.9311\n",
            " 55/469 [==>...........................] - ETA: 10s - loss: 0.3530 - accuracy: 0.9283\n",
            " 60/469 [==>...........................] - ETA: 10s - loss: 0.3484 - accuracy: 0.9290\n",
            " 64/469 [===>..........................] - ETA: 10s - loss: 0.3556 - accuracy: 0.9290\n",
            " 68/469 [===>..........................] - ETA: 10s - loss: 0.3498 - accuracy: 0.9288\n",
            " 71/469 [===>..........................] - ETA: 10s - loss: 0.3458 - accuracy: 0.9284\n",
            " 75/469 [===>..........................] - ETA: 10s - loss: 0.3548 - accuracy: 0.9265\n",
            " 79/469 [====>.........................] - ETA: 10s - loss: 0.3635 - accuracy: 0.9257\n",
            " 83/469 [====>.........................] - ETA: 10s - loss: 0.3832 - accuracy: 0.9239\n",
            " 87/469 [====>.........................] - ETA: 10s - loss: 0.3872 - accuracy: 0.9223\n",
            " 92/469 [====>.........................] - ETA: 9s - loss: 0.3823 - accuracy: 0.9222\n",
            " 96/469 [=====>........................] - ETA: 9s - loss: 0.4080 - accuracy: 0.9205\n",
            "100/469 [=====>........................] - ETA: 9s - loss: 0.4141 - accuracy: 0.9207\n",
            "105/469 [=====>........................] - ETA: 9s - loss: 0.4263 - accuracy: 0.9201\n",
            "107/469 [=====>........................] - ETA: 9s - loss: 0.4238 - accuracy: 0.9200\n",
            "109/469 [=====>........................] - ETA: 9s - loss: 0.4222 - accuracy: 0.9199\n",
            "111/469 [======>.......................] - ETA: 9s - loss: 0.4259 - accuracy: 0.9188\n",
            "113/469 [======>.......................] - ETA: 9s - loss: 0.4339 - accuracy: 0.9174\n",
            "115/469 [======>.......................] - ETA: 9s - loss: 0.4351 - accuracy: 0.9161\n",
            "117/469 [======>.......................] - ETA: 9s - loss: 0.4359 - accuracy: 0.9158\n",
            "121/469 [======>.......................] - ETA: 9s - loss: 0.4349 - accuracy: 0.9159\n",
            "124/469 [======>.......................] - ETA: 9s - loss: 0.4438 - accuracy: 0.9151\n",
            "129/469 [=======>......................] - ETA: 8s - loss: 0.4505 - accuracy: 0.9140\n",
            "133/469 [=======>......................] - ETA: 8s - loss: 0.4535 - accuracy: 0.9137\n",
            "136/469 [=======>......................] - ETA: 8s - loss: 0.4543 - accuracy: 0.9138\n",
            "140/469 [=======>......................] - ETA: 8s - loss: 0.4590 - accuracy: 0.9133\n",
            "144/469 [========>.....................] - ETA: 8s - loss: 0.4737 - accuracy: 0.9138\n",
            "148/469 [========>.....................] - ETA: 8s - loss: 0.4826 - accuracy: 0.9141\n",
            "152/469 [========>.....................] - ETA: 8s - loss: 0.4885 - accuracy: 0.9141\n",
            "156/469 [========>.....................] - ETA: 8s - loss: 0.4956 - accuracy: 0.9137\n",
            "160/469 [=========>....................] - ETA: 8s - loss: 0.4986 - accuracy: 0.9125\n",
            "165/469 [=========>....................] - ETA: 7s - loss: 0.5030 - accuracy: 0.9123\n",
            "170/469 [=========>....................] - ETA: 7s - loss: 0.5150 - accuracy: 0.9119\n",
            "174/469 [==========>...................] - ETA: 7s - loss: 0.5228 - accuracy: 0.9118\n",
            "176/469 [==========>...................] - ETA: 7s - loss: 0.5228 - accuracy: 0.9116\n",
            "178/469 [==========>...................] - ETA: 7s - loss: 0.5255 - accuracy: 0.9113\n",
            "180/469 [==========>...................] - ETA: 7s - loss: 0.5267 - accuracy: 0.9112\n",
            "182/469 [==========>...................] - ETA: 7s - loss: 0.5283 - accuracy: 0.9113\n",
            "186/469 [==========>...................] - ETA: 7s - loss: 0.5294 - accuracy: 0.9111\n",
            "190/469 [===========>..................] - ETA: 7s - loss: 0.5251 - accuracy: 0.9112\n",
            "194/469 [===========>..................] - ETA: 7s - loss: 0.5261 - accuracy: 0.9109\n",
            "196/469 [===========>..................] - ETA: 7s - loss: 0.5258 - accuracy: 0.9110\n",
            "202/469 [===========>..................] - ETA: 7s - loss: 0.5227 - accuracy: 0.9106\n",
            "204/469 [============>.................] - ETA: 6s - loss: 0.5204 - accuracy: 0.9107\n",
            "209/469 [============>.................] - ETA: 6s - loss: 0.5233 - accuracy: 0.9101\n",
            "213/469 [============>.................] - ETA: 6s - loss: 0.5286 - accuracy: 0.9098\n",
            "218/469 [============>.................] - ETA: 6s - loss: 0.5298 - accuracy: 0.9091\n",
            "222/469 [=============>................] - ETA: 6s - loss: 0.5390 - accuracy: 0.9088\n",
            "226/469 [=============>................] - ETA: 6s - loss: 0.5375 - accuracy: 0.9084\n",
            "230/469 [=============>................] - ETA: 6s - loss: 0.5362 - accuracy: 0.9076\n",
            "234/469 [=============>................] - ETA: 6s - loss: 0.5442 - accuracy: 0.9054\n",
            "238/469 [==============>...............] - ETA: 6s - loss: 0.5520 - accuracy: 0.9035\n",
            "242/469 [==============>...............] - ETA: 5s - loss: 0.5518 - accuracy: 0.9026\n",
            "246/469 [==============>...............] - ETA: 5s - loss: 0.5557 - accuracy: 0.9021\n",
            "250/469 [==============>...............] - ETA: 5s - loss: 0.5558 - accuracy: 0.9020\n",
            "254/469 [===============>..............] - ETA: 5s - loss: 0.5639 - accuracy: 0.9016\n",
            "258/469 [===============>..............] - ETA: 5s - loss: 0.5643 - accuracy: 0.9011\n",
            "262/469 [===============>..............] - ETA: 5s - loss: 0.5659 - accuracy: 0.9005\n",
            "266/469 [================>.............] - ETA: 5s - loss: 0.5714 - accuracy: 0.8993\n",
            "270/469 [================>.............] - ETA: 5s - loss: 0.5700 - accuracy: 0.8992\n",
            "274/469 [================>.............] - ETA: 5s - loss: 0.5717 - accuracy: 0.8984\n",
            "278/469 [================>.............] - ETA: 5s - loss: 0.5727 - accuracy: 0.8981\n",
            "282/469 [=================>............] - ETA: 4s - loss: 0.5792 - accuracy: 0.8977\n",
            "286/469 [=================>............] - ETA: 4s - loss: 0.5799 - accuracy: 0.8968\n",
            "290/469 [=================>............] - ETA: 4s - loss: 0.5803 - accuracy: 0.8960\n",
            "293/469 [=================>............] - ETA: 4s - loss: 0.5800 - accuracy: 0.8951\n",
            "298/469 [==================>...........] - ETA: 4s - loss: 0.5769 - accuracy: 0.8949\n",
            "302/469 [==================>...........] - ETA: 4s - loss: 0.5742 - accuracy: 0.8952\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.5782 - accuracy: 0.8948\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.5900 - accuracy: 0.8949\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.5891 - accuracy: 0.8945\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.5913 - accuracy: 0.8942\n",
            "322/469 [===================>..........] - ETA: 3s - loss: 0.6032 - accuracy: 0.8928\n",
            "326/469 [===================>..........] - ETA: 3s - loss: 0.6106 - accuracy: 0.8905\n",
            "330/469 [====================>.........] - ETA: 3s - loss: 0.6179 - accuracy: 0.8880\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 0.6321 - accuracy: 0.8851\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.6333 - accuracy: 0.8837\n",
            "344/469 [=====================>........] - ETA: 3s - loss: 0.6361 - accuracy: 0.8821\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.6381 - accuracy: 0.8817\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.6371 - accuracy: 0.8812\n",
            "356/469 [=====================>........] - ETA: 2s - loss: 0.6398 - accuracy: 0.8808\n",
            "360/469 [======================>.......] - ETA: 2s - loss: 0.6374 - accuracy: 0.8812\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.6370 - accuracy: 0.8814\n",
            "366/469 [======================>.......] - ETA: 2s - loss: 0.6374 - accuracy: 0.8813\n",
            "368/469 [======================>.......] - ETA: 2s - loss: 0.6379 - accuracy: 0.8811\n",
            "372/469 [======================>.......] - ETA: 2s - loss: 0.6372 - accuracy: 0.8811\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.6362 - accuracy: 0.8812\n",
            "380/469 [=======================>......] - ETA: 2s - loss: 0.6363 - accuracy: 0.8815\n",
            "384/469 [=======================>......] - ETA: 2s - loss: 0.6347 - accuracy: 0.8814\n",
            "388/469 [=======================>......] - ETA: 2s - loss: 0.6337 - accuracy: 0.8813\n",
            "392/469 [========================>.....] - ETA: 2s - loss: 0.6323 - accuracy: 0.8814\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.6315 - accuracy: 0.8814\n",
            "399/469 [========================>.....] - ETA: 1s - loss: 0.6292 - accuracy: 0.8818\n",
            "404/469 [========================>.....] - ETA: 1s - loss: 0.6290 - accuracy: 0.8819\n",
            "408/469 [=========================>....] - ETA: 1s - loss: 0.6285 - accuracy: 0.8820\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.6265 - accuracy: 0.8823\n",
            "416/469 [=========================>....] - ETA: 1s - loss: 0.6254 - accuracy: 0.8827\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.6257 - accuracy: 0.8828\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.6235 - accuracy: 0.8832\n",
            "428/469 [==========================>...] - ETA: 1s - loss: 0.6211 - accuracy: 0.8832\n",
            "432/469 [==========================>...] - ETA: 0s - loss: 0.6204 - accuracy: 0.8833\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.6186 - accuracy: 0.8834\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.6186 - accuracy: 0.8833\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.6174 - accuracy: 0.8832\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.6152 - accuracy: 0.8834\n",
            "452/469 [===========================>..] - ETA: 0s - loss: 0.6128 - accuracy: 0.8835\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.6120 - accuracy: 0.8838\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.6128 - accuracy: 0.8838\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.6121 - accuracy: 0.8838\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.6131 - accuracy: 0.8838\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.6146 - accuracy: 0.8837 - val_loss: 0.6936 - val_accuracy: 0.8828\n",
            "\u001b[36m(train_mnist pid=3297460)\u001b[0m Epoch 12/12\n",
            "  1/469 [..............................] - ETA: 11s - loss: 0.2656 - accuracy: 0.9062\n",
            "  5/469 [..............................] - ETA: 12s - loss: 0.4469 - accuracy: 0.8750\n",
            "  9/469 [..............................] - ETA: 12s - loss: 0.5416 - accuracy: 0.8707\n",
            " 13/469 [..............................] - ETA: 12s - loss: 0.5400 - accuracy: 0.8738\n",
            " 17/469 [>.............................] - ETA: 12s - loss: 0.4773 - accuracy: 0.8869\n",
            " 19/469 [>.............................] - ETA: 11s - loss: 0.4815 - accuracy: 0.8882\n",
            " 24/469 [>.............................] - ETA: 11s - loss: 0.4857 - accuracy: 0.8906\n",
            " 28/469 [>.............................] - ETA: 11s - loss: 0.4841 - accuracy: 0.8931\n",
            " 32/469 [=>............................] - ETA: 11s - loss: 0.4756 - accuracy: 0.8965\n",
            " 37/469 [=>............................] - ETA: 11s - loss: 0.4675 - accuracy: 0.8932\n",
            " 41/469 [=>............................] - ETA: 11s - loss: 0.4609 - accuracy: 0.8923\n",
            " 45/469 [=>............................] - ETA: 11s - loss: 0.4643 - accuracy: 0.8929\n",
            " 48/469 [==>...........................] - ETA: 11s - loss: 0.4642 - accuracy: 0.8937\n",
            " 50/469 [==>...........................] - ETA: 10s - loss: 0.4592 - accuracy: 0.8941\n",
            " 54/469 [==>...........................] - ETA: 10s - loss: 0.4502 - accuracy: 0.8947\n",
            " 56/469 [==>...........................] - ETA: 10s - loss: 0.4504 - accuracy: 0.8951\n",
            " 58/469 [==>...........................] - ETA: 10s - loss: 0.4454 - accuracy: 0.8947\n",
            " 60/469 [==>...........................] - ETA: 10s - loss: 0.4486 - accuracy: 0.8936\n",
            " 62/469 [==>...........................] - ETA: 10s - loss: 0.4442 - accuracy: 0.8944\n",
            " 64/469 [===>..........................] - ETA: 10s - loss: 0.4407 - accuracy: 0.8947\n",
            " 66/469 [===>..........................] - ETA: 10s - loss: 0.4582 - accuracy: 0.8943\n",
            " 70/469 [===>..........................] - ETA: 10s - loss: 0.4513 - accuracy: 0.8952\n",
            " 74/469 [===>..........................] - ETA: 10s - loss: 0.4452 - accuracy: 0.8964\n",
            " 76/469 [===>..........................] - ETA: 10s - loss: 0.4403 - accuracy: 0.8972\n",
            " 78/469 [===>..........................] - ETA: 10s - loss: 0.4432 - accuracy: 0.8983\n",
            " 81/469 [====>.........................] - ETA: 10s - loss: 0.4610 - accuracy: 0.8990\n",
            " 86/469 [====>.........................] - ETA: 10s - loss: 0.4491 - accuracy: 0.9002\n",
            " 89/469 [====>.........................] - ETA: 10s - loss: 0.4512 - accuracy: 0.8999\n",
            " 93/469 [====>.........................] - ETA: 9s - loss: 0.4529 - accuracy: 0.8982\n",
            " 97/469 [=====>........................] - ETA: 9s - loss: 0.4619 - accuracy: 0.8982\n",
            "101/469 [=====>........................] - ETA: 9s - loss: 0.4573 - accuracy: 0.8987\n",
            "105/469 [=====>........................] - ETA: 9s - loss: 0.4621 - accuracy: 0.8973\n",
            "109/469 [=====>........................] - ETA: 9s - loss: 0.4683 - accuracy: 0.8962\n",
            "114/469 [======>.......................] - ETA: 9s - loss: 0.4698 - accuracy: 0.8950\n",
            "118/469 [======>.......................] - ETA: 9s - loss: 0.4721 - accuracy: 0.8935\n",
            "123/469 [======>.......................] - ETA: 9s - loss: 0.4732 - accuracy: 0.8928\n",
            "126/469 [=======>......................] - ETA: 9s - loss: 0.4731 - accuracy: 0.8925\n",
            "131/469 [=======>......................] - ETA: 8s - loss: 0.4801 - accuracy: 0.8929\n",
            "135/469 [=======>......................] - ETA: 8s - loss: 0.4788 - accuracy: 0.8925\n",
            "139/469 [=======>......................] - ETA: 8s - loss: 0.4860 - accuracy: 0.8932\n",
            "143/469 [========>.....................] - ETA: 8s - loss: 0.4816 - accuracy: 0.8940\n",
            "145/469 [========>.....................] - ETA: 8s - loss: 0.4877 - accuracy: 0.8943\n",
            "150/469 [========>.....................] - ETA: 8s - loss: 0.4971 - accuracy: 0.8942\n",
            "154/469 [========>.....................] - ETA: 8s - loss: 0.5016 - accuracy: 0.8936\n",
            "158/469 [=========>....................] - ETA: 8s - loss: 0.4995 - accuracy: 0.8934\n",
            "163/469 [=========>....................] - ETA: 8s - loss: 0.4979 - accuracy: 0.8940\n",
            "166/469 [=========>....................] - ETA: 7s - loss: 0.5042 - accuracy: 0.8940\n",
            "168/469 [=========>....................] - ETA: 7s - loss: 0.5036 - accuracy: 0.8943\n",
            "171/469 [=========>....................] - ETA: 7s - loss: 0.5063 - accuracy: 0.8945\n",
            "175/469 [==========>...................] - ETA: 7s - loss: 0.5077 - accuracy: 0.8946\n",
            "179/469 [==========>...................] - ETA: 7s - loss: 0.5060 - accuracy: 0.8947\n",
            "183/469 [==========>...................] - ETA: 7s - loss: 0.5043 - accuracy: 0.8946\n",
            "187/469 [==========>...................] - ETA: 7s - loss: 0.5026 - accuracy: 0.8948\n",
            "191/469 [===========>..................] - ETA: 7s - loss: 0.5004 - accuracy: 0.8944\n",
            "195/469 [===========>..................] - ETA: 7s - loss: 0.4989 - accuracy: 0.8953\n",
            "199/469 [===========>..................] - ETA: 7s - loss: 0.5020 - accuracy: 0.8948\n",
            "204/469 [============>.................] - ETA: 6s - loss: 0.5009 - accuracy: 0.8950\n",
            "208/469 [============>.................] - ETA: 6s - loss: 0.4992 - accuracy: 0.8949\n",
            "212/469 [============>.................] - ETA: 6s - loss: 0.4973 - accuracy: 0.8942\n",
            "215/469 [============>.................] - ETA: 6s - loss: 0.4937 - accuracy: 0.8946\n",
            "220/469 [=============>................] - ETA: 6s - loss: 0.4934 - accuracy: 0.8943\n",
            "224/469 [=============>................] - ETA: 6s - loss: 0.4912 - accuracy: 0.8945\n",
            "228/469 [=============>................] - ETA: 6s - loss: 0.4911 - accuracy: 0.8945\n",
            "232/469 [=============>................] - ETA: 6s - loss: 0.4908 - accuracy: 0.8946\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.4897 - accuracy: 0.8949\n",
            "241/469 [==============>...............] - ETA: 5s - loss: 0.4870 - accuracy: 0.8954\n",
            "245/469 [==============>...............] - ETA: 5s - loss: 0.4846 - accuracy: 0.8956\n",
            "247/469 [==============>...............] - ETA: 5s - loss: 0.4837 - accuracy: 0.8957\n",
            "249/469 [==============>...............] - ETA: 5s - loss: 0.4829 - accuracy: 0.8961\n",
            "251/469 [===============>..............] - ETA: 5s - loss: 0.4805 - accuracy: 0.8963\n",
            "253/469 [===============>..............] - ETA: 5s - loss: 0.4794 - accuracy: 0.8966\n",
            "256/469 [===============>..............] - ETA: 5s - loss: 0.4791 - accuracy: 0.8966\n",
            "261/469 [===============>..............] - ETA: 5s - loss: 0.4823 - accuracy: 0.8968\n",
            "265/469 [===============>..............] - ETA: 5s - loss: 0.4785 - accuracy: 0.8972\n",
            "268/469 [================>.............] - ETA: 5s - loss: 0.4808 - accuracy: 0.8972\n",
            "272/469 [================>.............] - ETA: 5s - loss: 0.4815 - accuracy: 0.8968\n",
            "276/469 [================>.............] - ETA: 5s - loss: 0.4814 - accuracy: 0.8965\n",
            "280/469 [================>.............] - ETA: 4s - loss: 0.4816 - accuracy: 0.8965\n",
            "284/469 [=================>............] - ETA: 4s - loss: 0.4850 - accuracy: 0.8961\n",
            "288/469 [=================>............] - ETA: 4s - loss: 0.4852 - accuracy: 0.8966\n",
            "293/469 [=================>............] - ETA: 4s - loss: 0.4853 - accuracy: 0.8966\n",
            "297/469 [=================>............] - ETA: 4s - loss: 0.4861 - accuracy: 0.8963\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 0.4897 - accuracy: 0.8961\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 0.4898 - accuracy: 0.8956\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.4897 - accuracy: 0.8953\n",
            "312/469 [==================>...........] - ETA: 4s - loss: 0.4966 - accuracy: 0.8940\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.5045 - accuracy: 0.8924\n",
            "321/469 [===================>..........] - ETA: 3s - loss: 0.5127 - accuracy: 0.8898\n",
            "323/469 [===================>..........] - ETA: 3s - loss: 0.5207 - accuracy: 0.8881\n",
            "328/469 [===================>..........] - ETA: 3s - loss: 0.5246 - accuracy: 0.8860\n",
            "332/469 [====================>.........] - ETA: 3s - loss: 0.5348 - accuracy: 0.8846\n",
            "336/469 [====================>.........] - ETA: 3s - loss: 0.5366 - accuracy: 0.8833\n",
            "340/469 [====================>.........] - ETA: 3s - loss: 0.5418 - accuracy: 0.8815\n",
            "344/469 [=====================>........] - ETA: 3s - loss: 0.5487 - accuracy: 0.8792\n",
            "348/469 [=====================>........] - ETA: 3s - loss: 0.5534 - accuracy: 0.8771\n",
            "352/469 [=====================>........] - ETA: 3s - loss: 0.5563 - accuracy: 0.8750\n",
            "356/469 [=====================>........] - ETA: 2s - loss: 0.5594 - accuracy: 0.8742\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.5632 - accuracy: 0.8739\n",
            "360/469 [======================>.......] - ETA: 2s - loss: 0.5641 - accuracy: 0.8737\n",
            "365/469 [======================>.......] - ETA: 2s - loss: 0.5707 - accuracy: 0.8731\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.5747 - accuracy: 0.8725\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 0.5826 - accuracy: 0.8721\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 0.5878 - accuracy: 0.8722\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 0.5878 - accuracy: 0.8719\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 0.5890 - accuracy: 0.8717\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.5922 - accuracy: 0.8714\n",
            "390/469 [=======================>......] - ETA: 2s - loss: 0.5928 - accuracy: 0.8710\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.6027 - accuracy: 0.8703\n",
            "398/469 [========================>.....] - ETA: 1s - loss: 0.6076 - accuracy: 0.8696\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.6082 - accuracy: 0.8692\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.6080 - accuracy: 0.8690\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.6077 - accuracy: 0.8687\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 0.6078 - accuracy: 0.8685\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.6070 - accuracy: 0.8684\n",
            "420/469 [=========================>....] - ETA: 1s - loss: 0.6079 - accuracy: 0.8681\n",
            "424/469 [==========================>...] - ETA: 1s - loss: 0.6080 - accuracy: 0.8679\n",
            "426/469 [==========================>...] - ETA: 1s - loss: 0.6080 - accuracy: 0.8677\n",
            "430/469 [==========================>...] - ETA: 1s - loss: 0.6077 - accuracy: 0.8678\n",
            "434/469 [==========================>...] - ETA: 0s - loss: 0.6068 - accuracy: 0.8678\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.6091 - accuracy: 0.8678\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.6086 - accuracy: 0.8677\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.6067 - accuracy: 0.8676\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.6072 - accuracy: 0.8675\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.6077 - accuracy: 0.8675\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.6057 - accuracy: 0.8676\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.6047 - accuracy: 0.8676\n",
            "462/469 [============================>.] - ETA: 0s - loss: 0.6063 - accuracy: 0.8675\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.6059 - accuracy: 0.8677\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.6064 - accuracy: 0.8675\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.6062 - accuracy: 0.8675 - val_loss: 0.9355 - val_accuracy: 0.8620\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3298267)\u001b[0m 2023-12-05 01:46:57.960253: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3298267)\u001b[0m 2023-12-05 01:46:57.962491: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3298267)\u001b[0m 2023-12-05 01:46:58.007638: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3298267)\u001b[0m 2023-12-05 01:46:58.008125: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3298267)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3298267)\u001b[0m 2023-12-05 01:46:58.888965: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3298267)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3298267)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3298267)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3298267)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3298267)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3298267)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3298267)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3298267)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m 2023-12-05 01:47:00.458540: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 1/12\n",
            "  4/235 [..............................] - ETA: 5s - loss: 4.4735 - accuracy: 0.1787  \n",
            "  9/235 [>.............................] - ETA: 5s - loss: 3.0384 - accuracy: 0.3351\n",
            " 11/235 [>.............................] - ETA: 5s - loss: 2.6836 - accuracy: 0.3995\n",
            " 17/235 [=>............................] - ETA: 5s - loss: 1.9486 - accuracy: 0.5441\n",
            " 20/235 [=>............................] - ETA: 5s - loss: 1.7158 - accuracy: 0.5928\n",
            " 26/235 [==>...........................] - ETA: 5s - loss: 1.3952 - accuracy: 0.6663\n",
            " 30/235 [==>...........................] - ETA: 5s - loss: 1.2480 - accuracy: 0.6986\n",
            " 35/235 [===>..........................] - ETA: 4s - loss: 1.1066 - accuracy: 0.7316\n",
            " 37/235 [===>..........................] - ETA: 4s - loss: 1.0593 - accuracy: 0.7427\n",
            " 43/235 [====>.........................] - ETA: 4s - loss: 0.9418 - accuracy: 0.7703\n",
            " 46/235 [====>.........................] - ETA: 4s - loss: 0.8947 - accuracy: 0.7808\n",
            " 52/235 [=====>........................] - ETA: 4s - loss: 0.8165 - accuracy: 0.7988\n",
            " 57/235 [======>.......................] - ETA: 4s - loss: 0.7617 - accuracy: 0.8113\n",
            " 60/235 [======>.......................] - ETA: 4s - loss: 0.7332 - accuracy: 0.8182\n",
            " 66/235 [=======>......................] - ETA: 4s - loss: 0.6837 - accuracy: 0.8295\n",
            " 71/235 [========>.....................] - ETA: 3s - loss: 0.6470 - accuracy: 0.8380\n",
            " 74/235 [========>.....................] - ETA: 3s - loss: 0.6265 - accuracy: 0.8427\n",
            " 80/235 [=========>....................] - ETA: 3s - loss: 0.5909 - accuracy: 0.8507\n",
            " 83/235 [=========>....................] - ETA: 3s - loss: 0.5756 - accuracy: 0.8544\n",
            " 89/235 [==========>...................] - ETA: 3s - loss: 0.5477 - accuracy: 0.8613\n",
            " 92/235 [==========>...................] - ETA: 3s - loss: 0.5339 - accuracy: 0.8644\n",
            " 94/235 [===========>..................] - ETA: 3s - loss: 0.5249 - accuracy: 0.8666\n",
            " 97/235 [===========>..................] - ETA: 3s - loss: 0.5124 - accuracy: 0.8695\n",
            "102/235 [============>.................] - ETA: 3s - loss: 0.4940 - accuracy: 0.8739\n",
            "105/235 [============>.................] - ETA: 3s - loss: 0.4843 - accuracy: 0.8761\n",
            "111/235 [=============>................] - ETA: 2s - loss: 0.4652 - accuracy: 0.8806\n",
            "116/235 [=============>................] - ETA: 2s - loss: 0.4506 - accuracy: 0.8841\n",
            "118/235 [==============>...............] - ETA: 2s - loss: 0.4455 - accuracy: 0.8853\n",
            "124/235 [==============>...............] - ETA: 2s - loss: 0.4293 - accuracy: 0.8892\n",
            "128/235 [===============>..............] - ETA: 2s - loss: 0.4194 - accuracy: 0.8916\n",
            "131/235 [===============>..............] - ETA: 2s - loss: 0.4123 - accuracy: 0.8933\n",
            "137/235 [================>.............] - ETA: 2s - loss: 0.3990 - accuracy: 0.8967\n",
            "140/235 [================>.............] - ETA: 2s - loss: 0.3927 - accuracy: 0.8980\n",
            "143/235 [=================>............] - ETA: 2s - loss: 0.3870 - accuracy: 0.8994\n",
            "146/235 [=================>............] - ETA: 2s - loss: 0.3808 - accuracy: 0.9009\n",
            "152/235 [==================>...........] - ETA: 1s - loss: 0.3691 - accuracy: 0.9038\n",
            "155/235 [==================>...........] - ETA: 1s - loss: 0.3637 - accuracy: 0.9052\n",
            "160/235 [===================>..........] - ETA: 1s - loss: 0.3559 - accuracy: 0.9071\n",
            "163/235 [===================>..........] - ETA: 1s - loss: 0.3514 - accuracy: 0.9082\n",
            "169/235 [====================>.........] - ETA: 1s - loss: 0.3430 - accuracy: 0.9102\n",
            "175/235 [=====================>........] - ETA: 1s - loss: 0.3349 - accuracy: 0.9121\n",
            "178/235 [=====================>........] - ETA: 1s - loss: 0.3311 - accuracy: 0.9130\n",
            "184/235 [======================>.......] - ETA: 1s - loss: 0.3233 - accuracy: 0.9150\n",
            "187/235 [======================>.......] - ETA: 1s - loss: 0.3194 - accuracy: 0.9160\n",
            "192/235 [=======================>......] - ETA: 1s - loss: 0.3133 - accuracy: 0.9175\n",
            "198/235 [========================>.....] - ETA: 0s - loss: 0.3064 - accuracy: 0.9191\n",
            "201/235 [========================>.....] - ETA: 0s - loss: 0.3029 - accuracy: 0.9199\n",
            "207/235 [=========================>....] - ETA: 0s - loss: 0.2967 - accuracy: 0.9215\n",
            "210/235 [=========================>....] - ETA: 0s - loss: 0.2940 - accuracy: 0.9222\n",
            "216/235 [==========================>...] - ETA: 0s - loss: 0.2890 - accuracy: 0.9235\n",
            "221/235 [===========================>..] - ETA: 0s - loss: 0.2846 - accuracy: 0.9246\n",
            "224/235 [===========================>..] - ETA: 0s - loss: 0.2817 - accuracy: 0.9254\n",
            "230/235 [============================>.] - ETA: 0s - loss: 0.2765 - accuracy: 0.9266\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.9273\n",
            "235/235 [==============================] - 7s 26ms/step - loss: 0.2736 - accuracy: 0.9273 - val_loss: 0.0661 - val_accuracy: 0.9793\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 2/12\n",
            "  1/235 [..............................] - ETA: 5s - loss: 0.1200 - accuracy: 0.9648\n",
            "  6/235 [..............................] - ETA: 5s - loss: 0.0783 - accuracy: 0.9740\n",
            "  9/235 [>.............................] - ETA: 5s - loss: 0.0736 - accuracy: 0.9753\n",
            " 12/235 [>.............................] - ETA: 5s - loss: 0.0763 - accuracy: 0.9756\n",
            " 15/235 [>.............................] - ETA: 5s - loss: 0.0745 - accuracy: 0.9760\n",
            " 21/235 [=>............................] - ETA: 4s - loss: 0.0762 - accuracy: 0.9769\n",
            " 24/235 [==>...........................] - ETA: 4s - loss: 0.0765 - accuracy: 0.9771\n",
            " 26/235 [==>...........................] - ETA: 4s - loss: 0.0785 - accuracy: 0.9767\n",
            " 29/235 [==>...........................] - ETA: 4s - loss: 0.0801 - accuracy: 0.9751\n",
            " 34/235 [===>..........................] - ETA: 4s - loss: 0.0770 - accuracy: 0.9763\n",
            " 38/235 [===>..........................] - ETA: 4s - loss: 0.0765 - accuracy: 0.9768\n",
            " 41/235 [====>.........................] - ETA: 4s - loss: 0.0762 - accuracy: 0.9767\n",
            " 47/235 [=====>........................] - ETA: 4s - loss: 0.0799 - accuracy: 0.9753\n",
            " 52/235 [=====>........................] - ETA: 4s - loss: 0.0793 - accuracy: 0.9755\n",
            " 54/235 [=====>........................] - ETA: 4s - loss: 0.0786 - accuracy: 0.9758\n",
            " 57/235 [======>.......................] - ETA: 4s - loss: 0.0786 - accuracy: 0.9755\n",
            " 59/235 [======>.......................] - ETA: 4s - loss: 0.0776 - accuracy: 0.9758\n",
            " 65/235 [=======>......................] - ETA: 4s - loss: 0.0761 - accuracy: 0.9761\n",
            " 68/235 [=======>......................] - ETA: 3s - loss: 0.0770 - accuracy: 0.9759\n",
            " 74/235 [========>.....................] - ETA: 3s - loss: 0.0773 - accuracy: 0.9760\n",
            " 77/235 [========>.....................] - ETA: 3s - loss: 0.0781 - accuracy: 0.9756\n",
            " 83/235 [=========>....................] - ETA: 3s - loss: 0.0786 - accuracy: 0.9755\n",
            " 86/235 [=========>....................] - ETA: 3s - loss: 0.0781 - accuracy: 0.9757\n",
            " 89/235 [==========>...................] - ETA: 3s - loss: 0.0780 - accuracy: 0.9758\n",
            " 91/235 [==========>...................] - ETA: 3s - loss: 0.0772 - accuracy: 0.9760\n",
            " 97/235 [===========>..................] - ETA: 3s - loss: 0.0768 - accuracy: 0.9762\n",
            "100/235 [===========>..................] - ETA: 3s - loss: 0.0770 - accuracy: 0.9761\n",
            "103/235 [============>.................] - ETA: 3s - loss: 0.0769 - accuracy: 0.9760\n",
            "106/235 [============>.................] - ETA: 3s - loss: 0.0766 - accuracy: 0.9761\n",
            "111/235 [=============>................] - ETA: 2s - loss: 0.0768 - accuracy: 0.9760\n",
            "114/235 [=============>................] - ETA: 2s - loss: 0.0770 - accuracy: 0.9759\n",
            "120/235 [==============>...............] - ETA: 2s - loss: 0.0757 - accuracy: 0.9764\n",
            "123/235 [==============>...............] - ETA: 2s - loss: 0.0755 - accuracy: 0.9764\n",
            "126/235 [===============>..............] - ETA: 2s - loss: 0.0754 - accuracy: 0.9764\n",
            "129/235 [===============>..............] - ETA: 2s - loss: 0.0754 - accuracy: 0.9764\n",
            "134/235 [================>.............] - ETA: 2s - loss: 0.0756 - accuracy: 0.9764\n",
            "139/235 [================>.............] - ETA: 2s - loss: 0.0757 - accuracy: 0.9763\n",
            "144/235 [=================>............] - ETA: 2s - loss: 0.0756 - accuracy: 0.9763\n",
            "147/235 [=================>............] - ETA: 2s - loss: 0.0753 - accuracy: 0.9765\n",
            "153/235 [==================>...........] - ETA: 1s - loss: 0.0752 - accuracy: 0.9765\n",
            "156/235 [==================>...........] - ETA: 1s - loss: 0.0754 - accuracy: 0.9765\n",
            "162/235 [===================>..........] - ETA: 1s - loss: 0.0761 - accuracy: 0.9761\n",
            "167/235 [====================>.........] - ETA: 1s - loss: 0.0765 - accuracy: 0.9760\n",
            "170/235 [====================>.........] - ETA: 1s - loss: 0.0766 - accuracy: 0.9760\n",
            "176/235 [=====================>........] - ETA: 1s - loss: 0.0769 - accuracy: 0.9759\n",
            "179/235 [=====================>........] - ETA: 1s - loss: 0.0768 - accuracy: 0.9760\n",
            "182/235 [======================>.......] - ETA: 1s - loss: 0.0767 - accuracy: 0.9761\n",
            "184/235 [======================>.......] - ETA: 1s - loss: 0.0767 - accuracy: 0.9761\n",
            "190/235 [=======================>......] - ETA: 1s - loss: 0.0766 - accuracy: 0.9762\n",
            "195/235 [=======================>......] - ETA: 0s - loss: 0.0763 - accuracy: 0.9762\n",
            "198/235 [========================>.....] - ETA: 0s - loss: 0.0761 - accuracy: 0.9762\n",
            "204/235 [=========================>....] - ETA: 0s - loss: 0.0762 - accuracy: 0.9762\n",
            "207/235 [=========================>....] - ETA: 0s - loss: 0.0756 - accuracy: 0.9764\n",
            "213/235 [==========================>...] - ETA: 0s - loss: 0.0754 - accuracy: 0.9764\n",
            "216/235 [==========================>...] - ETA: 0s - loss: 0.0753 - accuracy: 0.9765\n",
            "222/235 [===========================>..] - ETA: 0s - loss: 0.0755 - accuracy: 0.9765\n",
            "228/235 [============================>.] - ETA: 0s - loss: 0.0762 - accuracy: 0.9764\n",
            "231/235 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9763\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9764\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0763 - accuracy: 0.9765 - val_loss: 0.0712 - val_accuracy: 0.9747\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 3/12\n",
            "  1/235 [..............................] - ETA: 5s - loss: 0.0649 - accuracy: 0.9883\n",
            "  4/235 [..............................] - ETA: 5s - loss: 0.0619 - accuracy: 0.9824\n",
            "  6/235 [..............................] - ETA: 5s - loss: 0.0603 - accuracy: 0.9818\n",
            " 12/235 [>.............................] - ETA: 5s - loss: 0.0566 - accuracy: 0.9824\n",
            " 15/235 [>.............................] - ETA: 5s - loss: 0.0550 - accuracy: 0.9833\n",
            " 18/235 [=>............................] - ETA: 4s - loss: 0.0546 - accuracy: 0.9831\n",
            " 21/235 [=>............................] - ETA: 4s - loss: 0.0521 - accuracy: 0.9836\n",
            " 27/235 [==>...........................] - ETA: 4s - loss: 0.0547 - accuracy: 0.9826\n",
            " 30/235 [==>...........................] - ETA: 4s - loss: 0.0559 - accuracy: 0.9820\n",
            " 36/235 [===>..........................] - ETA: 4s - loss: 0.0573 - accuracy: 0.9818\n",
            " 39/235 [===>..........................] - ETA: 4s - loss: 0.0567 - accuracy: 0.9823\n",
            " 45/235 [====>.........................] - ETA: 4s - loss: 0.0575 - accuracy: 0.9819\n",
            " 51/235 [=====>........................] - ETA: 4s - loss: 0.0590 - accuracy: 0.9812\n",
            " 55/235 [======>.......................] - ETA: 4s - loss: 0.0589 - accuracy: 0.9813\n",
            " 58/235 [======>.......................] - ETA: 4s - loss: 0.0600 - accuracy: 0.9808\n",
            " 64/235 [=======>......................] - ETA: 3s - loss: 0.0598 - accuracy: 0.9807\n",
            " 70/235 [=======>......................] - ETA: 3s - loss: 0.0597 - accuracy: 0.9806\n",
            " 73/235 [========>.....................] - ETA: 3s - loss: 0.0601 - accuracy: 0.9803\n",
            " 79/235 [=========>....................] - ETA: 3s - loss: 0.0586 - accuracy: 0.9806\n",
            " 82/235 [=========>....................] - ETA: 3s - loss: 0.0591 - accuracy: 0.9804\n",
            " 88/235 [==========>...................] - ETA: 3s - loss: 0.0591 - accuracy: 0.9805\n",
            " 94/235 [===========>..................] - ETA: 3s - loss: 0.0579 - accuracy: 0.9808\n",
            " 97/235 [===========>..................] - ETA: 3s - loss: 0.0571 - accuracy: 0.9810\n",
            "103/235 [============>.................] - ETA: 2s - loss: 0.0575 - accuracy: 0.9808\n",
            "106/235 [============>.................] - ETA: 2s - loss: 0.0574 - accuracy: 0.9809\n",
            "109/235 [============>.................] - ETA: 2s - loss: 0.0585 - accuracy: 0.9806\n",
            "112/235 [=============>................] - ETA: 2s - loss: 0.0587 - accuracy: 0.9805\n",
            "118/235 [==============>...............] - ETA: 2s - loss: 0.0582 - accuracy: 0.9807\n",
            "121/235 [==============>...............] - ETA: 2s - loss: 0.0583 - accuracy: 0.9806\n",
            "127/235 [===============>..............] - ETA: 2s - loss: 0.0589 - accuracy: 0.9807\n",
            "130/235 [===============>..............] - ETA: 2s - loss: 0.0587 - accuracy: 0.9808\n",
            "136/235 [================>.............] - ETA: 2s - loss: 0.0584 - accuracy: 0.9810\n",
            "139/235 [================>.............] - ETA: 2s - loss: 0.0587 - accuracy: 0.9809\n",
            "145/235 [=================>............] - ETA: 2s - loss: 0.0580 - accuracy: 0.9809\n",
            "148/235 [=================>............] - ETA: 1s - loss: 0.0576 - accuracy: 0.9810\n",
            "154/235 [==================>...........] - ETA: 1s - loss: 0.0575 - accuracy: 0.9811\n",
            "160/235 [===================>..........] - ETA: 1s - loss: 0.0576 - accuracy: 0.9812\n",
            "163/235 [===================>..........] - ETA: 1s - loss: 0.0577 - accuracy: 0.9810\n",
            "169/235 [====================>.........] - ETA: 1s - loss: 0.0576 - accuracy: 0.9811\n",
            "172/235 [====================>.........] - ETA: 1s - loss: 0.0582 - accuracy: 0.9809\n",
            "178/235 [=====================>........] - ETA: 1s - loss: 0.0589 - accuracy: 0.9809\n",
            "181/235 [======================>.......] - ETA: 1s - loss: 0.0584 - accuracy: 0.9811\n",
            "187/235 [======================>.......] - ETA: 1s - loss: 0.0581 - accuracy: 0.9813\n",
            "193/235 [=======================>......] - ETA: 0s - loss: 0.0581 - accuracy: 0.9814\n",
            "196/235 [========================>.....] - ETA: 0s - loss: 0.0579 - accuracy: 0.9815\n",
            "202/235 [========================>.....] - ETA: 0s - loss: 0.0585 - accuracy: 0.9814\n",
            "205/235 [=========================>....] - ETA: 0s - loss: 0.0587 - accuracy: 0.9814\n",
            "211/235 [=========================>....] - ETA: 0s - loss: 0.0589 - accuracy: 0.9813\n",
            "214/235 [==========================>...] - ETA: 0s - loss: 0.0587 - accuracy: 0.9813\n",
            "220/235 [===========================>..] - ETA: 0s - loss: 0.0585 - accuracy: 0.9815\n",
            "223/235 [===========================>..] - ETA: 0s - loss: 0.0591 - accuracy: 0.9814\n",
            "229/235 [============================>.] - ETA: 0s - loss: 0.0589 - accuracy: 0.9814\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.0590 - accuracy: 0.9813\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0590 - accuracy: 0.9813 - val_loss: 0.0519 - val_accuracy: 0.9831\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 4/12\n",
            "  1/235 [..............................] - ETA: 4s - loss: 0.0568 - accuracy: 0.9805\n",
            "  4/235 [..............................] - ETA: 5s - loss: 0.0348 - accuracy: 0.9902\n",
            " 10/235 [>.............................] - ETA: 5s - loss: 0.0317 - accuracy: 0.9898\n",
            " 13/235 [>.............................] - ETA: 5s - loss: 0.0338 - accuracy: 0.9880\n",
            " 19/235 [=>............................] - ETA: 4s - loss: 0.0327 - accuracy: 0.9893\n",
            " 22/235 [=>............................] - ETA: 4s - loss: 0.0362 - accuracy: 0.9886\n",
            " 28/235 [==>...........................] - ETA: 4s - loss: 0.0359 - accuracy: 0.9881\n",
            " 33/235 [===>..........................] - ETA: 4s - loss: 0.0360 - accuracy: 0.9879\n",
            " 36/235 [===>..........................] - ETA: 4s - loss: 0.0358 - accuracy: 0.9882\n",
            " 42/235 [====>.........................] - ETA: 4s - loss: 0.0381 - accuracy: 0.9877\n",
            " 44/235 [====>.........................] - ETA: 4s - loss: 0.0388 - accuracy: 0.9872\n",
            " 47/235 [=====>........................] - ETA: 4s - loss: 0.0401 - accuracy: 0.9869\n",
            " 50/235 [=====>........................] - ETA: 4s - loss: 0.0408 - accuracy: 0.9865\n",
            " 56/235 [======>.......................] - ETA: 4s - loss: 0.0430 - accuracy: 0.9860\n",
            " 59/235 [======>.......................] - ETA: 4s - loss: 0.0424 - accuracy: 0.9861\n",
            " 65/235 [=======>......................] - ETA: 3s - loss: 0.0426 - accuracy: 0.9861\n",
            " 68/235 [=======>......................] - ETA: 3s - loss: 0.0418 - accuracy: 0.9863\n",
            " 74/235 [========>.....................] - ETA: 3s - loss: 0.0422 - accuracy: 0.9863\n",
            " 77/235 [========>.....................] - ETA: 3s - loss: 0.0419 - accuracy: 0.9865\n",
            " 83/235 [=========>....................] - ETA: 3s - loss: 0.0437 - accuracy: 0.9858\n",
            " 89/235 [==========>...................] - ETA: 3s - loss: 0.0426 - accuracy: 0.9863\n",
            " 92/235 [==========>...................] - ETA: 3s - loss: 0.0430 - accuracy: 0.9862\n",
            " 98/235 [===========>..................] - ETA: 3s - loss: 0.0427 - accuracy: 0.9863\n",
            "103/235 [============>.................] - ETA: 3s - loss: 0.0426 - accuracy: 0.9862\n",
            "106/235 [============>.................] - ETA: 2s - loss: 0.0421 - accuracy: 0.9863\n",
            "112/235 [=============>................] - ETA: 2s - loss: 0.0420 - accuracy: 0.9863\n",
            "115/235 [=============>................] - ETA: 2s - loss: 0.0421 - accuracy: 0.9863\n",
            "120/235 [==============>...............] - ETA: 2s - loss: 0.0426 - accuracy: 0.9863\n",
            "123/235 [==============>...............] - ETA: 2s - loss: 0.0438 - accuracy: 0.9860\n",
            "126/235 [===============>..............] - ETA: 2s - loss: 0.0434 - accuracy: 0.9861\n",
            "129/235 [===============>..............] - ETA: 2s - loss: 0.0440 - accuracy: 0.9858\n",
            "135/235 [================>.............] - ETA: 2s - loss: 0.0444 - accuracy: 0.9857\n",
            "137/235 [================>.............] - ETA: 2s - loss: 0.0452 - accuracy: 0.9855\n",
            "143/235 [=================>............] - ETA: 2s - loss: 0.0451 - accuracy: 0.9855\n",
            "146/235 [=================>............] - ETA: 2s - loss: 0.0454 - accuracy: 0.9854\n",
            "149/235 [==================>...........] - ETA: 1s - loss: 0.0456 - accuracy: 0.9855\n",
            "152/235 [==================>...........] - ETA: 1s - loss: 0.0459 - accuracy: 0.9854\n",
            "157/235 [===================>..........] - ETA: 1s - loss: 0.0459 - accuracy: 0.9853\n",
            "162/235 [===================>..........] - ETA: 1s - loss: 0.0458 - accuracy: 0.9854\n",
            "165/235 [====================>.........] - ETA: 1s - loss: 0.0456 - accuracy: 0.9855\n",
            "171/235 [====================>.........] - ETA: 1s - loss: 0.0457 - accuracy: 0.9855\n",
            "174/235 [=====================>........] - ETA: 1s - loss: 0.0461 - accuracy: 0.9853\n",
            "179/235 [=====================>........] - ETA: 1s - loss: 0.0459 - accuracy: 0.9853\n",
            "183/235 [======================>.......] - ETA: 1s - loss: 0.0457 - accuracy: 0.9853\n",
            "185/235 [======================>.......] - ETA: 1s - loss: 0.0455 - accuracy: 0.9854\n",
            "188/235 [=======================>......] - ETA: 1s - loss: 0.0458 - accuracy: 0.9853\n",
            "194/235 [=======================>......] - ETA: 0s - loss: 0.0457 - accuracy: 0.9854\n",
            "197/235 [========================>.....] - ETA: 0s - loss: 0.0455 - accuracy: 0.9855\n",
            "202/235 [========================>.....] - ETA: 0s - loss: 0.0453 - accuracy: 0.9854\n",
            "207/235 [=========================>....] - ETA: 0s - loss: 0.0451 - accuracy: 0.9854\n",
            "210/235 [=========================>....] - ETA: 0s - loss: 0.0457 - accuracy: 0.9852\n",
            "215/235 [==========================>...] - ETA: 0s - loss: 0.0458 - accuracy: 0.9852\n",
            "221/235 [===========================>..] - ETA: 0s - loss: 0.0458 - accuracy: 0.9853\n",
            "224/235 [===========================>..] - ETA: 0s - loss: 0.0460 - accuracy: 0.9852\n",
            "230/235 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9852\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9852\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0460 - accuracy: 0.9852 - val_loss: 0.0575 - val_accuracy: 0.9817\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 5/12\n",
            "  1/235 [..............................] - ETA: 5s - loss: 0.0110 - accuracy: 0.9961\n",
            "  7/235 [..............................] - ETA: 5s - loss: 0.0320 - accuracy: 0.9900\n",
            " 10/235 [>.............................] - ETA: 5s - loss: 0.0288 - accuracy: 0.9910\n",
            " 16/235 [=>............................] - ETA: 5s - loss: 0.0376 - accuracy: 0.9890\n",
            " 19/235 [=>............................] - ETA: 5s - loss: 0.0373 - accuracy: 0.9887\n",
            " 25/235 [==>...........................] - ETA: 4s - loss: 0.0367 - accuracy: 0.9881\n",
            " 28/235 [==>...........................] - ETA: 4s - loss: 0.0386 - accuracy: 0.9883\n",
            " 34/235 [===>..........................] - ETA: 4s - loss: 0.0377 - accuracy: 0.9885\n",
            " 37/235 [===>..........................] - ETA: 4s - loss: 0.0378 - accuracy: 0.9886\n",
            " 40/235 [====>.........................] - ETA: 4s - loss: 0.0382 - accuracy: 0.9885\n",
            " 43/235 [====>.........................] - ETA: 4s - loss: 0.0380 - accuracy: 0.9886\n",
            " 49/235 [=====>........................] - ETA: 4s - loss: 0.0391 - accuracy: 0.9884\n",
            " 52/235 [=====>........................] - ETA: 4s - loss: 0.0386 - accuracy: 0.9882\n",
            " 58/235 [======>.......................] - ETA: 4s - loss: 0.0378 - accuracy: 0.9881\n",
            " 61/235 [======>.......................] - ETA: 3s - loss: 0.0386 - accuracy: 0.9877\n",
            " 67/235 [=======>......................] - ETA: 3s - loss: 0.0373 - accuracy: 0.9879\n",
            " 70/235 [=======>......................] - ETA: 3s - loss: 0.0376 - accuracy: 0.9877\n",
            " 76/235 [========>.....................] - ETA: 3s - loss: 0.0373 - accuracy: 0.9878\n",
            " 79/235 [=========>....................] - ETA: 3s - loss: 0.0380 - accuracy: 0.9877\n",
            " 85/235 [=========>....................] - ETA: 3s - loss: 0.0374 - accuracy: 0.9881\n",
            " 90/235 [==========>...................] - ETA: 3s - loss: 0.0374 - accuracy: 0.9879\n",
            " 93/235 [==========>...................] - ETA: 3s - loss: 0.0379 - accuracy: 0.9878\n",
            " 99/235 [===========>..................] - ETA: 3s - loss: 0.0381 - accuracy: 0.9878\n",
            "102/235 [============>.................] - ETA: 3s - loss: 0.0380 - accuracy: 0.9877\n",
            "105/235 [============>.................] - ETA: 2s - loss: 0.0377 - accuracy: 0.9879\n",
            "108/235 [============>.................] - ETA: 2s - loss: 0.0374 - accuracy: 0.9881\n",
            "114/235 [=============>................] - ETA: 2s - loss: 0.0370 - accuracy: 0.9881\n",
            "117/235 [=============>................] - ETA: 2s - loss: 0.0367 - accuracy: 0.9882\n",
            "123/235 [==============>...............] - ETA: 2s - loss: 0.0366 - accuracy: 0.9883\n",
            "125/235 [==============>...............] - ETA: 2s - loss: 0.0363 - accuracy: 0.9884\n",
            "131/235 [===============>..............] - ETA: 2s - loss: 0.0365 - accuracy: 0.9883\n",
            "137/235 [================>.............] - ETA: 2s - loss: 0.0364 - accuracy: 0.9883\n",
            "140/235 [================>.............] - ETA: 2s - loss: 0.0366 - accuracy: 0.9883\n",
            "142/235 [=================>............] - ETA: 2s - loss: 0.0367 - accuracy: 0.9882\n",
            "145/235 [=================>............] - ETA: 2s - loss: 0.0369 - accuracy: 0.9880\n",
            "151/235 [==================>...........] - ETA: 1s - loss: 0.0375 - accuracy: 0.9879\n",
            "156/235 [==================>...........] - ETA: 1s - loss: 0.0382 - accuracy: 0.9876\n",
            "159/235 [===================>..........] - ETA: 1s - loss: 0.0384 - accuracy: 0.9874\n",
            "165/235 [====================>.........] - ETA: 1s - loss: 0.0389 - accuracy: 0.9873\n",
            "168/235 [====================>.........] - ETA: 1s - loss: 0.0390 - accuracy: 0.9872\n",
            "174/235 [=====================>........] - ETA: 1s - loss: 0.0395 - accuracy: 0.9871\n",
            "177/235 [=====================>........] - ETA: 1s - loss: 0.0398 - accuracy: 0.9870\n",
            "182/235 [======================>.......] - ETA: 1s - loss: 0.0402 - accuracy: 0.9868\n",
            "188/235 [=======================>......] - ETA: 1s - loss: 0.0405 - accuracy: 0.9868\n",
            "191/235 [=======================>......] - ETA: 1s - loss: 0.0405 - accuracy: 0.9868\n",
            "196/235 [========================>.....] - ETA: 0s - loss: 0.0404 - accuracy: 0.9868\n",
            "202/235 [========================>.....] - ETA: 0s - loss: 0.0408 - accuracy: 0.9868\n",
            "207/235 [=========================>....] - ETA: 0s - loss: 0.0420 - accuracy: 0.9865\n",
            "210/235 [=========================>....] - ETA: 0s - loss: 0.0419 - accuracy: 0.9866\n",
            "216/235 [==========================>...] - ETA: 0s - loss: 0.0422 - accuracy: 0.9864\n",
            "219/235 [==========================>...] - ETA: 0s - loss: 0.0426 - accuracy: 0.9864\n",
            "224/235 [===========================>..] - ETA: 0s - loss: 0.0426 - accuracy: 0.9864\n",
            "230/235 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9864\n",
            "233/235 [============================>.] - ETA: 0s - loss: 0.0426 - accuracy: 0.9864\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0426 - accuracy: 0.9863 - val_loss: 0.0560 - val_accuracy: 0.9830\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 6/12\n",
            "  1/235 [..............................] - ETA: 4s - loss: 0.0443 - accuracy: 0.9883\n",
            "  4/235 [..............................] - ETA: 5s - loss: 0.0465 - accuracy: 0.9863\n",
            " 10/235 [>.............................] - ETA: 5s - loss: 0.0376 - accuracy: 0.9875\n",
            " 13/235 [>.............................] - ETA: 5s - loss: 0.0431 - accuracy: 0.9856\n",
            " 19/235 [=>............................] - ETA: 4s - loss: 0.0397 - accuracy: 0.9864\n",
            " 25/235 [==>...........................] - ETA: 4s - loss: 0.0399 - accuracy: 0.9862\n",
            " 28/235 [==>...........................] - ETA: 4s - loss: 0.0401 - accuracy: 0.9859\n",
            " 34/235 [===>..........................] - ETA: 4s - loss: 0.0408 - accuracy: 0.9856\n",
            " 37/235 [===>..........................] - ETA: 4s - loss: 0.0401 - accuracy: 0.9864\n",
            " 43/235 [====>.........................] - ETA: 4s - loss: 0.0389 - accuracy: 0.9872\n",
            " 46/235 [====>.........................] - ETA: 4s - loss: 0.0381 - accuracy: 0.9875\n",
            " 52/235 [=====>........................] - ETA: 4s - loss: 0.0372 - accuracy: 0.9881\n",
            " 55/235 [======>.......................] - ETA: 4s - loss: 0.0371 - accuracy: 0.9881\n",
            " 61/235 [======>.......................] - ETA: 4s - loss: 0.0371 - accuracy: 0.9874\n",
            " 64/235 [=======>......................] - ETA: 3s - loss: 0.0367 - accuracy: 0.9877\n",
            " 67/235 [=======>......................] - ETA: 3s - loss: 0.0370 - accuracy: 0.9877\n",
            " 70/235 [=======>......................] - ETA: 3s - loss: 0.0373 - accuracy: 0.9874\n",
            " 76/235 [========>.....................] - ETA: 3s - loss: 0.0374 - accuracy: 0.9874\n",
            " 79/235 [=========>....................] - ETA: 3s - loss: 0.0368 - accuracy: 0.9876\n",
            " 85/235 [=========>....................] - ETA: 3s - loss: 0.0373 - accuracy: 0.9875\n",
            " 88/235 [==========>...................] - ETA: 3s - loss: 0.0377 - accuracy: 0.9873\n",
            " 94/235 [===========>..................] - ETA: 3s - loss: 0.0375 - accuracy: 0.9874\n",
            "100/235 [===========>..................] - ETA: 3s - loss: 0.0393 - accuracy: 0.9868\n",
            "103/235 [============>.................] - ETA: 3s - loss: 0.0388 - accuracy: 0.9870\n",
            "109/235 [============>.................] - ETA: 2s - loss: 0.0390 - accuracy: 0.9870\n",
            "114/235 [=============>................] - ETA: 2s - loss: 0.0397 - accuracy: 0.9867\n",
            "117/235 [=============>................] - ETA: 2s - loss: 0.0401 - accuracy: 0.9865\n",
            "123/235 [==============>...............] - ETA: 2s - loss: 0.0402 - accuracy: 0.9866\n",
            "126/235 [===============>..............] - ETA: 2s - loss: 0.0401 - accuracy: 0.9866\n",
            "132/235 [===============>..............] - ETA: 2s - loss: 0.0401 - accuracy: 0.9865\n",
            "138/235 [================>.............] - ETA: 2s - loss: 0.0400 - accuracy: 0.9865\n",
            "141/235 [=================>............] - ETA: 2s - loss: 0.0400 - accuracy: 0.9865\n",
            "147/235 [=================>............] - ETA: 2s - loss: 0.0404 - accuracy: 0.9865\n",
            "150/235 [==================>...........] - ETA: 1s - loss: 0.0402 - accuracy: 0.9865\n",
            "156/235 [==================>...........] - ETA: 1s - loss: 0.0401 - accuracy: 0.9866\n",
            "161/235 [===================>..........] - ETA: 1s - loss: 0.0400 - accuracy: 0.9866\n",
            "164/235 [===================>..........] - ETA: 1s - loss: 0.0401 - accuracy: 0.9866\n",
            "170/235 [====================>.........] - ETA: 1s - loss: 0.0404 - accuracy: 0.9867\n",
            "173/235 [=====================>........] - ETA: 1s - loss: 0.0408 - accuracy: 0.9867\n",
            "179/235 [=====================>........] - ETA: 1s - loss: 0.0414 - accuracy: 0.9866\n",
            "182/235 [======================>.......] - ETA: 1s - loss: 0.0412 - accuracy: 0.9866\n",
            "188/235 [=======================>......] - ETA: 1s - loss: 0.0414 - accuracy: 0.9866\n",
            "191/235 [=======================>......] - ETA: 1s - loss: 0.0413 - accuracy: 0.9866\n",
            "197/235 [========================>.....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9867\n",
            "202/235 [========================>.....] - ETA: 0s - loss: 0.0411 - accuracy: 0.9868\n",
            "205/235 [=========================>....] - ETA: 0s - loss: 0.0411 - accuracy: 0.9867\n",
            "211/235 [=========================>....] - ETA: 0s - loss: 0.0416 - accuracy: 0.9866\n",
            "216/235 [==========================>...] - ETA: 0s - loss: 0.0415 - accuracy: 0.9866\n",
            "219/235 [==========================>...] - ETA: 0s - loss: 0.0414 - accuracy: 0.9866\n",
            "225/235 [===========================>..] - ETA: 0s - loss: 0.0416 - accuracy: 0.9866\n",
            "228/235 [============================>.] - ETA: 0s - loss: 0.0418 - accuracy: 0.9865\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.0419 - accuracy: 0.9864\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0418 - accuracy: 0.9864 - val_loss: 0.0796 - val_accuracy: 0.9788\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 7/12\n",
            "  1/235 [..............................] - ETA: 4s - loss: 0.0526 - accuracy: 0.9766\n",
            "  7/235 [..............................] - ETA: 5s - loss: 0.0401 - accuracy: 0.9860\n",
            " 10/235 [>.............................] - ETA: 5s - loss: 0.0406 - accuracy: 0.9871\n",
            " 16/235 [=>............................] - ETA: 4s - loss: 0.0409 - accuracy: 0.9858\n",
            " 18/235 [=>............................] - ETA: 5s - loss: 0.0431 - accuracy: 0.9852\n",
            " 21/235 [=>............................] - ETA: 4s - loss: 0.0421 - accuracy: 0.9859\n",
            " 24/235 [==>...........................] - ETA: 4s - loss: 0.0403 - accuracy: 0.9863\n",
            " 30/235 [==>...........................] - ETA: 4s - loss: 0.0385 - accuracy: 0.9865\n",
            " 35/235 [===>..........................] - ETA: 4s - loss: 0.0369 - accuracy: 0.9874\n",
            " 38/235 [===>..........................] - ETA: 4s - loss: 0.0359 - accuracy: 0.9877\n",
            " 44/235 [====>.........................] - ETA: 4s - loss: 0.0382 - accuracy: 0.9874\n",
            " 47/235 [=====>........................] - ETA: 4s - loss: 0.0382 - accuracy: 0.9874\n",
            " 53/235 [=====>........................] - ETA: 4s - loss: 0.0376 - accuracy: 0.9878\n",
            " 56/235 [======>.......................] - ETA: 4s - loss: 0.0368 - accuracy: 0.9879\n",
            " 62/235 [======>.......................] - ETA: 4s - loss: 0.0370 - accuracy: 0.9878\n",
            " 65/235 [=======>......................] - ETA: 3s - loss: 0.0370 - accuracy: 0.9879\n",
            " 71/235 [========>.....................] - ETA: 3s - loss: 0.0364 - accuracy: 0.9880\n",
            " 76/235 [========>.....................] - ETA: 3s - loss: 0.0366 - accuracy: 0.9880\n",
            " 79/235 [=========>....................] - ETA: 3s - loss: 0.0361 - accuracy: 0.9883\n",
            " 85/235 [=========>....................] - ETA: 3s - loss: 0.0352 - accuracy: 0.9887\n",
            " 88/235 [==========>...................] - ETA: 3s - loss: 0.0354 - accuracy: 0.9889\n",
            " 93/235 [==========>...................] - ETA: 3s - loss: 0.0353 - accuracy: 0.9890\n",
            " 99/235 [===========>..................] - ETA: 3s - loss: 0.0355 - accuracy: 0.9888\n",
            "102/235 [============>.................] - ETA: 3s - loss: 0.0353 - accuracy: 0.9888\n",
            "108/235 [============>.................] - ETA: 2s - loss: 0.0356 - accuracy: 0.9887\n",
            "111/235 [=============>................] - ETA: 2s - loss: 0.0363 - accuracy: 0.9885\n",
            "117/235 [=============>................] - ETA: 2s - loss: 0.0358 - accuracy: 0.9885\n",
            "120/235 [==============>...............] - ETA: 2s - loss: 0.0355 - accuracy: 0.9886\n",
            "126/235 [===============>..............] - ETA: 2s - loss: 0.0359 - accuracy: 0.9884\n",
            "129/235 [===============>..............] - ETA: 2s - loss: 0.0360 - accuracy: 0.9883\n",
            "135/235 [================>.............] - ETA: 2s - loss: 0.0354 - accuracy: 0.9885\n",
            "138/235 [================>.............] - ETA: 2s - loss: 0.0354 - accuracy: 0.9885\n",
            "144/235 [=================>............] - ETA: 2s - loss: 0.0353 - accuracy: 0.9884\n",
            "149/235 [==================>...........] - ETA: 1s - loss: 0.0357 - accuracy: 0.9883\n",
            "152/235 [==================>...........] - ETA: 1s - loss: 0.0363 - accuracy: 0.9881\n",
            "158/235 [===================>..........] - ETA: 1s - loss: 0.0358 - accuracy: 0.9883\n",
            "161/235 [===================>..........] - ETA: 1s - loss: 0.0357 - accuracy: 0.9883\n",
            "167/235 [====================>.........] - ETA: 1s - loss: 0.0362 - accuracy: 0.9883\n",
            "170/235 [====================>.........] - ETA: 1s - loss: 0.0359 - accuracy: 0.9883\n",
            "176/235 [=====================>........] - ETA: 1s - loss: 0.0365 - accuracy: 0.9883\n",
            "182/235 [======================>.......] - ETA: 1s - loss: 0.0369 - accuracy: 0.9882\n",
            "185/235 [======================>.......] - ETA: 1s - loss: 0.0369 - accuracy: 0.9883\n",
            "191/235 [=======================>......] - ETA: 1s - loss: 0.0370 - accuracy: 0.9883\n",
            "194/235 [=======================>......] - ETA: 0s - loss: 0.0368 - accuracy: 0.9883\n",
            "199/235 [========================>.....] - ETA: 0s - loss: 0.0366 - accuracy: 0.9884\n",
            "205/235 [=========================>....] - ETA: 0s - loss: 0.0367 - accuracy: 0.9884\n",
            "208/235 [=========================>....] - ETA: 0s - loss: 0.0364 - accuracy: 0.9885\n",
            "213/235 [==========================>...] - ETA: 0s - loss: 0.0365 - accuracy: 0.9885\n",
            "216/235 [==========================>...] - ETA: 0s - loss: 0.0365 - accuracy: 0.9885\n",
            "222/235 [===========================>..] - ETA: 0s - loss: 0.0364 - accuracy: 0.9886\n",
            "228/235 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9886\n",
            "231/235 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9886\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9886\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0364 - accuracy: 0.9886 - val_loss: 0.0615 - val_accuracy: 0.9831\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 8/12\n",
            "  4/235 [..............................] - ETA: 5s - loss: 0.0147 - accuracy: 0.9951\n",
            "  7/235 [..............................] - ETA: 5s - loss: 0.0172 - accuracy: 0.9927\n",
            " 13/235 [>.............................] - ETA: 5s - loss: 0.0162 - accuracy: 0.9937\n",
            " 19/235 [=>............................] - ETA: 4s - loss: 0.0226 - accuracy: 0.9924\n",
            " 22/235 [=>............................] - ETA: 4s - loss: 0.0209 - accuracy: 0.9925\n",
            " 28/235 [==>...........................] - ETA: 4s - loss: 0.0224 - accuracy: 0.9919\n",
            " 31/235 [==>...........................] - ETA: 4s - loss: 0.0230 - accuracy: 0.9914\n",
            " 37/235 [===>..........................] - ETA: 4s - loss: 0.0264 - accuracy: 0.9900\n",
            " 43/235 [====>.........................] - ETA: 4s - loss: 0.0252 - accuracy: 0.9907\n",
            " 46/235 [====>.........................] - ETA: 4s - loss: 0.0254 - accuracy: 0.9906\n",
            " 52/235 [=====>........................] - ETA: 4s - loss: 0.0249 - accuracy: 0.9908\n",
            " 54/235 [=====>........................] - ETA: 4s - loss: 0.0251 - accuracy: 0.9906\n",
            " 60/235 [======>.......................] - ETA: 3s - loss: 0.0258 - accuracy: 0.9906\n",
            " 65/235 [=======>......................] - ETA: 3s - loss: 0.0261 - accuracy: 0.9902\n",
            " 68/235 [=======>......................] - ETA: 3s - loss: 0.0259 - accuracy: 0.9902\n",
            " 74/235 [========>.....................] - ETA: 3s - loss: 0.0259 - accuracy: 0.9904\n",
            " 77/235 [========>.....................] - ETA: 3s - loss: 0.0267 - accuracy: 0.9901\n",
            " 80/235 [=========>....................] - ETA: 3s - loss: 0.0274 - accuracy: 0.9901\n",
            " 83/235 [=========>....................] - ETA: 3s - loss: 0.0273 - accuracy: 0.9900\n",
            " 89/235 [==========>...................] - ETA: 3s - loss: 0.0282 - accuracy: 0.9900\n",
            " 92/235 [==========>...................] - ETA: 3s - loss: 0.0281 - accuracy: 0.9901\n",
            " 97/235 [===========>..................] - ETA: 3s - loss: 0.0277 - accuracy: 0.9903\n",
            "100/235 [===========>..................] - ETA: 3s - loss: 0.0273 - accuracy: 0.9905\n",
            "106/235 [============>.................] - ETA: 2s - loss: 0.0268 - accuracy: 0.9906\n",
            "112/235 [=============>................] - ETA: 2s - loss: 0.0272 - accuracy: 0.9905\n",
            "114/235 [=============>................] - ETA: 2s - loss: 0.0275 - accuracy: 0.9904\n",
            "117/235 [=============>................] - ETA: 2s - loss: 0.0273 - accuracy: 0.9906\n",
            "119/235 [==============>...............] - ETA: 2s - loss: 0.0271 - accuracy: 0.9905\n",
            "125/235 [==============>...............] - ETA: 2s - loss: 0.0273 - accuracy: 0.9906\n",
            "131/235 [===============>..............] - ETA: 2s - loss: 0.0271 - accuracy: 0.9906\n",
            "134/235 [================>.............] - ETA: 2s - loss: 0.0275 - accuracy: 0.9906\n",
            "140/235 [================>.............] - ETA: 2s - loss: 0.0273 - accuracy: 0.9907\n",
            "143/235 [=================>............] - ETA: 2s - loss: 0.0274 - accuracy: 0.9906\n",
            "149/235 [==================>...........] - ETA: 1s - loss: 0.0277 - accuracy: 0.9906\n",
            "152/235 [==================>...........] - ETA: 1s - loss: 0.0275 - accuracy: 0.9906\n",
            "158/235 [===================>..........] - ETA: 1s - loss: 0.0276 - accuracy: 0.9906\n",
            "163/235 [===================>..........] - ETA: 1s - loss: 0.0276 - accuracy: 0.9907\n",
            "166/235 [====================>.........] - ETA: 1s - loss: 0.0278 - accuracy: 0.9906\n",
            "172/235 [====================>.........] - ETA: 1s - loss: 0.0282 - accuracy: 0.9905\n",
            "175/235 [=====================>........] - ETA: 1s - loss: 0.0285 - accuracy: 0.9904\n",
            "181/235 [======================>.......] - ETA: 1s - loss: 0.0288 - accuracy: 0.9903\n",
            "184/235 [======================>.......] - ETA: 1s - loss: 0.0287 - accuracy: 0.9903\n",
            "190/235 [=======================>......] - ETA: 1s - loss: 0.0288 - accuracy: 0.9903\n",
            "193/235 [=======================>......] - ETA: 0s - loss: 0.0292 - accuracy: 0.9902\n",
            "199/235 [========================>.....] - ETA: 0s - loss: 0.0296 - accuracy: 0.9901\n",
            "202/235 [========================>.....] - ETA: 0s - loss: 0.0299 - accuracy: 0.9900\n",
            "205/235 [=========================>....] - ETA: 0s - loss: 0.0301 - accuracy: 0.9900\n",
            "208/235 [=========================>....] - ETA: 0s - loss: 0.0305 - accuracy: 0.9899\n",
            "214/235 [==========================>...] - ETA: 0s - loss: 0.0309 - accuracy: 0.9899\n",
            "217/235 [==========================>...] - ETA: 0s - loss: 0.0313 - accuracy: 0.9897\n",
            "223/235 [===========================>..] - ETA: 0s - loss: 0.0320 - accuracy: 0.9895\n",
            "226/235 [===========================>..] - ETA: 0s - loss: 0.0319 - accuracy: 0.9894\n",
            "232/235 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9893\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9893\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 0.0677 - val_accuracy: 0.9828\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 9/12\n",
            "  4/235 [..............................] - ETA: 5s - loss: 0.0303 - accuracy: 0.9863\n",
            "  7/235 [..............................] - ETA: 5s - loss: 0.0286 - accuracy: 0.9900\n",
            " 13/235 [>.............................] - ETA: 5s - loss: 0.0338 - accuracy: 0.9880\n",
            " 16/235 [=>............................] - ETA: 5s - loss: 0.0301 - accuracy: 0.9893\n",
            " 22/235 [=>............................] - ETA: 4s - loss: 0.0348 - accuracy: 0.9877\n",
            " 28/235 [==>...........................] - ETA: 4s - loss: 0.0316 - accuracy: 0.9890\n",
            " 31/235 [==>...........................] - ETA: 4s - loss: 0.0300 - accuracy: 0.9895\n",
            " 37/235 [===>..........................] - ETA: 4s - loss: 0.0305 - accuracy: 0.9895\n",
            " 40/235 [====>.........................] - ETA: 4s - loss: 0.0319 - accuracy: 0.9895\n",
            " 46/235 [====>.........................] - ETA: 4s - loss: 0.0315 - accuracy: 0.9899\n",
            " 49/235 [=====>........................] - ETA: 4s - loss: 0.0315 - accuracy: 0.9898\n",
            " 55/235 [======>.......................] - ETA: 4s - loss: 0.0314 - accuracy: 0.9896\n",
            " 58/235 [======>.......................] - ETA: 4s - loss: 0.0314 - accuracy: 0.9896\n",
            " 64/235 [=======>......................] - ETA: 3s - loss: 0.0306 - accuracy: 0.9897\n",
            " 70/235 [=======>......................] - ETA: 3s - loss: 0.0304 - accuracy: 0.9897\n",
            " 72/235 [========>.....................] - ETA: 3s - loss: 0.0311 - accuracy: 0.9895\n",
            " 78/235 [========>.....................] - ETA: 3s - loss: 0.0313 - accuracy: 0.9893\n",
            " 84/235 [=========>....................] - ETA: 3s - loss: 0.0313 - accuracy: 0.9894\n",
            " 87/235 [==========>...................] - ETA: 3s - loss: 0.0308 - accuracy: 0.9894\n",
            " 92/235 [==========>...................] - ETA: 3s - loss: 0.0305 - accuracy: 0.9895\n",
            " 98/235 [===========>..................] - ETA: 3s - loss: 0.0307 - accuracy: 0.9895\n",
            "101/235 [===========>..................] - ETA: 3s - loss: 0.0306 - accuracy: 0.9896\n",
            "107/235 [============>.................] - ETA: 2s - loss: 0.0309 - accuracy: 0.9895\n",
            "110/235 [=============>................] - ETA: 2s - loss: 0.0310 - accuracy: 0.9896\n",
            "116/235 [=============>................] - ETA: 2s - loss: 0.0321 - accuracy: 0.9895\n",
            "119/235 [==============>...............] - ETA: 2s - loss: 0.0323 - accuracy: 0.9893\n",
            "125/235 [==============>...............] - ETA: 2s - loss: 0.0330 - accuracy: 0.9892\n",
            "128/235 [===============>..............] - ETA: 2s - loss: 0.0333 - accuracy: 0.9892\n",
            "134/235 [================>.............] - ETA: 2s - loss: 0.0351 - accuracy: 0.9887\n",
            "137/235 [================>.............] - ETA: 2s - loss: 0.0373 - accuracy: 0.9883\n",
            "143/235 [=================>............] - ETA: 2s - loss: 0.0389 - accuracy: 0.9880\n",
            "146/235 [=================>............] - ETA: 2s - loss: 0.0389 - accuracy: 0.9880\n",
            "152/235 [==================>...........] - ETA: 1s - loss: 0.0395 - accuracy: 0.9878\n",
            "158/235 [===================>..........] - ETA: 1s - loss: 0.0399 - accuracy: 0.9878\n",
            "160/235 [===================>..........] - ETA: 1s - loss: 0.0400 - accuracy: 0.9879\n",
            "166/235 [====================>.........] - ETA: 1s - loss: 0.0402 - accuracy: 0.9877\n",
            "172/235 [====================>.........] - ETA: 1s - loss: 0.0404 - accuracy: 0.9875\n",
            "175/235 [=====================>........] - ETA: 1s - loss: 0.0402 - accuracy: 0.9875\n",
            "181/235 [======================>.......] - ETA: 1s - loss: 0.0400 - accuracy: 0.9874\n",
            "186/235 [======================>.......] - ETA: 1s - loss: 0.0403 - accuracy: 0.9875\n",
            "189/235 [=======================>......] - ETA: 1s - loss: 0.0402 - accuracy: 0.9875\n",
            "195/235 [=======================>......] - ETA: 0s - loss: 0.0405 - accuracy: 0.9873\n",
            "198/235 [========================>.....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9872\n",
            "203/235 [========================>.....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9871\n",
            "209/235 [=========================>....] - ETA: 0s - loss: 0.0411 - accuracy: 0.9871\n",
            "214/235 [==========================>...] - ETA: 0s - loss: 0.0415 - accuracy: 0.9870\n",
            "217/235 [==========================>...] - ETA: 0s - loss: 0.0412 - accuracy: 0.9871\n",
            "223/235 [===========================>..] - ETA: 0s - loss: 0.0408 - accuracy: 0.9873\n",
            "226/235 [===========================>..] - ETA: 0s - loss: 0.0407 - accuracy: 0.9873\n",
            "232/235 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9874\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9874\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0401 - accuracy: 0.9874 - val_loss: 0.0713 - val_accuracy: 0.9830\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 10/12\n",
            "  1/235 [..............................] - ETA: 6s - loss: 0.0193 - accuracy: 0.9844\n",
            "  7/235 [..............................] - ETA: 5s - loss: 0.0219 - accuracy: 0.9905\n",
            " 12/235 [>.............................] - ETA: 5s - loss: 0.0258 - accuracy: 0.9909\n",
            " 15/235 [>.............................] - ETA: 5s - loss: 0.0266 - accuracy: 0.9914\n",
            " 21/235 [=>............................] - ETA: 4s - loss: 0.0257 - accuracy: 0.9911\n",
            " 24/235 [==>...........................] - ETA: 4s - loss: 0.0253 - accuracy: 0.9912\n",
            " 27/235 [==>...........................] - ETA: 4s - loss: 0.0296 - accuracy: 0.9903\n",
            " 30/235 [==>...........................] - ETA: 4s - loss: 0.0317 - accuracy: 0.9900\n",
            " 36/235 [===>..........................] - ETA: 4s - loss: 0.0323 - accuracy: 0.9899\n",
            " 39/235 [===>..........................] - ETA: 4s - loss: 0.0339 - accuracy: 0.9897\n",
            " 44/235 [====>.........................] - ETA: 4s - loss: 0.0341 - accuracy: 0.9893\n",
            " 47/235 [=====>........................] - ETA: 4s - loss: 0.0338 - accuracy: 0.9894\n",
            " 52/235 [=====>........................] - ETA: 4s - loss: 0.0331 - accuracy: 0.9896\n",
            " 58/235 [======>.......................] - ETA: 4s - loss: 0.0350 - accuracy: 0.9893\n",
            " 61/235 [======>.......................] - ETA: 4s - loss: 0.0346 - accuracy: 0.9895\n",
            " 67/235 [=======>......................] - ETA: 3s - loss: 0.0354 - accuracy: 0.9892\n",
            " 73/235 [========>.....................] - ETA: 3s - loss: 0.0362 - accuracy: 0.9888\n",
            " 76/235 [========>.....................] - ETA: 3s - loss: 0.0365 - accuracy: 0.9886\n",
            " 82/235 [=========>....................] - ETA: 3s - loss: 0.0358 - accuracy: 0.9887\n",
            " 87/235 [==========>...................] - ETA: 3s - loss: 0.0363 - accuracy: 0.9886\n",
            " 90/235 [==========>...................] - ETA: 3s - loss: 0.0369 - accuracy: 0.9886\n",
            " 96/235 [===========>..................] - ETA: 3s - loss: 0.0360 - accuracy: 0.9887\n",
            " 98/235 [===========>..................] - ETA: 3s - loss: 0.0359 - accuracy: 0.9887\n",
            "101/235 [===========>..................] - ETA: 3s - loss: 0.0357 - accuracy: 0.9886\n",
            "104/235 [============>.................] - ETA: 3s - loss: 0.0355 - accuracy: 0.9886\n",
            "110/235 [=============>................] - ETA: 2s - loss: 0.0347 - accuracy: 0.9888\n",
            "113/235 [=============>................] - ETA: 2s - loss: 0.0349 - accuracy: 0.9887\n",
            "119/235 [==============>...............] - ETA: 2s - loss: 0.0353 - accuracy: 0.9888\n",
            "122/235 [==============>...............] - ETA: 2s - loss: 0.0356 - accuracy: 0.9887\n",
            "128/235 [===============>..............] - ETA: 2s - loss: 0.0355 - accuracy: 0.9887\n",
            "131/235 [===============>..............] - ETA: 2s - loss: 0.0352 - accuracy: 0.9888\n",
            "137/235 [================>.............] - ETA: 2s - loss: 0.0357 - accuracy: 0.9887\n",
            "140/235 [================>.............] - ETA: 2s - loss: 0.0355 - accuracy: 0.9888\n",
            "146/235 [=================>............] - ETA: 2s - loss: 0.0356 - accuracy: 0.9888\n",
            "151/235 [==================>...........] - ETA: 1s - loss: 0.0355 - accuracy: 0.9888\n",
            "154/235 [==================>...........] - ETA: 1s - loss: 0.0354 - accuracy: 0.9888\n",
            "159/235 [===================>..........] - ETA: 1s - loss: 0.0356 - accuracy: 0.9888\n",
            "162/235 [===================>..........] - ETA: 1s - loss: 0.0354 - accuracy: 0.9889\n",
            "168/235 [====================>.........] - ETA: 1s - loss: 0.0356 - accuracy: 0.9889\n",
            "171/235 [====================>.........] - ETA: 1s - loss: 0.0353 - accuracy: 0.9889\n",
            "174/235 [=====================>........] - ETA: 1s - loss: 0.0353 - accuracy: 0.9889\n",
            "177/235 [=====================>........] - ETA: 1s - loss: 0.0354 - accuracy: 0.9888\n",
            "182/235 [======================>.......] - ETA: 1s - loss: 0.0352 - accuracy: 0.9890\n",
            "187/235 [======================>.......] - ETA: 1s - loss: 0.0354 - accuracy: 0.9890\n",
            "192/235 [=======================>......] - ETA: 1s - loss: 0.0353 - accuracy: 0.9890\n",
            "194/235 [=======================>......] - ETA: 0s - loss: 0.0352 - accuracy: 0.9890\n",
            "200/235 [========================>.....] - ETA: 0s - loss: 0.0350 - accuracy: 0.9891\n",
            "206/235 [=========================>....] - ETA: 0s - loss: 0.0350 - accuracy: 0.9891\n",
            "208/235 [=========================>....] - ETA: 0s - loss: 0.0351 - accuracy: 0.9890\n",
            "211/235 [=========================>....] - ETA: 0s - loss: 0.0351 - accuracy: 0.9891\n",
            "214/235 [==========================>...] - ETA: 0s - loss: 0.0352 - accuracy: 0.9890\n",
            "220/235 [===========================>..] - ETA: 0s - loss: 0.0352 - accuracy: 0.9890\n",
            "223/235 [===========================>..] - ETA: 0s - loss: 0.0353 - accuracy: 0.9889\n",
            "229/235 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9889\n",
            "231/235 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9889\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9889\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 0.0351 - accuracy: 0.9889 - val_loss: 0.0902 - val_accuracy: 0.9771\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 11/12\n",
            "  4/235 [..............................] - ETA: 5s - loss: 0.0312 - accuracy: 0.9854\n",
            "  7/235 [..............................] - ETA: 5s - loss: 0.0352 - accuracy: 0.9855\n",
            " 13/235 [>.............................] - ETA: 5s - loss: 0.0375 - accuracy: 0.9877\n",
            " 16/235 [=>............................] - ETA: 5s - loss: 0.0371 - accuracy: 0.9878\n",
            " 21/235 [=>............................] - ETA: 4s - loss: 0.0366 - accuracy: 0.9877\n",
            " 27/235 [==>...........................] - ETA: 4s - loss: 0.0343 - accuracy: 0.9883\n",
            " 30/235 [==>...........................] - ETA: 4s - loss: 0.0352 - accuracy: 0.9884\n",
            " 36/235 [===>..........................] - ETA: 4s - loss: 0.0330 - accuracy: 0.9891\n",
            " 39/235 [===>..........................] - ETA: 4s - loss: 0.0342 - accuracy: 0.9892\n",
            " 45/235 [====>.........................] - ETA: 4s - loss: 0.0319 - accuracy: 0.9897\n",
            " 48/235 [=====>........................] - ETA: 4s - loss: 0.0317 - accuracy: 0.9898\n",
            " 51/235 [=====>........................] - ETA: 4s - loss: 0.0315 - accuracy: 0.9900\n",
            " 53/235 [=====>........................] - ETA: 4s - loss: 0.0309 - accuracy: 0.9901\n",
            " 59/235 [======>.......................] - ETA: 4s - loss: 0.0307 - accuracy: 0.9904\n",
            " 62/235 [======>.......................] - ETA: 4s - loss: 0.0304 - accuracy: 0.9904\n",
            " 68/235 [=======>......................] - ETA: 3s - loss: 0.0298 - accuracy: 0.9904\n",
            " 71/235 [========>.....................] - ETA: 3s - loss: 0.0295 - accuracy: 0.9906\n",
            " 77/235 [========>.....................] - ETA: 3s - loss: 0.0289 - accuracy: 0.9910\n",
            " 83/235 [=========>....................] - ETA: 3s - loss: 0.0297 - accuracy: 0.9906\n",
            " 86/235 [=========>....................] - ETA: 3s - loss: 0.0299 - accuracy: 0.9906\n",
            " 92/235 [==========>...................] - ETA: 3s - loss: 0.0297 - accuracy: 0.9906\n",
            " 95/235 [===========>..................] - ETA: 3s - loss: 0.0293 - accuracy: 0.9907\n",
            "101/235 [===========>..................] - ETA: 3s - loss: 0.0291 - accuracy: 0.9909\n",
            "104/235 [============>.................] - ETA: 3s - loss: 0.0293 - accuracy: 0.9908\n",
            "107/235 [============>.................] - ETA: 2s - loss: 0.0293 - accuracy: 0.9909\n",
            "110/235 [=============>................] - ETA: 2s - loss: 0.0291 - accuracy: 0.9910\n",
            "116/235 [=============>................] - ETA: 2s - loss: 0.0292 - accuracy: 0.9909\n",
            "119/235 [==============>...............] - ETA: 2s - loss: 0.0298 - accuracy: 0.9908\n",
            "125/235 [==============>...............] - ETA: 2s - loss: 0.0303 - accuracy: 0.9907\n",
            "128/235 [===============>..............] - ETA: 2s - loss: 0.0305 - accuracy: 0.9906\n",
            "134/235 [================>.............] - ETA: 2s - loss: 0.0313 - accuracy: 0.9905\n",
            "139/235 [================>.............] - ETA: 2s - loss: 0.0328 - accuracy: 0.9902\n",
            "144/235 [=================>............] - ETA: 2s - loss: 0.0334 - accuracy: 0.9901\n",
            "147/235 [=================>............] - ETA: 2s - loss: 0.0335 - accuracy: 0.9901\n",
            "153/235 [==================>...........] - ETA: 1s - loss: 0.0332 - accuracy: 0.9901\n",
            "156/235 [==================>...........] - ETA: 1s - loss: 0.0332 - accuracy: 0.9901\n",
            "162/235 [===================>..........] - ETA: 1s - loss: 0.0330 - accuracy: 0.9901\n",
            "168/235 [====================>.........] - ETA: 1s - loss: 0.0332 - accuracy: 0.9900\n",
            "173/235 [=====================>........] - ETA: 1s - loss: 0.0336 - accuracy: 0.9899\n",
            "175/235 [=====================>........] - ETA: 1s - loss: 0.0333 - accuracy: 0.9900\n",
            "181/235 [======================>.......] - ETA: 1s - loss: 0.0331 - accuracy: 0.9901\n",
            "184/235 [======================>.......] - ETA: 1s - loss: 0.0330 - accuracy: 0.9900\n",
            "190/235 [=======================>......] - ETA: 1s - loss: 0.0332 - accuracy: 0.9900\n",
            "193/235 [=======================>......] - ETA: 0s - loss: 0.0332 - accuracy: 0.9899\n",
            "199/235 [========================>.....] - ETA: 0s - loss: 0.0329 - accuracy: 0.9899\n",
            "202/235 [========================>.....] - ETA: 0s - loss: 0.0327 - accuracy: 0.9900\n",
            "208/235 [=========================>....] - ETA: 0s - loss: 0.0324 - accuracy: 0.9900\n",
            "211/235 [=========================>....] - ETA: 0s - loss: 0.0324 - accuracy: 0.9901\n",
            "217/235 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 0.9901\n",
            "223/235 [===========================>..] - ETA: 0s - loss: 0.0331 - accuracy: 0.9901\n",
            "226/235 [===========================>..] - ETA: 0s - loss: 0.0332 - accuracy: 0.9901\n",
            "232/235 [============================>.] - ETA: 0s - loss: 0.0334 - accuracy: 0.9901\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9900\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0336 - accuracy: 0.9900 - val_loss: 0.0761 - val_accuracy: 0.9832\n",
            "\u001b[36m(train_mnist pid=3298267)\u001b[0m Epoch 12/12\n",
            "  4/235 [..............................] - ETA: 5s - loss: 0.0572 - accuracy: 0.9844\n",
            "  7/235 [..............................] - ETA: 5s - loss: 0.0588 - accuracy: 0.9827\n",
            " 12/235 [>.............................] - ETA: 5s - loss: 0.0475 - accuracy: 0.9857\n",
            " 17/235 [=>............................] - ETA: 5s - loss: 0.0393 - accuracy: 0.9881\n",
            " 20/235 [=>............................] - ETA: 5s - loss: 0.0373 - accuracy: 0.9889\n",
            " 26/235 [==>...........................] - ETA: 4s - loss: 0.0366 - accuracy: 0.9884\n",
            " 29/235 [==>...........................] - ETA: 4s - loss: 0.0364 - accuracy: 0.9887\n",
            " 32/235 [===>..........................] - ETA: 4s - loss: 0.0382 - accuracy: 0.9879\n",
            " 35/235 [===>..........................] - ETA: 4s - loss: 0.0374 - accuracy: 0.9883\n",
            " 41/235 [====>.........................] - ETA: 4s - loss: 0.0384 - accuracy: 0.9883\n",
            " 43/235 [====>.........................] - ETA: 4s - loss: 0.0387 - accuracy: 0.9878\n",
            " 49/235 [=====>........................] - ETA: 4s - loss: 0.0363 - accuracy: 0.9885\n",
            " 55/235 [======>.......................] - ETA: 4s - loss: 0.0365 - accuracy: 0.9886\n",
            " 58/235 [======>.......................] - ETA: 4s - loss: 0.0357 - accuracy: 0.9888\n",
            " 64/235 [=======>......................] - ETA: 3s - loss: 0.0344 - accuracy: 0.9892\n",
            " 67/235 [=======>......................] - ETA: 3s - loss: 0.0339 - accuracy: 0.9894\n",
            " 73/235 [========>.....................] - ETA: 3s - loss: 0.0328 - accuracy: 0.9897\n",
            " 78/235 [========>.....................] - ETA: 3s - loss: 0.0325 - accuracy: 0.9897\n",
            " 81/235 [=========>....................] - ETA: 3s - loss: 0.0316 - accuracy: 0.9900\n",
            " 86/235 [=========>....................] - ETA: 3s - loss: 0.0307 - accuracy: 0.9903\n",
            " 89/235 [==========>...................] - ETA: 3s - loss: 0.0307 - accuracy: 0.9903\n",
            " 95/235 [===========>..................] - ETA: 3s - loss: 0.0301 - accuracy: 0.9905\n",
            "101/235 [===========>..................] - ETA: 3s - loss: 0.0307 - accuracy: 0.9904\n",
            "104/235 [============>.................] - ETA: 3s - loss: 0.0309 - accuracy: 0.9904\n",
            "110/235 [=============>................] - ETA: 2s - loss: 0.0316 - accuracy: 0.9903\n",
            "113/235 [=============>................] - ETA: 2s - loss: 0.0318 - accuracy: 0.9903\n",
            "119/235 [==============>...............] - ETA: 2s - loss: 0.0317 - accuracy: 0.9903\n",
            "125/235 [==============>...............] - ETA: 2s - loss: 0.0322 - accuracy: 0.9902\n",
            "128/235 [===============>..............] - ETA: 2s - loss: 0.0328 - accuracy: 0.9901\n",
            "134/235 [================>.............] - ETA: 2s - loss: 0.0325 - accuracy: 0.9902\n",
            "137/235 [================>.............] - ETA: 2s - loss: 0.0323 - accuracy: 0.9902\n",
            "143/235 [=================>............] - ETA: 2s - loss: 0.0322 - accuracy: 0.9902\n",
            "146/235 [=================>............] - ETA: 2s - loss: 0.0324 - accuracy: 0.9902\n",
            "152/235 [==================>...........] - ETA: 1s - loss: 0.0320 - accuracy: 0.9903\n",
            "155/235 [==================>...........] - ETA: 1s - loss: 0.0318 - accuracy: 0.9903\n",
            "161/235 [===================>..........] - ETA: 1s - loss: 0.0315 - accuracy: 0.9904\n",
            "167/235 [====================>.........] - ETA: 1s - loss: 0.0315 - accuracy: 0.9906\n",
            "170/235 [====================>.........] - ETA: 1s - loss: 0.0314 - accuracy: 0.9905\n",
            "176/235 [=====================>........] - ETA: 1s - loss: 0.0322 - accuracy: 0.9903\n",
            "179/235 [=====================>........] - ETA: 1s - loss: 0.0323 - accuracy: 0.9902\n",
            "184/235 [======================>.......] - ETA: 1s - loss: 0.0327 - accuracy: 0.9901\n",
            "190/235 [=======================>......] - ETA: 1s - loss: 0.0338 - accuracy: 0.9899\n",
            "192/235 [=======================>......] - ETA: 0s - loss: 0.0338 - accuracy: 0.9899\n",
            "198/235 [========================>.....] - ETA: 0s - loss: 0.0349 - accuracy: 0.9898\n",
            "204/235 [=========================>....] - ETA: 0s - loss: 0.0352 - accuracy: 0.9895\n",
            "207/235 [=========================>....] - ETA: 0s - loss: 0.0354 - accuracy: 0.9894\n",
            "213/235 [==========================>...] - ETA: 0s - loss: 0.0368 - accuracy: 0.9891\n",
            "216/235 [==========================>...] - ETA: 0s - loss: 0.0369 - accuracy: 0.9890\n",
            "222/235 [===========================>..] - ETA: 0s - loss: 0.0372 - accuracy: 0.9889\n",
            "228/235 [============================>.] - ETA: 0s - loss: 0.0375 - accuracy: 0.9888\n",
            "231/235 [============================>.] - ETA: 0s - loss: 0.0378 - accuracy: 0.9887\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.0378 - accuracy: 0.9887\n",
            "235/235 [==============================] - 6s 24ms/step - loss: 0.0377 - accuracy: 0.9887 - val_loss: 0.0966 - val_accuracy: 0.9812\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3298881)\u001b[0m 2023-12-05 01:48:12.996169: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3298881)\u001b[0m 2023-12-05 01:48:12.998432: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3298881)\u001b[0m 2023-12-05 01:48:13.044090: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3298881)\u001b[0m 2023-12-05 01:48:13.044591: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3298881)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3298881)\u001b[0m 2023-12-05 01:48:13.928254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3298881)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3298881)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3298881)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3298881)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3298881)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3298881)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3298881)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3298881)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m 2023-12-05 01:48:15.698187: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 1/12\n",
            "  4/469 [..............................] - ETA: 8s - loss: 11.6311 - accuracy: 0.1543 \n",
            " 10/469 [..............................] - ETA: 8s - loss: 6.0415 - accuracy: 0.1156\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 4.6400 - accuracy: 0.1172\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 4.2650 - accuracy: 0.1324\n",
            " 25/469 [>.............................] - ETA: 8s - loss: 3.6833 - accuracy: 0.1991\n",
            " 31/469 [>.............................] - ETA: 8s - loss: 3.1754 - accuracy: 0.2865\n",
            " 37/469 [=>............................] - ETA: 8s - loss: 2.7903 - accuracy: 0.3640\n",
            " 43/469 [=>............................] - ETA: 8s - loss: 2.4995 - accuracy: 0.4246\n",
            " 49/469 [==>...........................] - ETA: 8s - loss: 2.2761 - accuracy: 0.4700\n",
            " 55/469 [==>...........................] - ETA: 8s - loss: 2.0917 - accuracy: 0.5075\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 2.0155 - accuracy: 0.5237\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 1.8728 - accuracy: 0.5538\n",
            " 70/469 [===>..........................] - ETA: 7s - loss: 1.7597 - accuracy: 0.5797\n",
            " 76/469 [===>..........................] - ETA: 7s - loss: 1.6609 - accuracy: 0.6002\n",
            " 82/469 [====>.........................] - ETA: 7s - loss: 1.5751 - accuracy: 0.6191\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 1.5334 - accuracy: 0.6282\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 1.4678 - accuracy: 0.6418\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 1.4092 - accuracy: 0.6541\n",
            "103/469 [=====>........................] - ETA: 7s - loss: 1.3534 - accuracy: 0.6661\n",
            "109/469 [=====>........................] - ETA: 7s - loss: 1.3033 - accuracy: 0.6775\n",
            "115/469 [======>.......................] - ETA: 6s - loss: 1.2569 - accuracy: 0.6877\n",
            "118/469 [======>.......................] - ETA: 6s - loss: 1.2356 - accuracy: 0.6927\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 1.1956 - accuracy: 0.7018\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 1.1639 - accuracy: 0.7082\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 1.1304 - accuracy: 0.7154\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 1.1004 - accuracy: 0.7223\n",
            "148/469 [========>.....................] - ETA: 6s - loss: 1.0717 - accuracy: 0.7291\n",
            "151/469 [========>.....................] - ETA: 6s - loss: 1.0601 - accuracy: 0.7317\n",
            "154/469 [========>.....................] - ETA: 6s - loss: 1.0474 - accuracy: 0.7346\n",
            "157/469 [=========>....................] - ETA: 6s - loss: 1.0361 - accuracy: 0.7372\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 1.0127 - accuracy: 0.7419\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.9900 - accuracy: 0.7472\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.9676 - accuracy: 0.7519\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.9511 - accuracy: 0.7556\n",
            "187/469 [==========>...................] - ETA: 5s - loss: 0.9343 - accuracy: 0.7593\n",
            "190/469 [===========>..................] - ETA: 5s - loss: 0.9263 - accuracy: 0.7611\n",
            "193/469 [===========>..................] - ETA: 5s - loss: 0.9168 - accuracy: 0.7632\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.9080 - accuracy: 0.7653\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.8938 - accuracy: 0.7682\n",
            "205/469 [============>.................] - ETA: 5s - loss: 0.8872 - accuracy: 0.7699\n",
            "208/469 [============>.................] - ETA: 5s - loss: 0.8804 - accuracy: 0.7712\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.8675 - accuracy: 0.7742\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.8549 - accuracy: 0.7765\n",
            "226/469 [=============>................] - ETA: 4s - loss: 0.8412 - accuracy: 0.7800\n",
            "232/469 [=============>................] - ETA: 4s - loss: 0.8284 - accuracy: 0.7831\n",
            "238/469 [==============>...............] - ETA: 4s - loss: 0.8166 - accuracy: 0.7857\n",
            "244/469 [==============>...............] - ETA: 4s - loss: 0.8042 - accuracy: 0.7885\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.7984 - accuracy: 0.7899\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.7866 - accuracy: 0.7926\n",
            "259/469 [===============>..............] - ETA: 4s - loss: 0.7766 - accuracy: 0.7947\n",
            "265/469 [===============>..............] - ETA: 3s - loss: 0.7681 - accuracy: 0.7968\n",
            "271/469 [================>.............] - ETA: 3s - loss: 0.7600 - accuracy: 0.7984\n",
            "277/469 [================>.............] - ETA: 3s - loss: 0.7509 - accuracy: 0.8005\n",
            "283/469 [=================>............] - ETA: 3s - loss: 0.7424 - accuracy: 0.8026\n",
            "289/469 [=================>............] - ETA: 3s - loss: 0.7342 - accuracy: 0.8045\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.7300 - accuracy: 0.8054\n",
            "298/469 [==================>...........] - ETA: 3s - loss: 0.7223 - accuracy: 0.8072\n",
            "304/469 [==================>...........] - ETA: 3s - loss: 0.7157 - accuracy: 0.8088\n",
            "310/469 [==================>...........] - ETA: 3s - loss: 0.7082 - accuracy: 0.8106\n",
            "316/469 [===================>..........] - ETA: 2s - loss: 0.7015 - accuracy: 0.8121\n",
            "322/469 [===================>..........] - ETA: 2s - loss: 0.6939 - accuracy: 0.8141\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.6869 - accuracy: 0.8159\n",
            "331/469 [====================>.........] - ETA: 2s - loss: 0.6837 - accuracy: 0.8167\n",
            "337/469 [====================>.........] - ETA: 2s - loss: 0.6762 - accuracy: 0.8185\n",
            "343/469 [====================>.........] - ETA: 2s - loss: 0.6698 - accuracy: 0.8203\n",
            "349/469 [=====================>........] - ETA: 2s - loss: 0.6643 - accuracy: 0.8216\n",
            "355/469 [=====================>........] - ETA: 2s - loss: 0.6576 - accuracy: 0.8233\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.6514 - accuracy: 0.8248\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.6489 - accuracy: 0.8253\n",
            "370/469 [======================>.......] - ETA: 1s - loss: 0.6438 - accuracy: 0.8266\n",
            "376/469 [=======================>......] - ETA: 1s - loss: 0.6399 - accuracy: 0.8277\n",
            "382/469 [=======================>......] - ETA: 1s - loss: 0.6352 - accuracy: 0.8289\n",
            "388/469 [=======================>......] - ETA: 1s - loss: 0.6313 - accuracy: 0.8296\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.6268 - accuracy: 0.8306\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.6228 - accuracy: 0.8316\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.6203 - accuracy: 0.8323\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.6180 - accuracy: 0.8328\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.6166 - accuracy: 0.8331\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.6129 - accuracy: 0.8340\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.6084 - accuracy: 0.8350\n",
            "427/469 [==========================>...] - ETA: 0s - loss: 0.6049 - accuracy: 0.8358\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 0.6006 - accuracy: 0.8369\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.5966 - accuracy: 0.8379\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.5951 - accuracy: 0.8383\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.5930 - accuracy: 0.8388\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.5913 - accuracy: 0.8393\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.5875 - accuracy: 0.8403\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.5853 - accuracy: 0.8408\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.5819 - accuracy: 0.8416\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.5799 - accuracy: 0.8421\n",
            "469/469 [==============================] - 10s 21ms/step - loss: 0.5799 - accuracy: 0.8421 - val_loss: 0.1799 - val_accuracy: 0.9453\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 2/12\n",
            "  1/469 [..............................] - ETA: 9s - loss: 0.3238 - accuracy: 0.8984\n",
            "  7/469 [..............................] - ETA: 8s - loss: 0.2735 - accuracy: 0.9152\n",
            " 10/469 [..............................] - ETA: 8s - loss: 0.2957 - accuracy: 0.9078\n",
            " 13/469 [..............................] - ETA: 8s - loss: 0.3079 - accuracy: 0.9062\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 0.3011 - accuracy: 0.9028\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 0.2990 - accuracy: 0.9054\n",
            " 22/469 [>.............................] - ETA: 8s - loss: 0.2981 - accuracy: 0.9048\n",
            " 28/469 [>.............................] - ETA: 8s - loss: 0.2955 - accuracy: 0.9049\n",
            " 34/469 [=>............................] - ETA: 8s - loss: 0.2972 - accuracy: 0.9033\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.3033 - accuracy: 0.9033\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.3049 - accuracy: 0.9027\n",
            " 52/469 [==>...........................] - ETA: 7s - loss: 0.3033 - accuracy: 0.9026\n",
            " 55/469 [==>...........................] - ETA: 7s - loss: 0.3061 - accuracy: 0.9023\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 0.3031 - accuracy: 0.9032\n",
            " 61/469 [==>...........................] - ETA: 7s - loss: 0.3039 - accuracy: 0.9033\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 0.3055 - accuracy: 0.9025\n",
            " 67/469 [===>..........................] - ETA: 7s - loss: 0.3076 - accuracy: 0.9024\n",
            " 73/469 [===>..........................] - ETA: 7s - loss: 0.3068 - accuracy: 0.9027\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.3056 - accuracy: 0.9038\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.3046 - accuracy: 0.9037\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.3031 - accuracy: 0.9043\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.3028 - accuracy: 0.9050\n",
            "103/469 [=====>........................] - ETA: 6s - loss: 0.3009 - accuracy: 0.9056\n",
            "106/469 [=====>........................] - ETA: 6s - loss: 0.2993 - accuracy: 0.9062\n",
            "112/469 [======>.......................] - ETA: 6s - loss: 0.3007 - accuracy: 0.9062\n",
            "118/469 [======>.......................] - ETA: 6s - loss: 0.2981 - accuracy: 0.9073\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2963 - accuracy: 0.9080\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2970 - accuracy: 0.9078\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2980 - accuracy: 0.9075\n",
            "139/469 [=======>......................] - ETA: 6s - loss: 0.2979 - accuracy: 0.9078\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2964 - accuracy: 0.9078\n",
            "145/469 [========>.....................] - ETA: 6s - loss: 0.2955 - accuracy: 0.9083\n",
            "151/469 [========>.....................] - ETA: 6s - loss: 0.2979 - accuracy: 0.9081\n",
            "157/469 [=========>....................] - ETA: 5s - loss: 0.2972 - accuracy: 0.9084\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2957 - accuracy: 0.9089\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.2946 - accuracy: 0.9091\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2971 - accuracy: 0.9083\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2996 - accuracy: 0.9085\n",
            "184/469 [==========>...................] - ETA: 5s - loss: 0.2993 - accuracy: 0.9085\n",
            "190/469 [===========>..................] - ETA: 5s - loss: 0.2996 - accuracy: 0.9086\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2986 - accuracy: 0.9087\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.2968 - accuracy: 0.9088\n",
            "208/469 [============>.................] - ETA: 4s - loss: 0.2980 - accuracy: 0.9087\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2977 - accuracy: 0.9086\n",
            "217/469 [============>.................] - ETA: 4s - loss: 0.2977 - accuracy: 0.9086\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.2965 - accuracy: 0.9090\n",
            "223/469 [=============>................] - ETA: 4s - loss: 0.2972 - accuracy: 0.9091\n",
            "229/469 [=============>................] - ETA: 4s - loss: 0.2970 - accuracy: 0.9091\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2968 - accuracy: 0.9091\n",
            "241/469 [==============>...............] - ETA: 4s - loss: 0.2969 - accuracy: 0.9090\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2979 - accuracy: 0.9092\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2973 - accuracy: 0.9094\n",
            "259/469 [===============>..............] - ETA: 4s - loss: 0.2981 - accuracy: 0.9092\n",
            "262/469 [===============>..............] - ETA: 3s - loss: 0.2975 - accuracy: 0.9093\n",
            "268/469 [================>.............] - ETA: 3s - loss: 0.2959 - accuracy: 0.9097\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2968 - accuracy: 0.9096\n",
            "280/469 [================>.............] - ETA: 3s - loss: 0.2974 - accuracy: 0.9095\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2971 - accuracy: 0.9097\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2971 - accuracy: 0.9098\n",
            "298/469 [==================>...........] - ETA: 3s - loss: 0.2957 - accuracy: 0.9100\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2956 - accuracy: 0.9101\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2954 - accuracy: 0.9104\n",
            "313/469 [===================>..........] - ETA: 2s - loss: 0.2962 - accuracy: 0.9102\n",
            "319/469 [===================>..........] - ETA: 2s - loss: 0.2953 - accuracy: 0.9104\n",
            "325/469 [===================>..........] - ETA: 2s - loss: 0.2942 - accuracy: 0.9105\n",
            "331/469 [====================>.........] - ETA: 2s - loss: 0.2950 - accuracy: 0.9104\n",
            "337/469 [====================>.........] - ETA: 2s - loss: 0.2953 - accuracy: 0.9103\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2954 - accuracy: 0.9104\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2952 - accuracy: 0.9104\n",
            "352/469 [=====================>........] - ETA: 2s - loss: 0.2957 - accuracy: 0.9106\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2952 - accuracy: 0.9108\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2940 - accuracy: 0.9111\n",
            "370/469 [======================>.......] - ETA: 1s - loss: 0.2923 - accuracy: 0.9116\n",
            "376/469 [=======================>......] - ETA: 1s - loss: 0.2915 - accuracy: 0.9119\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2915 - accuracy: 0.9119\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2905 - accuracy: 0.9121\n",
            "391/469 [========================>.....] - ETA: 1s - loss: 0.2916 - accuracy: 0.9118\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2920 - accuracy: 0.9118\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.2925 - accuracy: 0.9117\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2915 - accuracy: 0.9118\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2907 - accuracy: 0.9120\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.2908 - accuracy: 0.9121\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2909 - accuracy: 0.9120\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2911 - accuracy: 0.9120\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.2913 - accuracy: 0.9119\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.2920 - accuracy: 0.9116\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2918 - accuracy: 0.9117\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2916 - accuracy: 0.9118\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.2913 - accuracy: 0.9119\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2914 - accuracy: 0.9118\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.2911 - accuracy: 0.9120\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9118\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2912 - accuracy: 0.9120\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.9121\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2908 - accuracy: 0.9121 - val_loss: 0.1521 - val_accuracy: 0.9552\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 3/12\n",
            "  4/469 [..............................] - ETA: 8s - loss: 0.2156 - accuracy: 0.9258\n",
            " 10/469 [..............................] - ETA: 8s - loss: 0.2406 - accuracy: 0.9281\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 0.2260 - accuracy: 0.9331\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 0.2353 - accuracy: 0.9289\n",
            " 25/469 [>.............................] - ETA: 8s - loss: 0.2384 - accuracy: 0.9269\n",
            " 31/469 [>.............................] - ETA: 8s - loss: 0.2442 - accuracy: 0.9254\n",
            " 37/469 [=>............................] - ETA: 8s - loss: 0.2466 - accuracy: 0.9255\n",
            " 43/469 [=>............................] - ETA: 8s - loss: 0.2472 - accuracy: 0.9257\n",
            " 49/469 [==>...........................] - ETA: 7s - loss: 0.2500 - accuracy: 0.9254\n",
            " 55/469 [==>...........................] - ETA: 7s - loss: 0.2489 - accuracy: 0.9261\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 0.2494 - accuracy: 0.9267\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 0.2561 - accuracy: 0.9263\n",
            " 70/469 [===>..........................] - ETA: 7s - loss: 0.2570 - accuracy: 0.9249\n",
            " 76/469 [===>..........................] - ETA: 7s - loss: 0.2570 - accuracy: 0.9249\n",
            " 82/469 [====>.........................] - ETA: 7s - loss: 0.2613 - accuracy: 0.9241\n",
            " 88/469 [====>.........................] - ETA: 7s - loss: 0.2622 - accuracy: 0.9239\n",
            " 94/469 [=====>........................] - ETA: 7s - loss: 0.2635 - accuracy: 0.9235\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2641 - accuracy: 0.9228\n",
            "103/469 [=====>........................] - ETA: 6s - loss: 0.2629 - accuracy: 0.9232\n",
            "109/469 [=====>........................] - ETA: 6s - loss: 0.2671 - accuracy: 0.9225\n",
            "115/469 [======>.......................] - ETA: 6s - loss: 0.2674 - accuracy: 0.9219\n",
            "121/469 [======>.......................] - ETA: 6s - loss: 0.2673 - accuracy: 0.9217\n",
            "127/469 [=======>......................] - ETA: 6s - loss: 0.2650 - accuracy: 0.9219\n",
            "133/469 [=======>......................] - ETA: 6s - loss: 0.2666 - accuracy: 0.9218\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2675 - accuracy: 0.9217\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2672 - accuracy: 0.9210\n",
            "148/469 [========>.....................] - ETA: 6s - loss: 0.2664 - accuracy: 0.9213\n",
            "154/469 [========>.....................] - ETA: 6s - loss: 0.2668 - accuracy: 0.9211\n",
            "160/469 [=========>....................] - ETA: 5s - loss: 0.2675 - accuracy: 0.9208\n",
            "166/469 [=========>....................] - ETA: 5s - loss: 0.2687 - accuracy: 0.9205\n",
            "172/469 [==========>...................] - ETA: 5s - loss: 0.2717 - accuracy: 0.9198\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2729 - accuracy: 0.9196\n",
            "178/469 [==========>...................] - ETA: 5s - loss: 0.2735 - accuracy: 0.9193\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2738 - accuracy: 0.9193\n",
            "187/469 [==========>...................] - ETA: 5s - loss: 0.2731 - accuracy: 0.9192\n",
            "193/469 [===========>..................] - ETA: 5s - loss: 0.2735 - accuracy: 0.9190\n",
            "199/469 [===========>..................] - ETA: 5s - loss: 0.2714 - accuracy: 0.9194\n",
            "205/469 [============>.................] - ETA: 5s - loss: 0.2708 - accuracy: 0.9194\n",
            "211/469 [============>.................] - ETA: 4s - loss: 0.2708 - accuracy: 0.9194\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2704 - accuracy: 0.9194\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.2688 - accuracy: 0.9196\n",
            "226/469 [=============>................] - ETA: 4s - loss: 0.2699 - accuracy: 0.9196\n",
            "232/469 [=============>................] - ETA: 4s - loss: 0.2701 - accuracy: 0.9195\n",
            "238/469 [==============>...............] - ETA: 4s - loss: 0.2689 - accuracy: 0.9197\n",
            "244/469 [==============>...............] - ETA: 4s - loss: 0.2697 - accuracy: 0.9196\n",
            "250/469 [==============>...............] - ETA: 4s - loss: 0.2681 - accuracy: 0.9198\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2675 - accuracy: 0.9201\n",
            "256/469 [===============>..............] - ETA: 4s - loss: 0.2673 - accuracy: 0.9203\n",
            "259/469 [===============>..............] - ETA: 4s - loss: 0.2679 - accuracy: 0.9200\n",
            "265/469 [===============>..............] - ETA: 3s - loss: 0.2681 - accuracy: 0.9201\n",
            "271/469 [================>.............] - ETA: 3s - loss: 0.2699 - accuracy: 0.9198\n",
            "277/469 [================>.............] - ETA: 3s - loss: 0.2691 - accuracy: 0.9200\n",
            "283/469 [=================>............] - ETA: 3s - loss: 0.2691 - accuracy: 0.9200\n",
            "289/469 [=================>............] - ETA: 3s - loss: 0.2713 - accuracy: 0.9193\n",
            "295/469 [=================>............] - ETA: 3s - loss: 0.2708 - accuracy: 0.9193\n",
            "298/469 [==================>...........] - ETA: 3s - loss: 0.2704 - accuracy: 0.9195\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2706 - accuracy: 0.9194\n",
            "304/469 [==================>...........] - ETA: 3s - loss: 0.2708 - accuracy: 0.9192\n",
            "310/469 [==================>...........] - ETA: 3s - loss: 0.2715 - accuracy: 0.9189\n",
            "316/469 [===================>..........] - ETA: 2s - loss: 0.2722 - accuracy: 0.9187\n",
            "322/469 [===================>..........] - ETA: 2s - loss: 0.2717 - accuracy: 0.9188\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2708 - accuracy: 0.9191\n",
            "334/469 [====================>.........] - ETA: 2s - loss: 0.2711 - accuracy: 0.9189\n",
            "337/469 [====================>.........] - ETA: 2s - loss: 0.2724 - accuracy: 0.9185\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2721 - accuracy: 0.9187\n",
            "343/469 [====================>.........] - ETA: 2s - loss: 0.2722 - accuracy: 0.9187\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2723 - accuracy: 0.9187\n",
            "349/469 [=====================>........] - ETA: 2s - loss: 0.2721 - accuracy: 0.9188\n",
            "355/469 [=====================>........] - ETA: 2s - loss: 0.2719 - accuracy: 0.9187\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2733 - accuracy: 0.9184\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2726 - accuracy: 0.9185\n",
            "373/469 [======================>.......] - ETA: 1s - loss: 0.2722 - accuracy: 0.9186\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2721 - accuracy: 0.9185\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2723 - accuracy: 0.9185\n",
            "388/469 [=======================>......] - ETA: 1s - loss: 0.2720 - accuracy: 0.9187\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.2717 - accuracy: 0.9188\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2717 - accuracy: 0.9186\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2719 - accuracy: 0.9186\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2724 - accuracy: 0.9186\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.2725 - accuracy: 0.9186\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2723 - accuracy: 0.9185\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2722 - accuracy: 0.9184\n",
            "427/469 [==========================>...] - ETA: 0s - loss: 0.2722 - accuracy: 0.9185\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 0.2722 - accuracy: 0.9184\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.2720 - accuracy: 0.9185\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2711 - accuracy: 0.9186\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2707 - accuracy: 0.9186\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2707 - accuracy: 0.9186\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.9186\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.2710 - accuracy: 0.9186\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2715 - accuracy: 0.9184\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.9184\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2715 - accuracy: 0.9184 - val_loss: 0.1423 - val_accuracy: 0.9567\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 4/12\n",
            "  1/469 [..............................] - ETA: 9s - loss: 0.1610 - accuracy: 0.9609\n",
            "  4/469 [..............................] - ETA: 8s - loss: 0.2211 - accuracy: 0.9375\n",
            "  7/469 [..............................] - ETA: 8s - loss: 0.2271 - accuracy: 0.9319\n",
            " 10/469 [..............................] - ETA: 8s - loss: 0.2250 - accuracy: 0.9305\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 0.2549 - accuracy: 0.9233\n",
            " 22/469 [>.............................] - ETA: 8s - loss: 0.2523 - accuracy: 0.9240\n",
            " 28/469 [>.............................] - ETA: 8s - loss: 0.2601 - accuracy: 0.9180\n",
            " 34/469 [=>............................] - ETA: 8s - loss: 0.2602 - accuracy: 0.9180\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.2510 - accuracy: 0.9211\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.2545 - accuracy: 0.9214\n",
            " 49/469 [==>...........................] - ETA: 8s - loss: 0.2511 - accuracy: 0.9227\n",
            " 52/469 [==>...........................] - ETA: 7s - loss: 0.2511 - accuracy: 0.9223\n",
            " 55/469 [==>...........................] - ETA: 7s - loss: 0.2524 - accuracy: 0.9213\n",
            " 61/469 [==>...........................] - ETA: 7s - loss: 0.2531 - accuracy: 0.9217\n",
            " 67/469 [===>..........................] - ETA: 7s - loss: 0.2513 - accuracy: 0.9225\n",
            " 73/469 [===>..........................] - ETA: 7s - loss: 0.2483 - accuracy: 0.9236\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.2449 - accuracy: 0.9254\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.2450 - accuracy: 0.9251\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.2456 - accuracy: 0.9253\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2463 - accuracy: 0.9253\n",
            "100/469 [=====>........................] - ETA: 7s - loss: 0.2453 - accuracy: 0.9257\n",
            "103/469 [=====>........................] - ETA: 6s - loss: 0.2452 - accuracy: 0.9257\n",
            "106/469 [=====>........................] - ETA: 6s - loss: 0.2440 - accuracy: 0.9259\n",
            "112/469 [======>.......................] - ETA: 6s - loss: 0.2444 - accuracy: 0.9253\n",
            "118/469 [======>.......................] - ETA: 6s - loss: 0.2420 - accuracy: 0.9256\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2435 - accuracy: 0.9249\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2458 - accuracy: 0.9242\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2449 - accuracy: 0.9245\n",
            "139/469 [=======>......................] - ETA: 6s - loss: 0.2442 - accuracy: 0.9244\n",
            "145/469 [========>.....................] - ETA: 6s - loss: 0.2431 - accuracy: 0.9245\n",
            "151/469 [========>.....................] - ETA: 6s - loss: 0.2444 - accuracy: 0.9246\n",
            "157/469 [=========>....................] - ETA: 5s - loss: 0.2448 - accuracy: 0.9245\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2439 - accuracy: 0.9248\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.2455 - accuracy: 0.9246\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2493 - accuracy: 0.9237\n",
            "178/469 [==========>...................] - ETA: 5s - loss: 0.2485 - accuracy: 0.9239\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2476 - accuracy: 0.9243\n",
            "184/469 [==========>...................] - ETA: 5s - loss: 0.2476 - accuracy: 0.9246\n",
            "190/469 [===========>..................] - ETA: 5s - loss: 0.2468 - accuracy: 0.9247\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2469 - accuracy: 0.9248\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.2466 - accuracy: 0.9251\n",
            "208/469 [============>.................] - ETA: 5s - loss: 0.2468 - accuracy: 0.9251\n",
            "211/469 [============>.................] - ETA: 4s - loss: 0.2463 - accuracy: 0.9253\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2473 - accuracy: 0.9250\n",
            "217/469 [============>.................] - ETA: 4s - loss: 0.2486 - accuracy: 0.9247\n",
            "223/469 [=============>................] - ETA: 4s - loss: 0.2482 - accuracy: 0.9249\n",
            "229/469 [=============>................] - ETA: 4s - loss: 0.2494 - accuracy: 0.9247\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2492 - accuracy: 0.9248\n",
            "241/469 [==============>...............] - ETA: 4s - loss: 0.2486 - accuracy: 0.9248\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2472 - accuracy: 0.9250\n",
            "250/469 [==============>...............] - ETA: 4s - loss: 0.2488 - accuracy: 0.9247\n",
            "256/469 [===============>..............] - ETA: 4s - loss: 0.2482 - accuracy: 0.9247\n",
            "262/469 [===============>..............] - ETA: 3s - loss: 0.2489 - accuracy: 0.9247\n",
            "268/469 [================>.............] - ETA: 3s - loss: 0.2490 - accuracy: 0.9249\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2481 - accuracy: 0.9254\n",
            "280/469 [================>.............] - ETA: 3s - loss: 0.2490 - accuracy: 0.9251\n",
            "283/469 [=================>............] - ETA: 3s - loss: 0.2483 - accuracy: 0.9252\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2494 - accuracy: 0.9250\n",
            "289/469 [=================>............] - ETA: 3s - loss: 0.2492 - accuracy: 0.9250\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2490 - accuracy: 0.9251\n",
            "295/469 [=================>............] - ETA: 3s - loss: 0.2497 - accuracy: 0.9248\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2489 - accuracy: 0.9251\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2487 - accuracy: 0.9251\n",
            "313/469 [===================>..........] - ETA: 2s - loss: 0.2483 - accuracy: 0.9253\n",
            "319/469 [===================>..........] - ETA: 2s - loss: 0.2494 - accuracy: 0.9250\n",
            "322/469 [===================>..........] - ETA: 2s - loss: 0.2493 - accuracy: 0.9251\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2492 - accuracy: 0.9254\n",
            "334/469 [====================>.........] - ETA: 2s - loss: 0.2494 - accuracy: 0.9251\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2501 - accuracy: 0.9247\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2506 - accuracy: 0.9247\n",
            "352/469 [=====================>........] - ETA: 2s - loss: 0.2528 - accuracy: 0.9245\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2531 - accuracy: 0.9243\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2531 - accuracy: 0.9242\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2532 - accuracy: 0.9242\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2539 - accuracy: 0.9241\n",
            "373/469 [======================>.......] - ETA: 1s - loss: 0.2548 - accuracy: 0.9236\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2549 - accuracy: 0.9236\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2565 - accuracy: 0.9233\n",
            "391/469 [========================>.....] - ETA: 1s - loss: 0.2576 - accuracy: 0.9230\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2581 - accuracy: 0.9229\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2580 - accuracy: 0.9230\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2582 - accuracy: 0.9228\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2578 - accuracy: 0.9229\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2568 - accuracy: 0.9231\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.2560 - accuracy: 0.9233\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.2562 - accuracy: 0.9231\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2570 - accuracy: 0.9231\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2583 - accuracy: 0.9229\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2583 - accuracy: 0.9228\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2594 - accuracy: 0.9225\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9223\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.9223\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.9218\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2604 - accuracy: 0.9218 - val_loss: 0.1548 - val_accuracy: 0.9549\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 5/12\n",
            "  1/469 [..............................] - ETA: 8s - loss: 0.2823 - accuracy: 0.9141\n",
            "  7/469 [..............................] - ETA: 8s - loss: 0.2602 - accuracy: 0.9174\n",
            " 13/469 [..............................] - ETA: 8s - loss: 0.2293 - accuracy: 0.9279\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 0.2238 - accuracy: 0.9268\n",
            " 25/469 [>.............................] - ETA: 8s - loss: 0.2146 - accuracy: 0.9278\n",
            " 31/469 [>.............................] - ETA: 8s - loss: 0.2332 - accuracy: 0.9272\n",
            " 34/469 [=>............................] - ETA: 8s - loss: 0.2311 - accuracy: 0.9285\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.2248 - accuracy: 0.9299\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.2305 - accuracy: 0.9288\n",
            " 52/469 [==>...........................] - ETA: 8s - loss: 0.2358 - accuracy: 0.9271\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 0.2393 - accuracy: 0.9263\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 0.2377 - accuracy: 0.9263\n",
            " 67/469 [===>..........................] - ETA: 7s - loss: 0.2378 - accuracy: 0.9261\n",
            " 73/469 [===>..........................] - ETA: 7s - loss: 0.2385 - accuracy: 0.9257\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.2409 - accuracy: 0.9257\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.2390 - accuracy: 0.9260\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.2372 - accuracy: 0.9269\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2368 - accuracy: 0.9269\n",
            "103/469 [=====>........................] - ETA: 7s - loss: 0.2346 - accuracy: 0.9273\n",
            "106/469 [=====>........................] - ETA: 7s - loss: 0.2342 - accuracy: 0.9278\n",
            "112/469 [======>.......................] - ETA: 6s - loss: 0.2369 - accuracy: 0.9272\n",
            "118/469 [======>.......................] - ETA: 6s - loss: 0.2389 - accuracy: 0.9268\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2387 - accuracy: 0.9275\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2403 - accuracy: 0.9270\n",
            "133/469 [=======>......................] - ETA: 6s - loss: 0.2404 - accuracy: 0.9270\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2392 - accuracy: 0.9273\n",
            "139/469 [=======>......................] - ETA: 6s - loss: 0.2383 - accuracy: 0.9278\n",
            "145/469 [========>.....................] - ETA: 6s - loss: 0.2367 - accuracy: 0.9286\n",
            "151/469 [========>.....................] - ETA: 6s - loss: 0.2366 - accuracy: 0.9284\n",
            "157/469 [=========>....................] - ETA: 6s - loss: 0.2362 - accuracy: 0.9284\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2363 - accuracy: 0.9283\n",
            "166/469 [=========>....................] - ETA: 5s - loss: 0.2362 - accuracy: 0.9282\n",
            "172/469 [==========>...................] - ETA: 5s - loss: 0.2368 - accuracy: 0.9279\n",
            "178/469 [==========>...................] - ETA: 5s - loss: 0.2379 - accuracy: 0.9277\n",
            "184/469 [==========>...................] - ETA: 5s - loss: 0.2386 - accuracy: 0.9274\n",
            "190/469 [===========>..................] - ETA: 5s - loss: 0.2385 - accuracy: 0.9269\n",
            "193/469 [===========>..................] - ETA: 5s - loss: 0.2412 - accuracy: 0.9267\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2411 - accuracy: 0.9267\n",
            "199/469 [===========>..................] - ETA: 5s - loss: 0.2411 - accuracy: 0.9268\n",
            "205/469 [============>.................] - ETA: 5s - loss: 0.2402 - accuracy: 0.9270\n",
            "211/469 [============>.................] - ETA: 5s - loss: 0.2396 - accuracy: 0.9274\n",
            "217/469 [============>.................] - ETA: 4s - loss: 0.2403 - accuracy: 0.9271\n",
            "223/469 [=============>................] - ETA: 4s - loss: 0.2413 - accuracy: 0.9273\n",
            "229/469 [=============>................] - ETA: 4s - loss: 0.2420 - accuracy: 0.9272\n",
            "232/469 [=============>................] - ETA: 4s - loss: 0.2417 - accuracy: 0.9273\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2423 - accuracy: 0.9273\n",
            "238/469 [==============>...............] - ETA: 4s - loss: 0.2430 - accuracy: 0.9272\n",
            "244/469 [==============>...............] - ETA: 4s - loss: 0.2438 - accuracy: 0.9267\n",
            "250/469 [==============>...............] - ETA: 4s - loss: 0.2436 - accuracy: 0.9265\n",
            "256/469 [===============>..............] - ETA: 4s - loss: 0.2437 - accuracy: 0.9265\n",
            "262/469 [===============>..............] - ETA: 4s - loss: 0.2429 - accuracy: 0.9267\n",
            "268/469 [================>.............] - ETA: 3s - loss: 0.2436 - accuracy: 0.9268\n",
            "271/469 [================>.............] - ETA: 3s - loss: 0.2428 - accuracy: 0.9272\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2430 - accuracy: 0.9270\n",
            "277/469 [================>.............] - ETA: 3s - loss: 0.2430 - accuracy: 0.9269\n",
            "283/469 [=================>............] - ETA: 3s - loss: 0.2434 - accuracy: 0.9268\n",
            "289/469 [=================>............] - ETA: 3s - loss: 0.2425 - accuracy: 0.9271\n",
            "295/469 [=================>............] - ETA: 3s - loss: 0.2441 - accuracy: 0.9267\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2437 - accuracy: 0.9269\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2436 - accuracy: 0.9270\n",
            "310/469 [==================>...........] - ETA: 3s - loss: 0.2442 - accuracy: 0.9270\n",
            "316/469 [===================>..........] - ETA: 2s - loss: 0.2441 - accuracy: 0.9269\n",
            "322/469 [===================>..........] - ETA: 2s - loss: 0.2438 - accuracy: 0.9269\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2441 - accuracy: 0.9270\n",
            "334/469 [====================>.........] - ETA: 2s - loss: 0.2436 - accuracy: 0.9272\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2434 - accuracy: 0.9274\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2444 - accuracy: 0.9273\n",
            "349/469 [=====================>........] - ETA: 2s - loss: 0.2450 - accuracy: 0.9273\n",
            "355/469 [=====================>........] - ETA: 2s - loss: 0.2451 - accuracy: 0.9274\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2452 - accuracy: 0.9274\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2453 - accuracy: 0.9273\n",
            "373/469 [======================>.......] - ETA: 1s - loss: 0.2453 - accuracy: 0.9273\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2454 - accuracy: 0.9275\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2461 - accuracy: 0.9273\n",
            "388/469 [=======================>......] - ETA: 1s - loss: 0.2460 - accuracy: 0.9273\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.2461 - accuracy: 0.9272\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2457 - accuracy: 0.9272\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2469 - accuracy: 0.9271\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2471 - accuracy: 0.9270\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2479 - accuracy: 0.9268\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2478 - accuracy: 0.9267\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.2482 - accuracy: 0.9266\n",
            "427/469 [==========================>...] - ETA: 0s - loss: 0.2483 - accuracy: 0.9267\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 0.2497 - accuracy: 0.9262\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.2498 - accuracy: 0.9260\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2501 - accuracy: 0.9259\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2504 - accuracy: 0.9258\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.2512 - accuracy: 0.9257\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.9256\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2514 - accuracy: 0.9255\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9257\n",
            "469/469 [==============================] - 10s 20ms/step - loss: 0.2511 - accuracy: 0.9257 - val_loss: 0.1315 - val_accuracy: 0.9609\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 6/12\n",
            "  1/469 [..............................] - ETA: 9s - loss: 0.2158 - accuracy: 0.9297\n",
            "  7/469 [..............................] - ETA: 9s - loss: 0.2808 - accuracy: 0.9263\n",
            " 10/469 [..............................] - ETA: 9s - loss: 0.2725 - accuracy: 0.9234\n",
            " 13/469 [..............................] - ETA: 8s - loss: 0.2555 - accuracy: 0.9285\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 0.2528 - accuracy: 0.9287\n",
            " 22/469 [>.............................] - ETA: 8s - loss: 0.2395 - accuracy: 0.9293\n",
            " 28/469 [>.............................] - ETA: 8s - loss: 0.2270 - accuracy: 0.9322\n",
            " 34/469 [=>............................] - ETA: 8s - loss: 0.2346 - accuracy: 0.9308\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.2436 - accuracy: 0.9291\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.2413 - accuracy: 0.9285\n",
            " 49/469 [==>...........................] - ETA: 8s - loss: 0.2371 - accuracy: 0.9295\n",
            " 52/469 [==>...........................] - ETA: 8s - loss: 0.2321 - accuracy: 0.9312\n",
            " 55/469 [==>...........................] - ETA: 7s - loss: 0.2269 - accuracy: 0.9324\n",
            " 61/469 [==>...........................] - ETA: 7s - loss: 0.2302 - accuracy: 0.9312\n",
            " 67/469 [===>..........................] - ETA: 7s - loss: 0.2269 - accuracy: 0.9330\n",
            " 73/469 [===>..........................] - ETA: 7s - loss: 0.2302 - accuracy: 0.9317\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.2321 - accuracy: 0.9304\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.2334 - accuracy: 0.9301\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.2342 - accuracy: 0.9301\n",
            " 94/469 [=====>........................] - ETA: 7s - loss: 0.2317 - accuracy: 0.9308\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2303 - accuracy: 0.9310\n",
            "100/469 [=====>........................] - ETA: 7s - loss: 0.2290 - accuracy: 0.9316\n",
            "106/469 [=====>........................] - ETA: 6s - loss: 0.2275 - accuracy: 0.9319\n",
            "112/469 [======>.......................] - ETA: 6s - loss: 0.2264 - accuracy: 0.9326\n",
            "118/469 [======>.......................] - ETA: 6s - loss: 0.2272 - accuracy: 0.9327\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2272 - accuracy: 0.9328\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2265 - accuracy: 0.9327\n",
            "133/469 [=======>......................] - ETA: 6s - loss: 0.2278 - accuracy: 0.9322\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2281 - accuracy: 0.9322\n",
            "139/469 [=======>......................] - ETA: 6s - loss: 0.2272 - accuracy: 0.9324\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2268 - accuracy: 0.9327\n",
            "145/469 [========>.....................] - ETA: 6s - loss: 0.2268 - accuracy: 0.9326\n",
            "151/469 [========>.....................] - ETA: 6s - loss: 0.2261 - accuracy: 0.9332\n",
            "157/469 [=========>....................] - ETA: 5s - loss: 0.2248 - accuracy: 0.9337\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2243 - accuracy: 0.9335\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.2230 - accuracy: 0.9338\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2251 - accuracy: 0.9338\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2263 - accuracy: 0.9334\n",
            "184/469 [==========>...................] - ETA: 5s - loss: 0.2270 - accuracy: 0.9332\n",
            "190/469 [===========>..................] - ETA: 5s - loss: 0.2269 - accuracy: 0.9335\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2280 - accuracy: 0.9331\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.2273 - accuracy: 0.9334\n",
            "208/469 [============>.................] - ETA: 5s - loss: 0.2280 - accuracy: 0.9330\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2303 - accuracy: 0.9324\n",
            "217/469 [============>.................] - ETA: 4s - loss: 0.2302 - accuracy: 0.9322\n",
            "223/469 [=============>................] - ETA: 4s - loss: 0.2304 - accuracy: 0.9320\n",
            "229/469 [=============>................] - ETA: 4s - loss: 0.2315 - accuracy: 0.9315\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2324 - accuracy: 0.9312\n",
            "241/469 [==============>...............] - ETA: 4s - loss: 0.2342 - accuracy: 0.9307\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2329 - accuracy: 0.9310\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2337 - accuracy: 0.9309\n",
            "256/469 [===============>..............] - ETA: 4s - loss: 0.2339 - accuracy: 0.9308\n",
            "262/469 [===============>..............] - ETA: 3s - loss: 0.2349 - accuracy: 0.9303\n",
            "268/469 [================>.............] - ETA: 3s - loss: 0.2347 - accuracy: 0.9301\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2356 - accuracy: 0.9303\n",
            "280/469 [================>.............] - ETA: 3s - loss: 0.2360 - accuracy: 0.9303\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2372 - accuracy: 0.9297\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2380 - accuracy: 0.9296\n",
            "295/469 [=================>............] - ETA: 3s - loss: 0.2384 - accuracy: 0.9295\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2393 - accuracy: 0.9293\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2408 - accuracy: 0.9291\n",
            "313/469 [===================>..........] - ETA: 2s - loss: 0.2419 - accuracy: 0.9292\n",
            "319/469 [===================>..........] - ETA: 2s - loss: 0.2422 - accuracy: 0.9289\n",
            "325/469 [===================>..........] - ETA: 2s - loss: 0.2420 - accuracy: 0.9288\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2427 - accuracy: 0.9285\n",
            "334/469 [====================>.........] - ETA: 2s - loss: 0.2439 - accuracy: 0.9283\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2457 - accuracy: 0.9277\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2448 - accuracy: 0.9279\n",
            "352/469 [=====================>........] - ETA: 2s - loss: 0.2457 - accuracy: 0.9279\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2466 - accuracy: 0.9277\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2467 - accuracy: 0.9276\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2470 - accuracy: 0.9275\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2464 - accuracy: 0.9276\n",
            "373/469 [======================>.......] - ETA: 1s - loss: 0.2459 - accuracy: 0.9279\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2451 - accuracy: 0.9280\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2451 - accuracy: 0.9278\n",
            "391/469 [========================>.....] - ETA: 1s - loss: 0.2455 - accuracy: 0.9278\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2449 - accuracy: 0.9279\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.2444 - accuracy: 0.9279\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2443 - accuracy: 0.9279\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2443 - accuracy: 0.9279\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2449 - accuracy: 0.9276\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2446 - accuracy: 0.9278\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.2440 - accuracy: 0.9279\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.2435 - accuracy: 0.9281\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2432 - accuracy: 0.9283\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2426 - accuracy: 0.9285\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2421 - accuracy: 0.9285\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2421 - accuracy: 0.9285\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.9286\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2419 - accuracy: 0.9288\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.9290\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2410 - accuracy: 0.9290 - val_loss: 0.1442 - val_accuracy: 0.9620\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 7/12\n",
            "  1/469 [..............................] - ETA: 8s - loss: 0.3599 - accuracy: 0.8828\n",
            "  7/469 [..............................] - ETA: 8s - loss: 0.2193 - accuracy: 0.9297\n",
            " 13/469 [..............................] - ETA: 8s - loss: 0.2250 - accuracy: 0.9303\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 0.2169 - accuracy: 0.9322\n",
            " 25/469 [>.............................] - ETA: 8s - loss: 0.2112 - accuracy: 0.9353\n",
            " 28/469 [>.............................] - ETA: 8s - loss: 0.2231 - accuracy: 0.9328\n",
            " 31/469 [>.............................] - ETA: 8s - loss: 0.2336 - accuracy: 0.9327\n",
            " 34/469 [=>............................] - ETA: 8s - loss: 0.2369 - accuracy: 0.9324\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.2588 - accuracy: 0.9291\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.2659 - accuracy: 0.9283\n",
            " 52/469 [==>...........................] - ETA: 7s - loss: 0.2731 - accuracy: 0.9273\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 0.2741 - accuracy: 0.9265\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 0.2777 - accuracy: 0.9241\n",
            " 67/469 [===>..........................] - ETA: 7s - loss: 0.2757 - accuracy: 0.9246\n",
            " 70/469 [===>..........................] - ETA: 7s - loss: 0.2749 - accuracy: 0.9253\n",
            " 73/469 [===>..........................] - ETA: 7s - loss: 0.2784 - accuracy: 0.9251\n",
            " 76/469 [===>..........................] - ETA: 7s - loss: 0.2744 - accuracy: 0.9258\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.2744 - accuracy: 0.9247\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.2736 - accuracy: 0.9243\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.2716 - accuracy: 0.9246\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2687 - accuracy: 0.9253\n",
            "103/469 [=====>........................] - ETA: 7s - loss: 0.2673 - accuracy: 0.9260\n",
            "109/469 [=====>........................] - ETA: 6s - loss: 0.2677 - accuracy: 0.9263\n",
            "115/469 [======>.......................] - ETA: 6s - loss: 0.2676 - accuracy: 0.9255\n",
            "121/469 [======>.......................] - ETA: 6s - loss: 0.2705 - accuracy: 0.9245\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2686 - accuracy: 0.9248\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2676 - accuracy: 0.9253\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2678 - accuracy: 0.9247\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2642 - accuracy: 0.9256\n",
            "148/469 [========>.....................] - ETA: 6s - loss: 0.2618 - accuracy: 0.9259\n",
            "154/469 [========>.....................] - ETA: 6s - loss: 0.2625 - accuracy: 0.9258\n",
            "160/469 [=========>....................] - ETA: 5s - loss: 0.2620 - accuracy: 0.9259\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2612 - accuracy: 0.9259\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.2623 - accuracy: 0.9259\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2607 - accuracy: 0.9265\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2608 - accuracy: 0.9265\n",
            "187/469 [==========>...................] - ETA: 5s - loss: 0.2626 - accuracy: 0.9262\n",
            "193/469 [===========>..................] - ETA: 5s - loss: 0.2618 - accuracy: 0.9264\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2620 - accuracy: 0.9262\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.2616 - accuracy: 0.9263\n",
            "208/469 [============>.................] - ETA: 4s - loss: 0.2601 - accuracy: 0.9267\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2591 - accuracy: 0.9270\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.2575 - accuracy: 0.9271\n",
            "226/469 [=============>................] - ETA: 4s - loss: 0.2569 - accuracy: 0.9272\n",
            "232/469 [=============>................] - ETA: 4s - loss: 0.2582 - accuracy: 0.9268\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2589 - accuracy: 0.9264\n",
            "241/469 [==============>...............] - ETA: 4s - loss: 0.2586 - accuracy: 0.9262\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2586 - accuracy: 0.9261\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2584 - accuracy: 0.9260\n",
            "259/469 [===============>..............] - ETA: 4s - loss: 0.2584 - accuracy: 0.9261\n",
            "265/469 [===============>..............] - ETA: 3s - loss: 0.2594 - accuracy: 0.9257\n",
            "271/469 [================>.............] - ETA: 3s - loss: 0.2583 - accuracy: 0.9258\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2579 - accuracy: 0.9259\n",
            "280/469 [================>.............] - ETA: 3s - loss: 0.2570 - accuracy: 0.9263\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2555 - accuracy: 0.9265\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2537 - accuracy: 0.9268\n",
            "298/469 [==================>...........] - ETA: 3s - loss: 0.2530 - accuracy: 0.9270\n",
            "304/469 [==================>...........] - ETA: 3s - loss: 0.2534 - accuracy: 0.9269\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2537 - accuracy: 0.9268\n",
            "313/469 [===================>..........] - ETA: 2s - loss: 0.2538 - accuracy: 0.9270\n",
            "319/469 [===================>..........] - ETA: 2s - loss: 0.2530 - accuracy: 0.9271\n",
            "325/469 [===================>..........] - ETA: 2s - loss: 0.2527 - accuracy: 0.9271\n",
            "331/469 [====================>.........] - ETA: 2s - loss: 0.2530 - accuracy: 0.9268\n",
            "337/469 [====================>.........] - ETA: 2s - loss: 0.2536 - accuracy: 0.9265\n",
            "343/469 [====================>.........] - ETA: 2s - loss: 0.2543 - accuracy: 0.9264\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2534 - accuracy: 0.9265\n",
            "352/469 [=====================>........] - ETA: 2s - loss: 0.2537 - accuracy: 0.9265\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2545 - accuracy: 0.9263\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2551 - accuracy: 0.9262\n",
            "370/469 [======================>.......] - ETA: 1s - loss: 0.2539 - accuracy: 0.9264\n",
            "376/469 [=======================>......] - ETA: 1s - loss: 0.2528 - accuracy: 0.9266\n",
            "382/469 [=======================>......] - ETA: 1s - loss: 0.2518 - accuracy: 0.9268\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2518 - accuracy: 0.9268\n",
            "388/469 [=======================>......] - ETA: 1s - loss: 0.2520 - accuracy: 0.9267\n",
            "391/469 [========================>.....] - ETA: 1s - loss: 0.2522 - accuracy: 0.9267\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2522 - accuracy: 0.9268\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.2509 - accuracy: 0.9271\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2509 - accuracy: 0.9272\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.2507 - accuracy: 0.9273\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2506 - accuracy: 0.9274\n",
            "427/469 [==========================>...] - ETA: 0s - loss: 0.2503 - accuracy: 0.9274\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.2511 - accuracy: 0.9274\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2509 - accuracy: 0.9274\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2505 - accuracy: 0.9273\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.2509 - accuracy: 0.9271\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.9269\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9269\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9270\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.9269\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2522 - accuracy: 0.9269 - val_loss: 0.1556 - val_accuracy: 0.9607\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 8/12\n",
            "  4/469 [..............................] - ETA: 9s - loss: 0.2153 - accuracy: 0.9434\n",
            " 10/469 [..............................] - ETA: 8s - loss: 0.2211 - accuracy: 0.9359\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 0.2397 - accuracy: 0.9287\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 0.2340 - accuracy: 0.9305\n",
            " 25/469 [>.............................] - ETA: 8s - loss: 0.2295 - accuracy: 0.9325\n",
            " 31/469 [>.............................] - ETA: 8s - loss: 0.2313 - accuracy: 0.9322\n",
            " 37/469 [=>............................] - ETA: 8s - loss: 0.2307 - accuracy: 0.9318\n",
            " 43/469 [=>............................] - ETA: 8s - loss: 0.2330 - accuracy: 0.9322\n",
            " 49/469 [==>...........................] - ETA: 8s - loss: 0.2386 - accuracy: 0.9318\n",
            " 52/469 [==>...........................] - ETA: 8s - loss: 0.2404 - accuracy: 0.9315\n",
            " 55/469 [==>...........................] - ETA: 8s - loss: 0.2410 - accuracy: 0.9312\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 0.2433 - accuracy: 0.9313\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 0.2466 - accuracy: 0.9286\n",
            " 70/469 [===>..........................] - ETA: 7s - loss: 0.2463 - accuracy: 0.9269\n",
            " 76/469 [===>..........................] - ETA: 7s - loss: 0.2437 - accuracy: 0.9272\n",
            " 82/469 [====>.........................] - ETA: 7s - loss: 0.2512 - accuracy: 0.9267\n",
            " 88/469 [====>.........................] - ETA: 7s - loss: 0.2539 - accuracy: 0.9266\n",
            " 94/469 [=====>........................] - ETA: 7s - loss: 0.2593 - accuracy: 0.9260\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2603 - accuracy: 0.9256\n",
            "100/469 [=====>........................] - ETA: 7s - loss: 0.2607 - accuracy: 0.9256\n",
            "103/469 [=====>........................] - ETA: 7s - loss: 0.2589 - accuracy: 0.9260\n",
            "109/469 [=====>........................] - ETA: 6s - loss: 0.2583 - accuracy: 0.9253\n",
            "115/469 [======>.......................] - ETA: 6s - loss: 0.2585 - accuracy: 0.9250\n",
            "121/469 [======>.......................] - ETA: 6s - loss: 0.2592 - accuracy: 0.9249\n",
            "127/469 [=======>......................] - ETA: 6s - loss: 0.2618 - accuracy: 0.9245\n",
            "133/469 [=======>......................] - ETA: 6s - loss: 0.2604 - accuracy: 0.9248\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2593 - accuracy: 0.9251\n",
            "139/469 [=======>......................] - ETA: 6s - loss: 0.2587 - accuracy: 0.9254\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2589 - accuracy: 0.9252\n",
            "148/469 [========>.....................] - ETA: 6s - loss: 0.2566 - accuracy: 0.9262\n",
            "154/469 [========>.....................] - ETA: 6s - loss: 0.2575 - accuracy: 0.9262\n",
            "160/469 [=========>....................] - ETA: 5s - loss: 0.2549 - accuracy: 0.9268\n",
            "166/469 [=========>....................] - ETA: 5s - loss: 0.2545 - accuracy: 0.9262\n",
            "172/469 [==========>...................] - ETA: 5s - loss: 0.2533 - accuracy: 0.9261\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2540 - accuracy: 0.9262\n",
            "178/469 [==========>...................] - ETA: 5s - loss: 0.2528 - accuracy: 0.9264\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2523 - accuracy: 0.9264\n",
            "187/469 [==========>...................] - ETA: 5s - loss: 0.2518 - accuracy: 0.9266\n",
            "193/469 [===========>..................] - ETA: 5s - loss: 0.2506 - accuracy: 0.9267\n",
            "199/469 [===========>..................] - ETA: 5s - loss: 0.2491 - accuracy: 0.9271\n",
            "205/469 [============>.................] - ETA: 5s - loss: 0.2480 - accuracy: 0.9272\n",
            "211/469 [============>.................] - ETA: 4s - loss: 0.2468 - accuracy: 0.9278\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2466 - accuracy: 0.9278\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.2466 - accuracy: 0.9277\n",
            "226/469 [=============>................] - ETA: 4s - loss: 0.2468 - accuracy: 0.9278\n",
            "232/469 [=============>................] - ETA: 4s - loss: 0.2473 - accuracy: 0.9276\n",
            "238/469 [==============>...............] - ETA: 4s - loss: 0.2467 - accuracy: 0.9279\n",
            "244/469 [==============>...............] - ETA: 4s - loss: 0.2457 - accuracy: 0.9282\n",
            "250/469 [==============>...............] - ETA: 4s - loss: 0.2458 - accuracy: 0.9281\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2449 - accuracy: 0.9285\n",
            "259/469 [===============>..............] - ETA: 4s - loss: 0.2443 - accuracy: 0.9284\n",
            "265/469 [===============>..............] - ETA: 3s - loss: 0.2437 - accuracy: 0.9287\n",
            "271/469 [================>.............] - ETA: 3s - loss: 0.2417 - accuracy: 0.9293\n",
            "277/469 [================>.............] - ETA: 3s - loss: 0.2420 - accuracy: 0.9294\n",
            "283/469 [=================>............] - ETA: 3s - loss: 0.2413 - accuracy: 0.9295\n",
            "289/469 [=================>............] - ETA: 3s - loss: 0.2400 - accuracy: 0.9298\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2403 - accuracy: 0.9296\n",
            "298/469 [==================>...........] - ETA: 3s - loss: 0.2391 - accuracy: 0.9301\n",
            "304/469 [==================>...........] - ETA: 3s - loss: 0.2391 - accuracy: 0.9301\n",
            "310/469 [==================>...........] - ETA: 3s - loss: 0.2387 - accuracy: 0.9300\n",
            "316/469 [===================>..........] - ETA: 2s - loss: 0.2391 - accuracy: 0.9301\n",
            "322/469 [===================>..........] - ETA: 2s - loss: 0.2388 - accuracy: 0.9299\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2396 - accuracy: 0.9299\n",
            "331/469 [====================>.........] - ETA: 2s - loss: 0.2392 - accuracy: 0.9299\n",
            "337/469 [====================>.........] - ETA: 2s - loss: 0.2385 - accuracy: 0.9302\n",
            "343/469 [====================>.........] - ETA: 2s - loss: 0.2388 - accuracy: 0.9302\n",
            "349/469 [=====================>........] - ETA: 2s - loss: 0.2385 - accuracy: 0.9304\n",
            "355/469 [=====================>........] - ETA: 2s - loss: 0.2387 - accuracy: 0.9305\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2384 - accuracy: 0.9305\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2382 - accuracy: 0.9304\n",
            "370/469 [======================>.......] - ETA: 1s - loss: 0.2385 - accuracy: 0.9302\n",
            "376/469 [=======================>......] - ETA: 1s - loss: 0.2378 - accuracy: 0.9304\n",
            "382/469 [=======================>......] - ETA: 1s - loss: 0.2383 - accuracy: 0.9305\n",
            "388/469 [=======================>......] - ETA: 1s - loss: 0.2381 - accuracy: 0.9305\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.2380 - accuracy: 0.9306\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2376 - accuracy: 0.9306\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2375 - accuracy: 0.9305\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2378 - accuracy: 0.9305\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.2374 - accuracy: 0.9307\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2381 - accuracy: 0.9306\n",
            "427/469 [==========================>...] - ETA: 0s - loss: 0.2385 - accuracy: 0.9304\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 0.2381 - accuracy: 0.9304\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.2379 - accuracy: 0.9305\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2380 - accuracy: 0.9307\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2383 - accuracy: 0.9304\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9303\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.2389 - accuracy: 0.9304\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.9301\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9299\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2396 - accuracy: 0.9299 - val_loss: 0.1596 - val_accuracy: 0.9539\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 9/12\n",
            "  1/469 [..............................] - ETA: 9s - loss: 0.3234 - accuracy: 0.9141\n",
            "  7/469 [..............................] - ETA: 8s - loss: 0.2495 - accuracy: 0.9263\n",
            " 10/469 [..............................] - ETA: 8s - loss: 0.2428 - accuracy: 0.9258\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 0.2513 - accuracy: 0.9263\n",
            " 22/469 [>.............................] - ETA: 8s - loss: 0.2394 - accuracy: 0.9304\n",
            " 28/469 [>.............................] - ETA: 8s - loss: 0.2386 - accuracy: 0.9297\n",
            " 34/469 [=>............................] - ETA: 8s - loss: 0.2321 - accuracy: 0.9311\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.2339 - accuracy: 0.9299\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.2249 - accuracy: 0.9319\n",
            " 49/469 [==>...........................] - ETA: 7s - loss: 0.2216 - accuracy: 0.9334\n",
            " 52/469 [==>...........................] - ETA: 7s - loss: 0.2214 - accuracy: 0.9339\n",
            " 55/469 [==>...........................] - ETA: 7s - loss: 0.2234 - accuracy: 0.9331\n",
            " 61/469 [==>...........................] - ETA: 7s - loss: 0.2263 - accuracy: 0.9322\n",
            " 67/469 [===>..........................] - ETA: 7s - loss: 0.2265 - accuracy: 0.9321\n",
            " 73/469 [===>..........................] - ETA: 7s - loss: 0.2312 - accuracy: 0.9308\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.2294 - accuracy: 0.9312\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.2332 - accuracy: 0.9313\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.2339 - accuracy: 0.9312\n",
            " 94/469 [=====>........................] - ETA: 7s - loss: 0.2343 - accuracy: 0.9309\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2380 - accuracy: 0.9307\n",
            "100/469 [=====>........................] - ETA: 7s - loss: 0.2361 - accuracy: 0.9314\n",
            "103/469 [=====>........................] - ETA: 6s - loss: 0.2375 - accuracy: 0.9309\n",
            "106/469 [=====>........................] - ETA: 6s - loss: 0.2361 - accuracy: 0.9310\n",
            "112/469 [======>.......................] - ETA: 6s - loss: 0.2362 - accuracy: 0.9309\n",
            "118/469 [======>.......................] - ETA: 6s - loss: 0.2360 - accuracy: 0.9308\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2349 - accuracy: 0.9307\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2331 - accuracy: 0.9304\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2335 - accuracy: 0.9304\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2326 - accuracy: 0.9302\n",
            "145/469 [========>.....................] - ETA: 6s - loss: 0.2316 - accuracy: 0.9304\n",
            "151/469 [========>.....................] - ETA: 6s - loss: 0.2318 - accuracy: 0.9299\n",
            "157/469 [=========>....................] - ETA: 5s - loss: 0.2329 - accuracy: 0.9297\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2321 - accuracy: 0.9303\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.2333 - accuracy: 0.9298\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2343 - accuracy: 0.9296\n",
            "178/469 [==========>...................] - ETA: 5s - loss: 0.2337 - accuracy: 0.9298\n",
            "184/469 [==========>...................] - ETA: 5s - loss: 0.2333 - accuracy: 0.9298\n",
            "190/469 [===========>..................] - ETA: 5s - loss: 0.2314 - accuracy: 0.9302\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2305 - accuracy: 0.9303\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.2297 - accuracy: 0.9307\n",
            "208/469 [============>.................] - ETA: 4s - loss: 0.2299 - accuracy: 0.9308\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2301 - accuracy: 0.9307\n",
            "217/469 [============>.................] - ETA: 4s - loss: 0.2300 - accuracy: 0.9308\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.2297 - accuracy: 0.9310\n",
            "223/469 [=============>................] - ETA: 4s - loss: 0.2290 - accuracy: 0.9310\n",
            "229/469 [=============>................] - ETA: 4s - loss: 0.2287 - accuracy: 0.9312\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2292 - accuracy: 0.9311\n",
            "241/469 [==============>...............] - ETA: 4s - loss: 0.2313 - accuracy: 0.9310\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2343 - accuracy: 0.9306\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2358 - accuracy: 0.9305\n",
            "256/469 [===============>..............] - ETA: 4s - loss: 0.2360 - accuracy: 0.9305\n",
            "262/469 [===============>..............] - ETA: 3s - loss: 0.2362 - accuracy: 0.9303\n",
            "268/469 [================>.............] - ETA: 3s - loss: 0.2369 - accuracy: 0.9300\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2391 - accuracy: 0.9295\n",
            "280/469 [================>.............] - ETA: 3s - loss: 0.2388 - accuracy: 0.9296\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2395 - accuracy: 0.9294\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2391 - accuracy: 0.9294\n",
            "295/469 [=================>............] - ETA: 3s - loss: 0.2392 - accuracy: 0.9293\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2390 - accuracy: 0.9292\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2381 - accuracy: 0.9294\n",
            "313/469 [===================>..........] - ETA: 2s - loss: 0.2392 - accuracy: 0.9292\n",
            "319/469 [===================>..........] - ETA: 2s - loss: 0.2389 - accuracy: 0.9291\n",
            "325/469 [===================>..........] - ETA: 2s - loss: 0.2400 - accuracy: 0.9292\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2403 - accuracy: 0.9292\n",
            "331/469 [====================>.........] - ETA: 2s - loss: 0.2405 - accuracy: 0.9291\n",
            "334/469 [====================>.........] - ETA: 2s - loss: 0.2405 - accuracy: 0.9291\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2412 - accuracy: 0.9288\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2430 - accuracy: 0.9284\n",
            "352/469 [=====================>........] - ETA: 2s - loss: 0.2443 - accuracy: 0.9283\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2456 - accuracy: 0.9283\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2460 - accuracy: 0.9281\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2463 - accuracy: 0.9279\n",
            "370/469 [======================>.......] - ETA: 1s - loss: 0.2464 - accuracy: 0.9279\n",
            "373/469 [======================>.......] - ETA: 1s - loss: 0.2462 - accuracy: 0.9279\n",
            "376/469 [=======================>......] - ETA: 1s - loss: 0.2457 - accuracy: 0.9280\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2459 - accuracy: 0.9279\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2456 - accuracy: 0.9281\n",
            "391/469 [========================>.....] - ETA: 1s - loss: 0.2452 - accuracy: 0.9282\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2446 - accuracy: 0.9283\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.2451 - accuracy: 0.9281\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2458 - accuracy: 0.9282\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2464 - accuracy: 0.9280\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2472 - accuracy: 0.9277\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2485 - accuracy: 0.9274\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.2493 - accuracy: 0.9269\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.2493 - accuracy: 0.9268\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2505 - accuracy: 0.9266\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2513 - accuracy: 0.9266\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.2523 - accuracy: 0.9263\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2526 - accuracy: 0.9263\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.9263\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9264\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2517 - accuracy: 0.9264\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2511 - accuracy: 0.9265\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2511 - accuracy: 0.9265 - val_loss: 0.1444 - val_accuracy: 0.9603\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 10/12\n",
            "  4/469 [..............................] - ETA: 8s - loss: 0.2709 - accuracy: 0.9277\n",
            " 10/469 [..............................] - ETA: 8s - loss: 0.2704 - accuracy: 0.9273\n",
            " 13/469 [..............................] - ETA: 8s - loss: 0.2565 - accuracy: 0.9279\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 0.2405 - accuracy: 0.9317\n",
            " 25/469 [>.............................] - ETA: 8s - loss: 0.2298 - accuracy: 0.9334\n",
            " 31/469 [>.............................] - ETA: 8s - loss: 0.2311 - accuracy: 0.9340\n",
            " 37/469 [=>............................] - ETA: 8s - loss: 0.2355 - accuracy: 0.9345\n",
            " 43/469 [=>............................] - ETA: 8s - loss: 0.2403 - accuracy: 0.9333\n",
            " 49/469 [==>...........................] - ETA: 7s - loss: 0.2374 - accuracy: 0.9334\n",
            " 52/469 [==>...........................] - ETA: 7s - loss: 0.2307 - accuracy: 0.9348\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 0.2322 - accuracy: 0.9336\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 0.2318 - accuracy: 0.9325\n",
            " 70/469 [===>..........................] - ETA: 7s - loss: 0.2328 - accuracy: 0.9326\n",
            " 76/469 [===>..........................] - ETA: 7s - loss: 0.2310 - accuracy: 0.9323\n",
            " 82/469 [====>.........................] - ETA: 7s - loss: 0.2325 - accuracy: 0.9315\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.2314 - accuracy: 0.9312\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.2302 - accuracy: 0.9326\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2270 - accuracy: 0.9336\n",
            "103/469 [=====>........................] - ETA: 7s - loss: 0.2268 - accuracy: 0.9340\n",
            "109/469 [=====>........................] - ETA: 6s - loss: 0.2272 - accuracy: 0.9343\n",
            "115/469 [======>.......................] - ETA: 6s - loss: 0.2266 - accuracy: 0.9346\n",
            "118/469 [======>.......................] - ETA: 6s - loss: 0.2267 - accuracy: 0.9341\n",
            "121/469 [======>.......................] - ETA: 6s - loss: 0.2290 - accuracy: 0.9336\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2284 - accuracy: 0.9338\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2270 - accuracy: 0.9340\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2287 - accuracy: 0.9336\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2285 - accuracy: 0.9338\n",
            "148/469 [========>.....................] - ETA: 6s - loss: 0.2285 - accuracy: 0.9339\n",
            "151/469 [========>.....................] - ETA: 6s - loss: 0.2281 - accuracy: 0.9339\n",
            "157/469 [=========>....................] - ETA: 6s - loss: 0.2278 - accuracy: 0.9341\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2268 - accuracy: 0.9344\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.2265 - accuracy: 0.9345\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2260 - accuracy: 0.9344\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2249 - accuracy: 0.9350\n",
            "184/469 [==========>...................] - ETA: 5s - loss: 0.2247 - accuracy: 0.9349\n",
            "190/469 [===========>..................] - ETA: 5s - loss: 0.2251 - accuracy: 0.9346\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2270 - accuracy: 0.9343\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.2271 - accuracy: 0.9341\n",
            "208/469 [============>.................] - ETA: 5s - loss: 0.2275 - accuracy: 0.9337\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2278 - accuracy: 0.9336\n",
            "217/469 [============>.................] - ETA: 4s - loss: 0.2282 - accuracy: 0.9338\n",
            "223/469 [=============>................] - ETA: 4s - loss: 0.2285 - accuracy: 0.9338\n",
            "229/469 [=============>................] - ETA: 4s - loss: 0.2294 - accuracy: 0.9334\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2304 - accuracy: 0.9334\n",
            "241/469 [==============>...............] - ETA: 4s - loss: 0.2300 - accuracy: 0.9333\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2304 - accuracy: 0.9331\n",
            "250/469 [==============>...............] - ETA: 4s - loss: 0.2304 - accuracy: 0.9331\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2315 - accuracy: 0.9328\n",
            "256/469 [===============>..............] - ETA: 4s - loss: 0.2326 - accuracy: 0.9327\n",
            "262/469 [===============>..............] - ETA: 4s - loss: 0.2323 - accuracy: 0.9325\n",
            "268/469 [================>.............] - ETA: 3s - loss: 0.2329 - accuracy: 0.9325\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2337 - accuracy: 0.9325\n",
            "280/469 [================>.............] - ETA: 3s - loss: 0.2343 - accuracy: 0.9323\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2345 - accuracy: 0.9324\n",
            "289/469 [=================>............] - ETA: 3s - loss: 0.2345 - accuracy: 0.9323\n",
            "295/469 [=================>............] - ETA: 3s - loss: 0.2344 - accuracy: 0.9325\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2345 - accuracy: 0.9323\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2341 - accuracy: 0.9324\n",
            "313/469 [===================>..........] - ETA: 3s - loss: 0.2347 - accuracy: 0.9320\n",
            "319/469 [===================>..........] - ETA: 2s - loss: 0.2353 - accuracy: 0.9317\n",
            "322/469 [===================>..........] - ETA: 2s - loss: 0.2353 - accuracy: 0.9317\n",
            "325/469 [===================>..........] - ETA: 2s - loss: 0.2359 - accuracy: 0.9316\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2374 - accuracy: 0.9314\n",
            "334/469 [====================>.........] - ETA: 2s - loss: 0.2404 - accuracy: 0.9310\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2412 - accuracy: 0.9307\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2415 - accuracy: 0.9309\n",
            "352/469 [=====================>........] - ETA: 2s - loss: 0.2428 - accuracy: 0.9306\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2435 - accuracy: 0.9305\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2440 - accuracy: 0.9301\n",
            "370/469 [======================>.......] - ETA: 1s - loss: 0.2443 - accuracy: 0.9299\n",
            "376/469 [=======================>......] - ETA: 1s - loss: 0.2443 - accuracy: 0.9299\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2441 - accuracy: 0.9300\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2440 - accuracy: 0.9299\n",
            "391/469 [========================>.....] - ETA: 1s - loss: 0.2447 - accuracy: 0.9299\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2454 - accuracy: 0.9296\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.2450 - accuracy: 0.9296\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.2448 - accuracy: 0.9297\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2449 - accuracy: 0.9296\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2450 - accuracy: 0.9297\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.2449 - accuracy: 0.9295\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.2454 - accuracy: 0.9293\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2460 - accuracy: 0.9292\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2464 - accuracy: 0.9292\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2465 - accuracy: 0.9291\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2470 - accuracy: 0.9290\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9291\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9290\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9292\n",
            "469/469 [==============================] - 10s 20ms/step - loss: 0.2459 - accuracy: 0.9292 - val_loss: 0.1619 - val_accuracy: 0.9594\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 11/12\n",
            "  1/469 [..............................] - ETA: 8s - loss: 0.2365 - accuracy: 0.9297\n",
            "  7/469 [..............................] - ETA: 8s - loss: 0.2741 - accuracy: 0.9286\n",
            " 13/469 [..............................] - ETA: 8s - loss: 0.2451 - accuracy: 0.9297\n",
            " 16/469 [>.............................] - ETA: 8s - loss: 0.2422 - accuracy: 0.9307\n",
            " 22/469 [>.............................] - ETA: 8s - loss: 0.2363 - accuracy: 0.9347\n",
            " 28/469 [>.............................] - ETA: 8s - loss: 0.2252 - accuracy: 0.9358\n",
            " 34/469 [=>............................] - ETA: 8s - loss: 0.2288 - accuracy: 0.9366\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.2188 - accuracy: 0.9383\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.2285 - accuracy: 0.9344\n",
            " 49/469 [==>...........................] - ETA: 8s - loss: 0.2263 - accuracy: 0.9338\n",
            " 52/469 [==>...........................] - ETA: 8s - loss: 0.2284 - accuracy: 0.9328\n",
            " 55/469 [==>...........................] - ETA: 7s - loss: 0.2304 - accuracy: 0.9322\n",
            " 61/469 [==>...........................] - ETA: 7s - loss: 0.2312 - accuracy: 0.9315\n",
            " 67/469 [===>..........................] - ETA: 7s - loss: 0.2370 - accuracy: 0.9299\n",
            " 73/469 [===>..........................] - ETA: 7s - loss: 0.2350 - accuracy: 0.9310\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.2339 - accuracy: 0.9313\n",
            " 82/469 [====>.........................] - ETA: 7s - loss: 0.2341 - accuracy: 0.9315\n",
            " 88/469 [====>.........................] - ETA: 7s - loss: 0.2351 - accuracy: 0.9313\n",
            " 94/469 [=====>........................] - ETA: 7s - loss: 0.2325 - accuracy: 0.9322\n",
            "100/469 [=====>........................] - ETA: 7s - loss: 0.2316 - accuracy: 0.9319\n",
            "106/469 [=====>........................] - ETA: 7s - loss: 0.2316 - accuracy: 0.9321\n",
            "109/469 [=====>........................] - ETA: 6s - loss: 0.2289 - accuracy: 0.9329\n",
            "115/469 [======>.......................] - ETA: 6s - loss: 0.2254 - accuracy: 0.9337\n",
            "121/469 [======>.......................] - ETA: 6s - loss: 0.2222 - accuracy: 0.9348\n",
            "127/469 [=======>......................] - ETA: 6s - loss: 0.2240 - accuracy: 0.9342\n",
            "133/469 [=======>......................] - ETA: 6s - loss: 0.2239 - accuracy: 0.9345\n",
            "139/469 [=======>......................] - ETA: 6s - loss: 0.2234 - accuracy: 0.9344\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2236 - accuracy: 0.9344\n",
            "148/469 [========>.....................] - ETA: 6s - loss: 0.2222 - accuracy: 0.9350\n",
            "154/469 [========>.....................] - ETA: 6s - loss: 0.2237 - accuracy: 0.9345\n",
            "160/469 [=========>....................] - ETA: 5s - loss: 0.2268 - accuracy: 0.9340\n",
            "166/469 [=========>....................] - ETA: 5s - loss: 0.2264 - accuracy: 0.9339\n",
            "172/469 [==========>...................] - ETA: 5s - loss: 0.2279 - accuracy: 0.9336\n",
            "178/469 [==========>...................] - ETA: 5s - loss: 0.2286 - accuracy: 0.9335\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2314 - accuracy: 0.9329\n",
            "187/469 [==========>...................] - ETA: 5s - loss: 0.2315 - accuracy: 0.9327\n",
            "193/469 [===========>..................] - ETA: 5s - loss: 0.2330 - accuracy: 0.9322\n",
            "199/469 [===========>..................] - ETA: 5s - loss: 0.2318 - accuracy: 0.9326\n",
            "205/469 [============>.................] - ETA: 5s - loss: 0.2324 - accuracy: 0.9324\n",
            "208/469 [============>.................] - ETA: 5s - loss: 0.2333 - accuracy: 0.9321\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2324 - accuracy: 0.9323\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.2317 - accuracy: 0.9324\n",
            "226/469 [=============>................] - ETA: 4s - loss: 0.2310 - accuracy: 0.9325\n",
            "232/469 [=============>................] - ETA: 4s - loss: 0.2301 - accuracy: 0.9328\n",
            "235/469 [==============>...............] - ETA: 4s - loss: 0.2293 - accuracy: 0.9330\n",
            "241/469 [==============>...............] - ETA: 4s - loss: 0.2285 - accuracy: 0.9334\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2285 - accuracy: 0.9332\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2300 - accuracy: 0.9327\n",
            "259/469 [===============>..............] - ETA: 4s - loss: 0.2293 - accuracy: 0.9329\n",
            "262/469 [===============>..............] - ETA: 4s - loss: 0.2300 - accuracy: 0.9330\n",
            "265/469 [===============>..............] - ETA: 3s - loss: 0.2299 - accuracy: 0.9331\n",
            "268/469 [================>.............] - ETA: 3s - loss: 0.2295 - accuracy: 0.9331\n",
            "274/469 [================>.............] - ETA: 3s - loss: 0.2292 - accuracy: 0.9331\n",
            "280/469 [================>.............] - ETA: 3s - loss: 0.2294 - accuracy: 0.9331\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2284 - accuracy: 0.9334\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2281 - accuracy: 0.9334\n",
            "295/469 [=================>............] - ETA: 3s - loss: 0.2282 - accuracy: 0.9334\n",
            "301/469 [==================>...........] - ETA: 3s - loss: 0.2277 - accuracy: 0.9336\n",
            "307/469 [==================>...........] - ETA: 3s - loss: 0.2290 - accuracy: 0.9333\n",
            "313/469 [===================>..........] - ETA: 3s - loss: 0.2282 - accuracy: 0.9335\n",
            "319/469 [===================>..........] - ETA: 2s - loss: 0.2271 - accuracy: 0.9338\n",
            "325/469 [===================>..........] - ETA: 2s - loss: 0.2277 - accuracy: 0.9336\n",
            "328/469 [===================>..........] - ETA: 2s - loss: 0.2275 - accuracy: 0.9335\n",
            "334/469 [====================>.........] - ETA: 2s - loss: 0.2281 - accuracy: 0.9336\n",
            "340/469 [====================>.........] - ETA: 2s - loss: 0.2274 - accuracy: 0.9338\n",
            "346/469 [=====================>........] - ETA: 2s - loss: 0.2266 - accuracy: 0.9338\n",
            "352/469 [=====================>........] - ETA: 2s - loss: 0.2266 - accuracy: 0.9337\n",
            "358/469 [=====================>........] - ETA: 2s - loss: 0.2268 - accuracy: 0.9336\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2265 - accuracy: 0.9337\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2263 - accuracy: 0.9337\n",
            "373/469 [======================>.......] - ETA: 1s - loss: 0.2274 - accuracy: 0.9335\n",
            "379/469 [=======================>......] - ETA: 1s - loss: 0.2279 - accuracy: 0.9334\n",
            "385/469 [=======================>......] - ETA: 1s - loss: 0.2284 - accuracy: 0.9334\n",
            "391/469 [========================>.....] - ETA: 1s - loss: 0.2282 - accuracy: 0.9333\n",
            "397/469 [========================>.....] - ETA: 1s - loss: 0.2291 - accuracy: 0.9333\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2290 - accuracy: 0.9333\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.2283 - accuracy: 0.9336\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2277 - accuracy: 0.9337\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2280 - accuracy: 0.9337\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.2285 - accuracy: 0.9334\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.2282 - accuracy: 0.9334\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.2285 - accuracy: 0.9334\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2279 - accuracy: 0.9337\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.2285 - accuracy: 0.9334\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2285 - accuracy: 0.9334\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.2289 - accuracy: 0.9333\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2287 - accuracy: 0.9334\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9336\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9336\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2277 - accuracy: 0.9335\n",
            "469/469 [==============================] - 10s 20ms/step - loss: 0.2277 - accuracy: 0.9335 - val_loss: 0.1540 - val_accuracy: 0.9612\n",
            "\u001b[36m(train_mnist pid=3298881)\u001b[0m Epoch 12/12\n",
            "  4/469 [..............................] - ETA: 8s - loss: 0.2880 - accuracy: 0.9219\n",
            "  7/469 [..............................] - ETA: 8s - loss: 0.2449 - accuracy: 0.9252\n",
            " 13/469 [..............................] - ETA: 8s - loss: 0.2354 - accuracy: 0.9273\n",
            " 19/469 [>.............................] - ETA: 8s - loss: 0.2486 - accuracy: 0.9211\n",
            " 25/469 [>.............................] - ETA: 8s - loss: 0.2361 - accuracy: 0.9272\n",
            " 31/469 [>.............................] - ETA: 8s - loss: 0.2291 - accuracy: 0.9307\n",
            " 37/469 [=>............................] - ETA: 8s - loss: 0.2364 - accuracy: 0.9278\n",
            " 40/469 [=>............................] - ETA: 8s - loss: 0.2378 - accuracy: 0.9279\n",
            " 43/469 [=>............................] - ETA: 8s - loss: 0.2400 - accuracy: 0.9284\n",
            " 46/469 [=>............................] - ETA: 8s - loss: 0.2357 - accuracy: 0.9304\n",
            " 52/469 [==>...........................] - ETA: 8s - loss: 0.2305 - accuracy: 0.9309\n",
            " 58/469 [==>...........................] - ETA: 7s - loss: 0.2334 - accuracy: 0.9293\n",
            " 64/469 [===>..........................] - ETA: 7s - loss: 0.2288 - accuracy: 0.9307\n",
            " 70/469 [===>..........................] - ETA: 7s - loss: 0.2272 - accuracy: 0.9325\n",
            " 76/469 [===>..........................] - ETA: 7s - loss: 0.2278 - accuracy: 0.9332\n",
            " 79/469 [====>.........................] - ETA: 7s - loss: 0.2283 - accuracy: 0.9328\n",
            " 82/469 [====>.........................] - ETA: 7s - loss: 0.2255 - accuracy: 0.9333\n",
            " 85/469 [====>.........................] - ETA: 7s - loss: 0.2235 - accuracy: 0.9335\n",
            " 91/469 [====>.........................] - ETA: 7s - loss: 0.2231 - accuracy: 0.9339\n",
            " 97/469 [=====>........................] - ETA: 7s - loss: 0.2213 - accuracy: 0.9345\n",
            "103/469 [=====>........................] - ETA: 7s - loss: 0.2229 - accuracy: 0.9338\n",
            "109/469 [=====>........................] - ETA: 6s - loss: 0.2193 - accuracy: 0.9347\n",
            "115/469 [======>.......................] - ETA: 6s - loss: 0.2216 - accuracy: 0.9346\n",
            "121/469 [======>.......................] - ETA: 6s - loss: 0.2209 - accuracy: 0.9349\n",
            "124/469 [======>.......................] - ETA: 6s - loss: 0.2210 - accuracy: 0.9347\n",
            "130/469 [=======>......................] - ETA: 6s - loss: 0.2185 - accuracy: 0.9354\n",
            "136/469 [=======>......................] - ETA: 6s - loss: 0.2176 - accuracy: 0.9364\n",
            "142/469 [========>.....................] - ETA: 6s - loss: 0.2162 - accuracy: 0.9365\n",
            "148/469 [========>.....................] - ETA: 6s - loss: 0.2162 - accuracy: 0.9364\n",
            "154/469 [========>.....................] - ETA: 6s - loss: 0.2139 - accuracy: 0.9367\n",
            "160/469 [=========>....................] - ETA: 5s - loss: 0.2141 - accuracy: 0.9368\n",
            "163/469 [=========>....................] - ETA: 5s - loss: 0.2148 - accuracy: 0.9366\n",
            "169/469 [=========>....................] - ETA: 5s - loss: 0.2148 - accuracy: 0.9364\n",
            "175/469 [==========>...................] - ETA: 5s - loss: 0.2154 - accuracy: 0.9364\n",
            "181/469 [==========>...................] - ETA: 5s - loss: 0.2158 - accuracy: 0.9364\n",
            "187/469 [==========>...................] - ETA: 5s - loss: 0.2147 - accuracy: 0.9365\n",
            "193/469 [===========>..................] - ETA: 5s - loss: 0.2148 - accuracy: 0.9364\n",
            "196/469 [===========>..................] - ETA: 5s - loss: 0.2142 - accuracy: 0.9365\n",
            "199/469 [===========>..................] - ETA: 5s - loss: 0.2157 - accuracy: 0.9363\n",
            "202/469 [===========>..................] - ETA: 5s - loss: 0.2157 - accuracy: 0.9361\n",
            "208/469 [============>.................] - ETA: 5s - loss: 0.2163 - accuracy: 0.9363\n",
            "214/469 [============>.................] - ETA: 4s - loss: 0.2173 - accuracy: 0.9360\n",
            "220/469 [=============>................] - ETA: 4s - loss: 0.2200 - accuracy: 0.9354\n",
            "226/469 [=============>................] - ETA: 4s - loss: 0.2212 - accuracy: 0.9354\n",
            "232/469 [=============>................] - ETA: 4s - loss: 0.2228 - accuracy: 0.9349\n",
            "238/469 [==============>...............] - ETA: 4s - loss: 0.2230 - accuracy: 0.9348\n",
            "244/469 [==============>...............] - ETA: 4s - loss: 0.2239 - accuracy: 0.9347\n",
            "247/469 [==============>...............] - ETA: 4s - loss: 0.2255 - accuracy: 0.9345\n",
            "253/469 [===============>..............] - ETA: 4s - loss: 0.2251 - accuracy: 0.9342\n",
            "259/469 [===============>..............] - ETA: 4s - loss: 0.2248 - accuracy: 0.9342\n",
            "265/469 [===============>..............] - ETA: 3s - loss: 0.2257 - accuracy: 0.9343\n",
            "271/469 [================>.............] - ETA: 3s - loss: 0.2256 - accuracy: 0.9343\n",
            "277/469 [================>.............] - ETA: 3s - loss: 0.2268 - accuracy: 0.9339\n",
            "283/469 [=================>............] - ETA: 3s - loss: 0.2267 - accuracy: 0.9338\n",
            "286/469 [=================>............] - ETA: 3s - loss: 0.2267 - accuracy: 0.9337\n",
            "292/469 [=================>............] - ETA: 3s - loss: 0.2267 - accuracy: 0.9339\n",
            "298/469 [==================>...........] - ETA: 3s - loss: 0.2284 - accuracy: 0.9335\n",
            "304/469 [==================>...........] - ETA: 3s - loss: 0.2285 - accuracy: 0.9337\n",
            "310/469 [==================>...........] - ETA: 3s - loss: 0.2290 - accuracy: 0.9337\n",
            "316/469 [===================>..........] - ETA: 2s - loss: 0.2302 - accuracy: 0.9335\n",
            "322/469 [===================>..........] - ETA: 2s - loss: 0.2293 - accuracy: 0.9338\n",
            "325/469 [===================>..........] - ETA: 2s - loss: 0.2293 - accuracy: 0.9338\n",
            "331/469 [====================>.........] - ETA: 2s - loss: 0.2298 - accuracy: 0.9337\n",
            "337/469 [====================>.........] - ETA: 2s - loss: 0.2312 - accuracy: 0.9335\n",
            "343/469 [====================>.........] - ETA: 2s - loss: 0.2310 - accuracy: 0.9334\n",
            "349/469 [=====================>........] - ETA: 2s - loss: 0.2322 - accuracy: 0.9331\n",
            "355/469 [=====================>........] - ETA: 2s - loss: 0.2320 - accuracy: 0.9331\n",
            "361/469 [======================>.......] - ETA: 2s - loss: 0.2318 - accuracy: 0.9333\n",
            "364/469 [======================>.......] - ETA: 2s - loss: 0.2317 - accuracy: 0.9333\n",
            "367/469 [======================>.......] - ETA: 1s - loss: 0.2312 - accuracy: 0.9334\n",
            "370/469 [======================>.......] - ETA: 1s - loss: 0.2315 - accuracy: 0.9333\n",
            "376/469 [=======================>......] - ETA: 1s - loss: 0.2307 - accuracy: 0.9336\n",
            "382/469 [=======================>......] - ETA: 1s - loss: 0.2302 - accuracy: 0.9338\n",
            "388/469 [=======================>......] - ETA: 1s - loss: 0.2295 - accuracy: 0.9338\n",
            "394/469 [========================>.....] - ETA: 1s - loss: 0.2299 - accuracy: 0.9338\n",
            "400/469 [========================>.....] - ETA: 1s - loss: 0.2294 - accuracy: 0.9338\n",
            "406/469 [========================>.....] - ETA: 1s - loss: 0.2289 - accuracy: 0.9340\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.2289 - accuracy: 0.9340\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.2283 - accuracy: 0.9340\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2279 - accuracy: 0.9340\n",
            "427/469 [==========================>...] - ETA: 0s - loss: 0.2281 - accuracy: 0.9341\n",
            "433/469 [==========================>...] - ETA: 0s - loss: 0.2283 - accuracy: 0.9340\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.2279 - accuracy: 0.9342\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2280 - accuracy: 0.9342\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2279 - accuracy: 0.9343\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9344\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9341\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2282 - accuracy: 0.9342\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9343\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.2278 - accuracy: 0.9343 - val_loss: 0.1549 - val_accuracy: 0.9616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3299519)\u001b[0m 2023-12-05 01:50:14.053198: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3299519)\u001b[0m 2023-12-05 01:50:14.100512: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3299519)\u001b[0m 2023-12-05 01:50:14.101004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3299519)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3299519)\u001b[0m 2023-12-05 01:50:14.977866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3299519)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3299519)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3299519)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3299519)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3299519)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3299519)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3299519)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3299519)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m 2023-12-05 01:50:16.622381: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 1/12\n",
            "  6/469 [..............................] - ETA: 5s - loss: 10.1741 - accuracy: 0.2240 \n",
            " 16/469 [>.............................] - ETA: 5s - loss: 4.6836 - accuracy: 0.4214\n",
            " 26/469 [>.............................] - ETA: 4s - loss: 3.2032 - accuracy: 0.5385\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 2.4880 - accuracy: 0.6118\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 2.0875 - accuracy: 0.6532\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 1.8194 - accuracy: 0.6790\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 1.6299 - accuracy: 0.6998\n",
            " 76/469 [===>..........................] - ETA: 4s - loss: 1.4888 - accuracy: 0.7161\n",
            " 86/469 [====>.........................] - ETA: 4s - loss: 1.3690 - accuracy: 0.7298\n",
            " 96/469 [=====>........................] - ETA: 4s - loss: 1.2734 - accuracy: 0.7424\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 1.1968 - accuracy: 0.7521\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 1.1290 - accuracy: 0.7617\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 1.0738 - accuracy: 0.7700\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 1.0275 - accuracy: 0.7765\n",
            "146/469 [========>.....................] - ETA: 3s - loss: 0.9839 - accuracy: 0.7830\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.9447 - accuracy: 0.7889\n",
            "166/469 [=========>....................] - ETA: 3s - loss: 0.9135 - accuracy: 0.7941\n",
            "176/469 [==========>...................] - ETA: 3s - loss: 0.8845 - accuracy: 0.7985\n",
            "186/469 [==========>...................] - ETA: 3s - loss: 0.8568 - accuracy: 0.8028\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.8319 - accuracy: 0.8070\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.8124 - accuracy: 0.8103\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.7911 - accuracy: 0.8141\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.7715 - accuracy: 0.8173\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.7564 - accuracy: 0.8196\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.7387 - accuracy: 0.8226\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.7233 - accuracy: 0.8254\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.7097 - accuracy: 0.8279\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.6969 - accuracy: 0.8299\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.6853 - accuracy: 0.8316\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.6746 - accuracy: 0.8332\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.6645 - accuracy: 0.8349\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.6541 - accuracy: 0.8368\n",
            "326/469 [===================>..........] - ETA: 1s - loss: 0.6443 - accuracy: 0.8384\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.6349 - accuracy: 0.8403\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.6248 - accuracy: 0.8423\n",
            "356/469 [=====================>........] - ETA: 1s - loss: 0.6164 - accuracy: 0.8440\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.6084 - accuracy: 0.8456\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.6008 - accuracy: 0.8469\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.5931 - accuracy: 0.8485\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.5858 - accuracy: 0.8500\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.5791 - accuracy: 0.8514\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.5735 - accuracy: 0.8525\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.5674 - accuracy: 0.8535\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.5610 - accuracy: 0.8548\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.5552 - accuracy: 0.8562\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.5494 - accuracy: 0.8575\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.5445 - accuracy: 0.8586\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.5429 - accuracy: 0.8589 - val_loss: 0.2432 - val_accuracy: 0.9241\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 2/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.3269 - accuracy: 0.8984\n",
            " 11/469 [..............................] - ETA: 4s - loss: 0.3051 - accuracy: 0.9077\n",
            " 21/469 [>.............................] - ETA: 4s - loss: 0.3206 - accuracy: 0.9025\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.3227 - accuracy: 0.9007\n",
            " 41/469 [=>............................] - ETA: 4s - loss: 0.3175 - accuracy: 0.9024\n",
            " 51/469 [==>...........................] - ETA: 4s - loss: 0.3253 - accuracy: 0.9012\n",
            " 61/469 [==>...........................] - ETA: 4s - loss: 0.3251 - accuracy: 0.9000\n",
            " 71/469 [===>..........................] - ETA: 4s - loss: 0.3234 - accuracy: 0.8989\n",
            " 81/469 [====>.........................] - ETA: 4s - loss: 0.3164 - accuracy: 0.9018\n",
            " 91/469 [====>.........................] - ETA: 4s - loss: 0.3150 - accuracy: 0.9026\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.3081 - accuracy: 0.9040\n",
            "111/469 [======>.......................] - ETA: 3s - loss: 0.3030 - accuracy: 0.9053\n",
            "121/469 [======>.......................] - ETA: 3s - loss: 0.3030 - accuracy: 0.9059\n",
            "131/469 [=======>......................] - ETA: 3s - loss: 0.3019 - accuracy: 0.9060\n",
            "141/469 [========>.....................] - ETA: 3s - loss: 0.2995 - accuracy: 0.9066\n",
            "151/469 [========>.....................] - ETA: 3s - loss: 0.2979 - accuracy: 0.9067\n",
            "161/469 [=========>....................] - ETA: 3s - loss: 0.2951 - accuracy: 0.9075\n",
            "171/469 [=========>....................] - ETA: 3s - loss: 0.2902 - accuracy: 0.9092\n",
            "181/469 [==========>...................] - ETA: 3s - loss: 0.2892 - accuracy: 0.9096\n",
            "191/469 [===========>..................] - ETA: 2s - loss: 0.2892 - accuracy: 0.9098\n",
            "201/469 [===========>..................] - ETA: 2s - loss: 0.2904 - accuracy: 0.9091\n",
            "211/469 [============>.................] - ETA: 2s - loss: 0.2915 - accuracy: 0.9091\n",
            "221/469 [=============>................] - ETA: 2s - loss: 0.2895 - accuracy: 0.9102\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.2894 - accuracy: 0.9106\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.2898 - accuracy: 0.9109\n",
            "251/469 [===============>..............] - ETA: 2s - loss: 0.2885 - accuracy: 0.9114\n",
            "261/469 [===============>..............] - ETA: 2s - loss: 0.2894 - accuracy: 0.9113\n",
            "271/469 [================>.............] - ETA: 2s - loss: 0.2897 - accuracy: 0.9114\n",
            "281/469 [================>.............] - ETA: 1s - loss: 0.2903 - accuracy: 0.9109\n",
            "291/469 [=================>............] - ETA: 1s - loss: 0.2892 - accuracy: 0.9113\n",
            "301/469 [==================>...........] - ETA: 1s - loss: 0.2905 - accuracy: 0.9113\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.2893 - accuracy: 0.9116\n",
            "321/469 [===================>..........] - ETA: 1s - loss: 0.2900 - accuracy: 0.9115\n",
            "331/469 [====================>.........] - ETA: 1s - loss: 0.2899 - accuracy: 0.9117\n",
            "341/469 [====================>.........] - ETA: 1s - loss: 0.2895 - accuracy: 0.9116\n",
            "351/469 [=====================>........] - ETA: 1s - loss: 0.2897 - accuracy: 0.9117\n",
            "361/469 [======================>.......] - ETA: 1s - loss: 0.2908 - accuracy: 0.9116\n",
            "371/469 [======================>.......] - ETA: 1s - loss: 0.2917 - accuracy: 0.9114\n",
            "381/469 [=======================>......] - ETA: 0s - loss: 0.2900 - accuracy: 0.9116\n",
            "391/469 [========================>.....] - ETA: 0s - loss: 0.2886 - accuracy: 0.9120\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.2888 - accuracy: 0.9120\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.2880 - accuracy: 0.9123\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2869 - accuracy: 0.9125\n",
            "431/469 [==========================>...] - ETA: 0s - loss: 0.2866 - accuracy: 0.9126\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.2862 - accuracy: 0.9126\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2865 - accuracy: 0.9126\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9124\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.9125\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2865 - accuracy: 0.9126 - val_loss: 0.1832 - val_accuracy: 0.9428\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 3/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.2244 - accuracy: 0.9297\n",
            " 11/469 [..............................] - ETA: 5s - loss: 0.2680 - accuracy: 0.9212\n",
            " 21/469 [>.............................] - ETA: 4s - loss: 0.2472 - accuracy: 0.9249\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.2385 - accuracy: 0.9262\n",
            " 41/469 [=>............................] - ETA: 4s - loss: 0.2385 - accuracy: 0.9268\n",
            " 51/469 [==>...........................] - ETA: 4s - loss: 0.2407 - accuracy: 0.9262\n",
            " 61/469 [==>...........................] - ETA: 4s - loss: 0.2431 - accuracy: 0.9252\n",
            " 71/469 [===>..........................] - ETA: 4s - loss: 0.2461 - accuracy: 0.9245\n",
            " 81/469 [====>.........................] - ETA: 4s - loss: 0.2451 - accuracy: 0.9250\n",
            " 91/469 [====>.........................] - ETA: 4s - loss: 0.2460 - accuracy: 0.9245\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.2494 - accuracy: 0.9242\n",
            "111/469 [======>.......................] - ETA: 3s - loss: 0.2506 - accuracy: 0.9236\n",
            "121/469 [======>.......................] - ETA: 3s - loss: 0.2490 - accuracy: 0.9242\n",
            "131/469 [=======>......................] - ETA: 3s - loss: 0.2481 - accuracy: 0.9246\n",
            "141/469 [========>.....................] - ETA: 3s - loss: 0.2488 - accuracy: 0.9242\n",
            "151/469 [========>.....................] - ETA: 3s - loss: 0.2499 - accuracy: 0.9243\n",
            "161/469 [=========>....................] - ETA: 3s - loss: 0.2501 - accuracy: 0.9241\n",
            "171/469 [=========>....................] - ETA: 3s - loss: 0.2477 - accuracy: 0.9248\n",
            "181/469 [==========>...................] - ETA: 3s - loss: 0.2478 - accuracy: 0.9248\n",
            "191/469 [===========>..................] - ETA: 2s - loss: 0.2487 - accuracy: 0.9251\n",
            "201/469 [===========>..................] - ETA: 2s - loss: 0.2487 - accuracy: 0.9250\n",
            "211/469 [============>.................] - ETA: 2s - loss: 0.2514 - accuracy: 0.9246\n",
            "221/469 [=============>................] - ETA: 2s - loss: 0.2515 - accuracy: 0.9246\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.2501 - accuracy: 0.9251\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.2492 - accuracy: 0.9252\n",
            "251/469 [===============>..............] - ETA: 2s - loss: 0.2481 - accuracy: 0.9254\n",
            "261/469 [===============>..............] - ETA: 2s - loss: 0.2476 - accuracy: 0.9256\n",
            "271/469 [================>.............] - ETA: 2s - loss: 0.2463 - accuracy: 0.9260\n",
            "281/469 [================>.............] - ETA: 2s - loss: 0.2466 - accuracy: 0.9260\n",
            "291/469 [=================>............] - ETA: 1s - loss: 0.2468 - accuracy: 0.9258\n",
            "301/469 [==================>...........] - ETA: 1s - loss: 0.2482 - accuracy: 0.9256\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.2492 - accuracy: 0.9251\n",
            "321/469 [===================>..........] - ETA: 1s - loss: 0.2497 - accuracy: 0.9249\n",
            "331/469 [====================>.........] - ETA: 1s - loss: 0.2507 - accuracy: 0.9244\n",
            "341/469 [====================>.........] - ETA: 1s - loss: 0.2513 - accuracy: 0.9242\n",
            "351/469 [=====================>........] - ETA: 1s - loss: 0.2511 - accuracy: 0.9240\n",
            "361/469 [======================>.......] - ETA: 1s - loss: 0.2500 - accuracy: 0.9243\n",
            "371/469 [======================>.......] - ETA: 1s - loss: 0.2500 - accuracy: 0.9243\n",
            "381/469 [=======================>......] - ETA: 0s - loss: 0.2495 - accuracy: 0.9244\n",
            "391/469 [========================>.....] - ETA: 0s - loss: 0.2497 - accuracy: 0.9243\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.2497 - accuracy: 0.9240\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.2494 - accuracy: 0.9240\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2491 - accuracy: 0.9238\n",
            "431/469 [==========================>...] - ETA: 0s - loss: 0.2498 - accuracy: 0.9236\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.2495 - accuracy: 0.9236\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2493 - accuracy: 0.9237\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2479 - accuracy: 0.9241\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2481 - accuracy: 0.9243\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2482 - accuracy: 0.9241 - val_loss: 0.1615 - val_accuracy: 0.9522\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 4/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.1830 - accuracy: 0.9375\n",
            "  6/469 [..............................] - ETA: 4s - loss: 0.2164 - accuracy: 0.9323\n",
            " 16/469 [>.............................] - ETA: 4s - loss: 0.2236 - accuracy: 0.9282\n",
            " 26/469 [>.............................] - ETA: 4s - loss: 0.2298 - accuracy: 0.9288\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.2266 - accuracy: 0.9299\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.2249 - accuracy: 0.9316\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 0.2218 - accuracy: 0.9324\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 0.2251 - accuracy: 0.9316\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 0.2327 - accuracy: 0.9293\n",
            " 76/469 [===>..........................] - ETA: 4s - loss: 0.2377 - accuracy: 0.9288\n",
            " 81/469 [====>.........................] - ETA: 4s - loss: 0.2382 - accuracy: 0.9292\n",
            " 86/469 [====>.........................] - ETA: 4s - loss: 0.2338 - accuracy: 0.9300\n",
            " 96/469 [=====>........................] - ETA: 3s - loss: 0.2340 - accuracy: 0.9299\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.2312 - accuracy: 0.9299\n",
            "111/469 [======>.......................] - ETA: 3s - loss: 0.2305 - accuracy: 0.9301\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.2293 - accuracy: 0.9304\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.2282 - accuracy: 0.9301\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.2243 - accuracy: 0.9314\n",
            "146/469 [========>.....................] - ETA: 3s - loss: 0.2211 - accuracy: 0.9324\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.2211 - accuracy: 0.9323\n",
            "166/469 [=========>....................] - ETA: 3s - loss: 0.2181 - accuracy: 0.9331\n",
            "176/469 [==========>...................] - ETA: 3s - loss: 0.2184 - accuracy: 0.9327\n",
            "186/469 [==========>...................] - ETA: 3s - loss: 0.2177 - accuracy: 0.9329\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.2182 - accuracy: 0.9325\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.2207 - accuracy: 0.9315\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.2215 - accuracy: 0.9311\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2224 - accuracy: 0.9310\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.2239 - accuracy: 0.9306\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.2250 - accuracy: 0.9303\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.2255 - accuracy: 0.9301\n",
            "251/469 [===============>..............] - ETA: 2s - loss: 0.2248 - accuracy: 0.9303\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.2268 - accuracy: 0.9297\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.2278 - accuracy: 0.9294\n",
            "271/469 [================>.............] - ETA: 2s - loss: 0.2275 - accuracy: 0.9294\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.2276 - accuracy: 0.9293\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.2269 - accuracy: 0.9297\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.2273 - accuracy: 0.9295\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.2289 - accuracy: 0.9296\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.2286 - accuracy: 0.9297\n",
            "326/469 [===================>..........] - ETA: 1s - loss: 0.2283 - accuracy: 0.9297\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.2293 - accuracy: 0.9294\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.2308 - accuracy: 0.9293\n",
            "356/469 [=====================>........] - ETA: 1s - loss: 0.2318 - accuracy: 0.9291\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.2325 - accuracy: 0.9289\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.2336 - accuracy: 0.9283\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.2339 - accuracy: 0.9281\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.2327 - accuracy: 0.9284\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.2331 - accuracy: 0.9282\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.2333 - accuracy: 0.9281\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.2341 - accuracy: 0.9279\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2346 - accuracy: 0.9277\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2359 - accuracy: 0.9276\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9273\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2370 - accuracy: 0.9273\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2371 - accuracy: 0.9273\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2375 - accuracy: 0.9272 - val_loss: 0.1606 - val_accuracy: 0.9527\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 5/12\n",
            "  6/469 [..............................] - ETA: 4s - loss: 0.2421 - accuracy: 0.9310\n",
            " 16/469 [>.............................] - ETA: 4s - loss: 0.2239 - accuracy: 0.9331\n",
            " 26/469 [>.............................] - ETA: 4s - loss: 0.2328 - accuracy: 0.9294\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.2352 - accuracy: 0.9271\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 0.2293 - accuracy: 0.9297\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 0.2224 - accuracy: 0.9316\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 0.2234 - accuracy: 0.9323\n",
            " 76/469 [===>..........................] - ETA: 4s - loss: 0.2211 - accuracy: 0.9336\n",
            " 86/469 [====>.........................] - ETA: 4s - loss: 0.2221 - accuracy: 0.9340\n",
            " 96/469 [=====>........................] - ETA: 3s - loss: 0.2283 - accuracy: 0.9329\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.2335 - accuracy: 0.9317\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.2342 - accuracy: 0.9314\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.2320 - accuracy: 0.9319\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.2320 - accuracy: 0.9325\n",
            "146/469 [========>.....................] - ETA: 3s - loss: 0.2322 - accuracy: 0.9320\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.2318 - accuracy: 0.9312\n",
            "166/469 [=========>....................] - ETA: 3s - loss: 0.2317 - accuracy: 0.9306\n",
            "176/469 [==========>...................] - ETA: 3s - loss: 0.2303 - accuracy: 0.9305\n",
            "186/469 [==========>...................] - ETA: 3s - loss: 0.2297 - accuracy: 0.9311\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.2293 - accuracy: 0.9310\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.2279 - accuracy: 0.9313\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.2279 - accuracy: 0.9310\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2282 - accuracy: 0.9312\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.2266 - accuracy: 0.9317\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.2259 - accuracy: 0.9322\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.2259 - accuracy: 0.9323\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.2271 - accuracy: 0.9318\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.2271 - accuracy: 0.9319\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.2261 - accuracy: 0.9322\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.2267 - accuracy: 0.9319\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.2281 - accuracy: 0.9317\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.2268 - accuracy: 0.9322\n",
            "326/469 [===================>..........] - ETA: 1s - loss: 0.2279 - accuracy: 0.9320\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.2291 - accuracy: 0.9315\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.2297 - accuracy: 0.9313\n",
            "356/469 [=====================>........] - ETA: 1s - loss: 0.2301 - accuracy: 0.9312\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.2307 - accuracy: 0.9308\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.2300 - accuracy: 0.9311\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.2306 - accuracy: 0.9311\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.2309 - accuracy: 0.9308\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.2311 - accuracy: 0.9309\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.2313 - accuracy: 0.9308\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.2314 - accuracy: 0.9309\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2319 - accuracy: 0.9309\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.2314 - accuracy: 0.9309\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2316 - accuracy: 0.9308\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.2317 - accuracy: 0.9307\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2317 - accuracy: 0.9307\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2320 - accuracy: 0.9307\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9309\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9305\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2327 - accuracy: 0.9306 - val_loss: 0.1352 - val_accuracy: 0.9597\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 6/12\n",
            "  6/469 [..............................] - ETA: 4s - loss: 0.1741 - accuracy: 0.9414\n",
            " 16/469 [>.............................] - ETA: 4s - loss: 0.2176 - accuracy: 0.9414\n",
            " 26/469 [>.............................] - ETA: 4s - loss: 0.2102 - accuracy: 0.9396\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.2121 - accuracy: 0.9390\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 0.2134 - accuracy: 0.9385\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 0.2155 - accuracy: 0.9379\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 0.2138 - accuracy: 0.9381\n",
            " 76/469 [===>..........................] - ETA: 4s - loss: 0.2148 - accuracy: 0.9382\n",
            " 86/469 [====>.........................] - ETA: 4s - loss: 0.2158 - accuracy: 0.9380\n",
            " 96/469 [=====>........................] - ETA: 4s - loss: 0.2130 - accuracy: 0.9382\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.2116 - accuracy: 0.9377\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.2072 - accuracy: 0.9380\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.2076 - accuracy: 0.9377\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.2060 - accuracy: 0.9378\n",
            "141/469 [========>.....................] - ETA: 3s - loss: 0.2067 - accuracy: 0.9377\n",
            "146/469 [========>.....................] - ETA: 3s - loss: 0.2102 - accuracy: 0.9364\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.2130 - accuracy: 0.9357\n",
            "161/469 [=========>....................] - ETA: 3s - loss: 0.2131 - accuracy: 0.9352\n",
            "166/469 [=========>....................] - ETA: 3s - loss: 0.2155 - accuracy: 0.9345\n",
            "171/469 [=========>....................] - ETA: 3s - loss: 0.2152 - accuracy: 0.9345\n",
            "176/469 [==========>...................] - ETA: 3s - loss: 0.2146 - accuracy: 0.9347\n",
            "181/469 [==========>...................] - ETA: 3s - loss: 0.2139 - accuracy: 0.9349\n",
            "186/469 [==========>...................] - ETA: 3s - loss: 0.2143 - accuracy: 0.9349\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.2148 - accuracy: 0.9347\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.2161 - accuracy: 0.9347\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.2158 - accuracy: 0.9347\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2169 - accuracy: 0.9341\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.2167 - accuracy: 0.9341\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.2161 - accuracy: 0.9341\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.2157 - accuracy: 0.9341\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.2184 - accuracy: 0.9338\n",
            "251/469 [===============>..............] - ETA: 2s - loss: 0.2195 - accuracy: 0.9337\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.2206 - accuracy: 0.9334\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.2204 - accuracy: 0.9336\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.2225 - accuracy: 0.9334\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.2215 - accuracy: 0.9336\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.2215 - accuracy: 0.9337\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.2218 - accuracy: 0.9338\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.2215 - accuracy: 0.9336\n",
            "326/469 [===================>..........] - ETA: 1s - loss: 0.2246 - accuracy: 0.9327\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.2249 - accuracy: 0.9325\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.2252 - accuracy: 0.9325\n",
            "356/469 [=====================>........] - ETA: 1s - loss: 0.2261 - accuracy: 0.9320\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.2269 - accuracy: 0.9318\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.2286 - accuracy: 0.9313\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.2297 - accuracy: 0.9311\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.2305 - accuracy: 0.9307\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.2305 - accuracy: 0.9306\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.2305 - accuracy: 0.9306\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.2321 - accuracy: 0.9301\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2335 - accuracy: 0.9299\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2336 - accuracy: 0.9299\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2338 - accuracy: 0.9299\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.9298\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.9297\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2356 - accuracy: 0.9297 - val_loss: 0.1618 - val_accuracy: 0.9527\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 7/12\n",
            "  6/469 [..............................] - ETA: 4s - loss: 0.2532 - accuracy: 0.9297\n",
            " 16/469 [>.............................] - ETA: 4s - loss: 0.2566 - accuracy: 0.9326\n",
            " 25/469 [>.............................] - ETA: 4s - loss: 0.2385 - accuracy: 0.9328\n",
            " 35/469 [=>............................] - ETA: 4s - loss: 0.2276 - accuracy: 0.9326\n",
            " 45/469 [=>............................] - ETA: 4s - loss: 0.2196 - accuracy: 0.9351\n",
            " 55/469 [==>...........................] - ETA: 4s - loss: 0.2448 - accuracy: 0.9330\n",
            " 65/469 [===>..........................] - ETA: 4s - loss: 0.2809 - accuracy: 0.9273\n",
            " 75/469 [===>..........................] - ETA: 4s - loss: 0.2848 - accuracy: 0.9240\n",
            " 85/469 [====>.........................] - ETA: 4s - loss: 0.2846 - accuracy: 0.9218\n",
            " 95/469 [=====>........................] - ETA: 4s - loss: 0.2822 - accuracy: 0.9219\n",
            "105/469 [=====>........................] - ETA: 3s - loss: 0.2742 - accuracy: 0.9237\n",
            "115/469 [======>.......................] - ETA: 3s - loss: 0.2766 - accuracy: 0.9236\n",
            "125/469 [======>.......................] - ETA: 3s - loss: 0.2739 - accuracy: 0.9244\n",
            "135/469 [=======>......................] - ETA: 3s - loss: 0.2734 - accuracy: 0.9240\n",
            "145/469 [========>.....................] - ETA: 3s - loss: 0.2682 - accuracy: 0.9254\n",
            "155/469 [========>.....................] - ETA: 3s - loss: 0.2641 - accuracy: 0.9266\n",
            "165/469 [=========>....................] - ETA: 3s - loss: 0.2637 - accuracy: 0.9265\n",
            "175/469 [==========>...................] - ETA: 3s - loss: 0.2625 - accuracy: 0.9268\n",
            "185/469 [==========>...................] - ETA: 3s - loss: 0.2610 - accuracy: 0.9272\n",
            "195/469 [===========>..................] - ETA: 2s - loss: 0.2600 - accuracy: 0.9273\n",
            "205/469 [============>.................] - ETA: 2s - loss: 0.2607 - accuracy: 0.9269\n",
            "215/469 [============>.................] - ETA: 2s - loss: 0.2568 - accuracy: 0.9277\n",
            "225/469 [=============>................] - ETA: 2s - loss: 0.2562 - accuracy: 0.9277\n",
            "235/469 [==============>...............] - ETA: 2s - loss: 0.2551 - accuracy: 0.9277\n",
            "245/469 [==============>...............] - ETA: 2s - loss: 0.2525 - accuracy: 0.9285\n",
            "255/469 [===============>..............] - ETA: 2s - loss: 0.2515 - accuracy: 0.9288\n",
            "265/469 [===============>..............] - ETA: 2s - loss: 0.2508 - accuracy: 0.9288\n",
            "275/469 [================>.............] - ETA: 2s - loss: 0.2492 - accuracy: 0.9288\n",
            "285/469 [=================>............] - ETA: 1s - loss: 0.2488 - accuracy: 0.9287\n",
            "290/469 [=================>............] - ETA: 1s - loss: 0.2486 - accuracy: 0.9287\n",
            "295/469 [=================>............] - ETA: 1s - loss: 0.2481 - accuracy: 0.9289\n",
            "305/469 [==================>...........] - ETA: 1s - loss: 0.2466 - accuracy: 0.9290\n",
            "310/469 [==================>...........] - ETA: 1s - loss: 0.2467 - accuracy: 0.9289\n",
            "315/469 [===================>..........] - ETA: 1s - loss: 0.2468 - accuracy: 0.9288\n",
            "325/469 [===================>..........] - ETA: 1s - loss: 0.2457 - accuracy: 0.9290\n",
            "335/469 [====================>.........] - ETA: 1s - loss: 0.2449 - accuracy: 0.9290\n",
            "340/469 [====================>.........] - ETA: 1s - loss: 0.2461 - accuracy: 0.9287\n",
            "345/469 [=====================>........] - ETA: 1s - loss: 0.2451 - accuracy: 0.9289\n",
            "355/469 [=====================>........] - ETA: 1s - loss: 0.2449 - accuracy: 0.9290\n",
            "365/469 [======================>.......] - ETA: 1s - loss: 0.2439 - accuracy: 0.9292\n",
            "375/469 [======================>.......] - ETA: 1s - loss: 0.2429 - accuracy: 0.9297\n",
            "385/469 [=======================>......] - ETA: 0s - loss: 0.2428 - accuracy: 0.9297\n",
            "395/469 [========================>.....] - ETA: 0s - loss: 0.2415 - accuracy: 0.9302\n",
            "405/469 [========================>.....] - ETA: 0s - loss: 0.2412 - accuracy: 0.9301\n",
            "410/469 [=========================>....] - ETA: 0s - loss: 0.2405 - accuracy: 0.9304\n",
            "415/469 [=========================>....] - ETA: 0s - loss: 0.2403 - accuracy: 0.9305\n",
            "425/469 [==========================>...] - ETA: 0s - loss: 0.2400 - accuracy: 0.9306\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.2408 - accuracy: 0.9304\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.2401 - accuracy: 0.9306\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9309\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.2402 - accuracy: 0.9309\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2401 - accuracy: 0.9309 - val_loss: 0.2407 - val_accuracy: 0.9456\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 8/12\n",
            "  6/469 [..............................] - ETA: 4s - loss: 0.2479 - accuracy: 0.9284\n",
            " 16/469 [>.............................] - ETA: 4s - loss: 0.2332 - accuracy: 0.9287\n",
            " 26/469 [>.............................] - ETA: 4s - loss: 0.2339 - accuracy: 0.9285\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.2272 - accuracy: 0.9301\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 0.2218 - accuracy: 0.9312\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 0.2152 - accuracy: 0.9325\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 0.2154 - accuracy: 0.9334\n",
            " 76/469 [===>..........................] - ETA: 4s - loss: 0.2162 - accuracy: 0.9334\n",
            " 86/469 [====>.........................] - ETA: 4s - loss: 0.2157 - accuracy: 0.9347\n",
            " 96/469 [=====>........................] - ETA: 3s - loss: 0.2163 - accuracy: 0.9347\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.2130 - accuracy: 0.9353\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.2145 - accuracy: 0.9357\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.2162 - accuracy: 0.9351\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.2161 - accuracy: 0.9353\n",
            "146/469 [========>.....................] - ETA: 3s - loss: 0.2163 - accuracy: 0.9356\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.2150 - accuracy: 0.9355\n",
            "166/469 [=========>....................] - ETA: 3s - loss: 0.2159 - accuracy: 0.9359\n",
            "176/469 [==========>...................] - ETA: 3s - loss: 0.2166 - accuracy: 0.9359\n",
            "186/469 [==========>...................] - ETA: 3s - loss: 0.2167 - accuracy: 0.9359\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.2157 - accuracy: 0.9364\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.2175 - accuracy: 0.9362\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.2187 - accuracy: 0.9359\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2199 - accuracy: 0.9354\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.2203 - accuracy: 0.9357\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.2189 - accuracy: 0.9356\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.2195 - accuracy: 0.9353\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.2191 - accuracy: 0.9352\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.2190 - accuracy: 0.9349\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.2194 - accuracy: 0.9344\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.2201 - accuracy: 0.9345\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.2203 - accuracy: 0.9343\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.2208 - accuracy: 0.9340\n",
            "326/469 [===================>..........] - ETA: 1s - loss: 0.2214 - accuracy: 0.9335\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.2216 - accuracy: 0.9337\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.2220 - accuracy: 0.9335\n",
            "356/469 [=====================>........] - ETA: 1s - loss: 0.2211 - accuracy: 0.9338\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.2247 - accuracy: 0.9333\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.2251 - accuracy: 0.9332\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.2244 - accuracy: 0.9334\n",
            "391/469 [========================>.....] - ETA: 0s - loss: 0.2247 - accuracy: 0.9333\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.2248 - accuracy: 0.9332\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.2257 - accuracy: 0.9328\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.2254 - accuracy: 0.9329\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.2255 - accuracy: 0.9330\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2250 - accuracy: 0.9329\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2242 - accuracy: 0.9333\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9332\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2251 - accuracy: 0.9330\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2254 - accuracy: 0.9331\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2250 - accuracy: 0.9331 - val_loss: 0.2006 - val_accuracy: 0.9574\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 9/12\n",
            "  6/469 [..............................] - ETA: 4s - loss: 0.1909 - accuracy: 0.9492\n",
            " 16/469 [>.............................] - ETA: 4s - loss: 0.2198 - accuracy: 0.9321\n",
            " 26/469 [>.............................] - ETA: 4s - loss: 0.2174 - accuracy: 0.9336\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.2104 - accuracy: 0.9358\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 0.2178 - accuracy: 0.9348\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 0.2118 - accuracy: 0.9353\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 0.2129 - accuracy: 0.9356\n",
            " 76/469 [===>..........................] - ETA: 4s - loss: 0.2144 - accuracy: 0.9362\n",
            " 86/469 [====>.........................] - ETA: 4s - loss: 0.2169 - accuracy: 0.9358\n",
            " 96/469 [=====>........................] - ETA: 3s - loss: 0.2206 - accuracy: 0.9343\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.2141 - accuracy: 0.9354\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.2187 - accuracy: 0.9353\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.2234 - accuracy: 0.9345\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.2261 - accuracy: 0.9337\n",
            "146/469 [========>.....................] - ETA: 3s - loss: 0.2267 - accuracy: 0.9338\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.2257 - accuracy: 0.9340\n",
            "166/469 [=========>....................] - ETA: 3s - loss: 0.2266 - accuracy: 0.9336\n",
            "176/469 [==========>...................] - ETA: 3s - loss: 0.2245 - accuracy: 0.9342\n",
            "186/469 [==========>...................] - ETA: 3s - loss: 0.2241 - accuracy: 0.9343\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.2231 - accuracy: 0.9348\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.2209 - accuracy: 0.9351\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.2188 - accuracy: 0.9354\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2193 - accuracy: 0.9355\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.2174 - accuracy: 0.9360\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.2161 - accuracy: 0.9368\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.2144 - accuracy: 0.9372\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.2141 - accuracy: 0.9370\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.2140 - accuracy: 0.9368\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.2131 - accuracy: 0.9372\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.2130 - accuracy: 0.9371\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.2128 - accuracy: 0.9372\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.2142 - accuracy: 0.9372\n",
            "326/469 [===================>..........] - ETA: 1s - loss: 0.2141 - accuracy: 0.9373\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.2127 - accuracy: 0.9377\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.2123 - accuracy: 0.9379\n",
            "356/469 [=====================>........] - ETA: 1s - loss: 0.2119 - accuracy: 0.9379\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.2122 - accuracy: 0.9378\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.2130 - accuracy: 0.9375\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.2137 - accuracy: 0.9374\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.2139 - accuracy: 0.9375\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.2144 - accuracy: 0.9373\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.2145 - accuracy: 0.9372\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.2145 - accuracy: 0.9372\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2150 - accuracy: 0.9371\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2147 - accuracy: 0.9373\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.2162 - accuracy: 0.9369\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2176 - accuracy: 0.9365\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2183 - accuracy: 0.9364 - val_loss: 0.2006 - val_accuracy: 0.9448\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 10/12\n",
            "  6/469 [..............................] - ETA: 5s - loss: 0.2974 - accuracy: 0.9232\n",
            " 16/469 [>.............................] - ETA: 4s - loss: 0.2514 - accuracy: 0.9272\n",
            " 26/469 [>.............................] - ETA: 4s - loss: 0.2440 - accuracy: 0.9303\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.2441 - accuracy: 0.9301\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 0.2370 - accuracy: 0.9321\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 0.2319 - accuracy: 0.9322\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 0.2355 - accuracy: 0.9309\n",
            " 76/469 [===>..........................] - ETA: 4s - loss: 0.2335 - accuracy: 0.9325\n",
            " 86/469 [====>.........................] - ETA: 4s - loss: 0.2313 - accuracy: 0.9334\n",
            " 96/469 [=====>........................] - ETA: 3s - loss: 0.2321 - accuracy: 0.9343\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.2310 - accuracy: 0.9346\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.2295 - accuracy: 0.9348\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.2262 - accuracy: 0.9358\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.2284 - accuracy: 0.9354\n",
            "146/469 [========>.....................] - ETA: 3s - loss: 0.2277 - accuracy: 0.9355\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.2287 - accuracy: 0.9348\n",
            "166/469 [=========>....................] - ETA: 3s - loss: 0.2301 - accuracy: 0.9343\n",
            "176/469 [==========>...................] - ETA: 3s - loss: 0.2302 - accuracy: 0.9343\n",
            "186/469 [==========>...................] - ETA: 3s - loss: 0.2323 - accuracy: 0.9337\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.2341 - accuracy: 0.9338\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.2358 - accuracy: 0.9333\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.2355 - accuracy: 0.9333\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2341 - accuracy: 0.9332\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.2320 - accuracy: 0.9337\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.2319 - accuracy: 0.9337\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.2308 - accuracy: 0.9336\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.2291 - accuracy: 0.9340\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.2451 - accuracy: 0.9324\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.2682 - accuracy: 0.9266\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.2771 - accuracy: 0.9245\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.2830 - accuracy: 0.9224\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.2856 - accuracy: 0.9215\n",
            "326/469 [===================>..........] - ETA: 1s - loss: 0.2873 - accuracy: 0.9207\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.2869 - accuracy: 0.9207\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.2868 - accuracy: 0.9206\n",
            "356/469 [=====================>........] - ETA: 1s - loss: 0.2866 - accuracy: 0.9205\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.2850 - accuracy: 0.9210\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.2840 - accuracy: 0.9208\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.2827 - accuracy: 0.9208\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.2828 - accuracy: 0.9208\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.2826 - accuracy: 0.9205\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.2820 - accuracy: 0.9209\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.2815 - accuracy: 0.9210\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2811 - accuracy: 0.9209\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.2807 - accuracy: 0.9209\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.9211\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.9209\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2809 - accuracy: 0.9210 - val_loss: 0.1609 - val_accuracy: 0.9580\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 11/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.2818 - accuracy: 0.9375\n",
            " 11/469 [..............................] - ETA: 4s - loss: 0.2054 - accuracy: 0.9432\n",
            " 21/469 [>.............................] - ETA: 4s - loss: 0.2024 - accuracy: 0.9401\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.2108 - accuracy: 0.9383\n",
            " 41/469 [=>............................] - ETA: 4s - loss: 0.2158 - accuracy: 0.9381\n",
            " 51/469 [==>...........................] - ETA: 4s - loss: 0.2166 - accuracy: 0.9369\n",
            " 61/469 [==>...........................] - ETA: 4s - loss: 0.2146 - accuracy: 0.9361\n",
            " 71/469 [===>..........................] - ETA: 4s - loss: 0.2142 - accuracy: 0.9373\n",
            " 81/469 [====>.........................] - ETA: 4s - loss: 0.2162 - accuracy: 0.9365\n",
            " 91/469 [====>.........................] - ETA: 4s - loss: 0.2199 - accuracy: 0.9361\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.2216 - accuracy: 0.9367\n",
            "111/469 [======>.......................] - ETA: 3s - loss: 0.2250 - accuracy: 0.9361\n",
            "121/469 [======>.......................] - ETA: 3s - loss: 0.2280 - accuracy: 0.9356\n",
            "131/469 [=======>......................] - ETA: 3s - loss: 0.2272 - accuracy: 0.9353\n",
            "141/469 [========>.....................] - ETA: 3s - loss: 0.2280 - accuracy: 0.9343\n",
            "151/469 [========>.....................] - ETA: 3s - loss: 0.2280 - accuracy: 0.9339\n",
            "161/469 [=========>....................] - ETA: 3s - loss: 0.2298 - accuracy: 0.9329\n",
            "171/469 [=========>....................] - ETA: 3s - loss: 0.2270 - accuracy: 0.9329\n",
            "181/469 [==========>...................] - ETA: 3s - loss: 0.2263 - accuracy: 0.9338\n",
            "191/469 [===========>..................] - ETA: 2s - loss: 0.2264 - accuracy: 0.9337\n",
            "196/469 [===========>..................] - ETA: 2s - loss: 0.2253 - accuracy: 0.9340\n",
            "201/469 [===========>..................] - ETA: 2s - loss: 0.2250 - accuracy: 0.9343\n",
            "211/469 [============>.................] - ETA: 2s - loss: 0.2267 - accuracy: 0.9342\n",
            "221/469 [=============>................] - ETA: 2s - loss: 0.2270 - accuracy: 0.9339\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2263 - accuracy: 0.9344\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.2244 - accuracy: 0.9349\n",
            "236/469 [==============>...............] - ETA: 2s - loss: 0.2240 - accuracy: 0.9351\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.2230 - accuracy: 0.9353\n",
            "251/469 [===============>..............] - ETA: 2s - loss: 0.2232 - accuracy: 0.9352\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.2226 - accuracy: 0.9354\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.2216 - accuracy: 0.9356\n",
            "276/469 [================>.............] - ETA: 2s - loss: 0.2238 - accuracy: 0.9352\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.2236 - accuracy: 0.9353\n",
            "291/469 [=================>............] - ETA: 1s - loss: 0.2232 - accuracy: 0.9354\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.2236 - accuracy: 0.9352\n",
            "301/469 [==================>...........] - ETA: 1s - loss: 0.2231 - accuracy: 0.9354\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.2224 - accuracy: 0.9354\n",
            "321/469 [===================>..........] - ETA: 1s - loss: 0.2220 - accuracy: 0.9355\n",
            "331/469 [====================>.........] - ETA: 1s - loss: 0.2222 - accuracy: 0.9355\n",
            "341/469 [====================>.........] - ETA: 1s - loss: 0.2218 - accuracy: 0.9354\n",
            "351/469 [=====================>........] - ETA: 1s - loss: 0.2218 - accuracy: 0.9355\n",
            "361/469 [======================>.......] - ETA: 1s - loss: 0.2215 - accuracy: 0.9354\n",
            "371/469 [======================>.......] - ETA: 1s - loss: 0.2217 - accuracy: 0.9356\n",
            "381/469 [=======================>......] - ETA: 0s - loss: 0.2212 - accuracy: 0.9356\n",
            "391/469 [========================>.....] - ETA: 0s - loss: 0.2206 - accuracy: 0.9358\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.2199 - accuracy: 0.9360\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.2196 - accuracy: 0.9362\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2192 - accuracy: 0.9364\n",
            "431/469 [==========================>...] - ETA: 0s - loss: 0.2189 - accuracy: 0.9364\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.2189 - accuracy: 0.9365\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.2186 - accuracy: 0.9365\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2180 - accuracy: 0.9365\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2174 - accuracy: 0.9368\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2171 - accuracy: 0.9368\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2171 - accuracy: 0.9368 - val_loss: 0.1577 - val_accuracy: 0.9583\n",
            "\u001b[36m(train_mnist pid=3299519)\u001b[0m Epoch 12/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.1647 - accuracy: 0.9531\n",
            " 11/469 [..............................] - ETA: 4s - loss: 0.2613 - accuracy: 0.9261\n",
            " 21/469 [>.............................] - ETA: 4s - loss: 0.2274 - accuracy: 0.9345\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.2207 - accuracy: 0.9357\n",
            " 41/469 [=>............................] - ETA: 4s - loss: 0.2266 - accuracy: 0.9350\n",
            " 51/469 [==>...........................] - ETA: 4s - loss: 0.2194 - accuracy: 0.9366\n",
            " 61/469 [==>...........................] - ETA: 4s - loss: 0.2064 - accuracy: 0.9406\n",
            " 71/469 [===>..........................] - ETA: 4s - loss: 0.2058 - accuracy: 0.9415\n",
            " 81/469 [====>.........................] - ETA: 4s - loss: 0.2040 - accuracy: 0.9410\n",
            " 91/469 [====>.........................] - ETA: 4s - loss: 0.2035 - accuracy: 0.9420\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.2021 - accuracy: 0.9418\n",
            "111/469 [======>.......................] - ETA: 3s - loss: 0.2016 - accuracy: 0.9417\n",
            "121/469 [======>.......................] - ETA: 3s - loss: 0.2025 - accuracy: 0.9423\n",
            "131/469 [=======>......................] - ETA: 3s - loss: 0.2036 - accuracy: 0.9423\n",
            "141/469 [========>.....................] - ETA: 3s - loss: 0.2025 - accuracy: 0.9423\n",
            "151/469 [========>.....................] - ETA: 3s - loss: 0.2025 - accuracy: 0.9422\n",
            "161/469 [=========>....................] - ETA: 3s - loss: 0.2180 - accuracy: 0.9401\n",
            "171/469 [=========>....................] - ETA: 3s - loss: 0.2230 - accuracy: 0.9387\n",
            "181/469 [==========>...................] - ETA: 3s - loss: 0.2254 - accuracy: 0.9377\n",
            "191/469 [===========>..................] - ETA: 2s - loss: 0.2312 - accuracy: 0.9367\n",
            "201/469 [===========>..................] - ETA: 2s - loss: 0.2320 - accuracy: 0.9358\n",
            "211/469 [============>.................] - ETA: 2s - loss: 0.2361 - accuracy: 0.9352\n",
            "221/469 [=============>................] - ETA: 2s - loss: 0.2364 - accuracy: 0.9351\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.2370 - accuracy: 0.9345\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.2392 - accuracy: 0.9336\n",
            "251/469 [===============>..............] - ETA: 2s - loss: 0.2437 - accuracy: 0.9333\n",
            "261/469 [===============>..............] - ETA: 2s - loss: 0.2473 - accuracy: 0.9325\n",
            "271/469 [================>.............] - ETA: 2s - loss: 0.2494 - accuracy: 0.9314\n",
            "281/469 [================>.............] - ETA: 2s - loss: 0.2483 - accuracy: 0.9319\n",
            "291/469 [=================>............] - ETA: 1s - loss: 0.2473 - accuracy: 0.9321\n",
            "301/469 [==================>...........] - ETA: 1s - loss: 0.2473 - accuracy: 0.9320\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.2466 - accuracy: 0.9320\n",
            "321/469 [===================>..........] - ETA: 1s - loss: 0.2463 - accuracy: 0.9319\n",
            "331/469 [====================>.........] - ETA: 1s - loss: 0.2461 - accuracy: 0.9320\n",
            "341/469 [====================>.........] - ETA: 1s - loss: 0.2462 - accuracy: 0.9318\n",
            "351/469 [=====================>........] - ETA: 1s - loss: 0.2461 - accuracy: 0.9320\n",
            "361/469 [======================>.......] - ETA: 1s - loss: 0.2478 - accuracy: 0.9317\n",
            "371/469 [======================>.......] - ETA: 1s - loss: 0.2490 - accuracy: 0.9315\n",
            "381/469 [=======================>......] - ETA: 0s - loss: 0.2584 - accuracy: 0.9304\n",
            "391/469 [========================>.....] - ETA: 0s - loss: 0.2685 - accuracy: 0.9293\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.2737 - accuracy: 0.9280\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.2770 - accuracy: 0.9268\n",
            "421/469 [=========================>....] - ETA: 0s - loss: 0.2804 - accuracy: 0.9264\n",
            "431/469 [==========================>...] - ETA: 0s - loss: 0.2837 - accuracy: 0.9256\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.2830 - accuracy: 0.9256\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.2843 - accuracy: 0.9253\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.9253\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.9256\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.2826 - accuracy: 0.9257 - val_loss: 0.1923 - val_accuracy: 0.9508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3300066)\u001b[0m 2023-12-05 01:51:24.057194: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3300066)\u001b[0m 2023-12-05 01:51:24.105627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3300066)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3300066)\u001b[0m 2023-12-05 01:51:24.105130: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3300066)\u001b[0m 2023-12-05 01:51:24.989849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3300066)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3300066)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3300066)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3300066)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3300066)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3300066)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3300066)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3300066)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m 2023-12-05 01:51:26.580894: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 1/12\n",
            "  3/469 [..............................] - ETA: 14s - loss: 79.7654 - accuracy: 0.1094\n",
            "  7/469 [..............................] - ETA: 14s - loss: 35.5304 - accuracy: 0.1060\n",
            " 11/469 [..............................] - ETA: 14s - loss: 23.4561 - accuracy: 0.1101\n",
            " 15/469 [..............................] - ETA: 13s - loss: 17.8238 - accuracy: 0.1036\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 15.9969 - accuracy: 0.1071\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 13.3905 - accuracy: 0.1012\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 11.6191 - accuracy: 0.0975\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 10.9272 - accuracy: 0.0998\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 10.3325 - accuracy: 0.0989\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 9.8144 - accuracy: 0.0988 \n",
            " 35/469 [=>............................] - ETA: 13s - loss: 8.9600 - accuracy: 0.0989\n",
            " 39/469 [=>............................] - ETA: 13s - loss: 8.2757 - accuracy: 0.0988\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 7.7196 - accuracy: 0.0988\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 7.4791 - accuracy: 0.0979\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 7.0567 - accuracy: 0.0977\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 6.6965 - accuracy: 0.0994\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 6.3798 - accuracy: 0.1061\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 6.2303 - accuracy: 0.1148\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 5.9375 - accuracy: 0.1372\n",
            " 67/469 [===>..........................] - ETA: 12s - loss: 5.6543 - accuracy: 0.1658\n",
            " 71/469 [===>..........................] - ETA: 12s - loss: 5.3980 - accuracy: 0.1940\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 5.1631 - accuracy: 0.2228\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 5.0531 - accuracy: 0.2370\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 4.8393 - accuracy: 0.2636\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 4.6364 - accuracy: 0.2912\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 4.4516 - accuracy: 0.3154\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 4.3683 - accuracy: 0.3264\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 4.2047 - accuracy: 0.3488\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 4.0547 - accuracy: 0.3701\n",
            "103/469 [=====>........................] - ETA: 11s - loss: 3.9156 - accuracy: 0.3895\n",
            "105/469 [=====>........................] - ETA: 11s - loss: 3.8477 - accuracy: 0.3990\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 3.7211 - accuracy: 0.4163\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 3.6003 - accuracy: 0.4339\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 3.4906 - accuracy: 0.4493\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 3.4372 - accuracy: 0.4571\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 3.3360 - accuracy: 0.4715\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 3.2407 - accuracy: 0.4855\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 3.1529 - accuracy: 0.4980\n",
            "135/469 [=======>......................] - ETA: 10s - loss: 3.0708 - accuracy: 0.5094\n",
            "137/469 [=======>......................] - ETA: 10s - loss: 3.0295 - accuracy: 0.5155\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 2.9531 - accuracy: 0.5269 \n",
            "145/469 [========>.....................] - ETA: 9s - loss: 2.8844 - accuracy: 0.5369\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 2.8141 - accuracy: 0.5474\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 2.7502 - accuracy: 0.5562\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 2.7190 - accuracy: 0.5606\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 2.6574 - accuracy: 0.5698\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 2.5997 - accuracy: 0.5781\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 2.5716 - accuracy: 0.5824\n",
            "167/469 [=========>....................] - ETA: 9s - loss: 2.5438 - accuracy: 0.5865\n",
            "169/469 [=========>....................] - ETA: 9s - loss: 2.5176 - accuracy: 0.5902\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 2.4658 - accuracy: 0.5977\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 2.4173 - accuracy: 0.6048\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 2.3693 - accuracy: 0.6120\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 2.3233 - accuracy: 0.6188\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 2.3014 - accuracy: 0.6218\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 2.2803 - accuracy: 0.6248\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 2.2588 - accuracy: 0.6279\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 2.2185 - accuracy: 0.6333\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 2.1800 - accuracy: 0.6389\n",
            "203/469 [===========>..................] - ETA: 8s - loss: 2.1418 - accuracy: 0.6446\n",
            "207/469 [============>.................] - ETA: 7s - loss: 2.1043 - accuracy: 0.6505\n",
            "209/469 [============>.................] - ETA: 7s - loss: 2.0870 - accuracy: 0.6530\n",
            "213/469 [============>.................] - ETA: 7s - loss: 2.0532 - accuracy: 0.6583\n",
            "217/469 [============>.................] - ETA: 7s - loss: 2.0191 - accuracy: 0.6633\n",
            "221/469 [=============>................] - ETA: 7s - loss: 1.9868 - accuracy: 0.6680\n",
            "223/469 [=============>................] - ETA: 7s - loss: 1.9716 - accuracy: 0.6701\n",
            "227/469 [=============>................] - ETA: 7s - loss: 1.9417 - accuracy: 0.6746\n",
            "231/469 [=============>................] - ETA: 7s - loss: 1.9114 - accuracy: 0.6793\n",
            "235/469 [==============>...............] - ETA: 7s - loss: 1.8832 - accuracy: 0.6832\n",
            "237/469 [==============>...............] - ETA: 7s - loss: 1.8690 - accuracy: 0.6853\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 1.8423 - accuracy: 0.6895\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 1.8162 - accuracy: 0.6937\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 1.7926 - accuracy: 0.6969\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 1.7804 - accuracy: 0.6987\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 1.7566 - accuracy: 0.7020\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 1.7336 - accuracy: 0.7053\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 1.7110 - accuracy: 0.7085\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 1.7000 - accuracy: 0.7100\n",
            "267/469 [================>.............] - ETA: 6s - loss: 1.6894 - accuracy: 0.7115\n",
            "269/469 [================>.............] - ETA: 6s - loss: 1.6789 - accuracy: 0.7130\n",
            "273/469 [================>.............] - ETA: 5s - loss: 1.6578 - accuracy: 0.7160\n",
            "277/469 [================>.............] - ETA: 5s - loss: 1.6381 - accuracy: 0.7189\n",
            "281/469 [================>.............] - ETA: 5s - loss: 1.6170 - accuracy: 0.7222\n",
            "283/469 [=================>............] - ETA: 5s - loss: 1.6075 - accuracy: 0.7237\n",
            "285/469 [=================>............] - ETA: 5s - loss: 1.5976 - accuracy: 0.7252\n",
            "287/469 [=================>............] - ETA: 5s - loss: 1.5888 - accuracy: 0.7266\n",
            "291/469 [=================>............] - ETA: 5s - loss: 1.5704 - accuracy: 0.7293\n",
            "295/469 [=================>............] - ETA: 5s - loss: 1.5520 - accuracy: 0.7320\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 1.5338 - accuracy: 0.7349\n",
            "301/469 [==================>...........] - ETA: 5s - loss: 1.5251 - accuracy: 0.7363\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 1.5077 - accuracy: 0.7390\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 1.4912 - accuracy: 0.7416\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 1.4749 - accuracy: 0.7441\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 1.4668 - accuracy: 0.7454\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 1.4507 - accuracy: 0.7479\n",
            "323/469 [===================>..........] - ETA: 4s - loss: 1.4353 - accuracy: 0.7503\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 1.4217 - accuracy: 0.7523\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 1.4079 - accuracy: 0.7542\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 1.4009 - accuracy: 0.7553\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 1.3868 - accuracy: 0.7575\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 1.3730 - accuracy: 0.7596\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 1.3602 - accuracy: 0.7616\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 1.3532 - accuracy: 0.7628\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 1.3401 - accuracy: 0.7648\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 1.3274 - accuracy: 0.7668\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 1.3146 - accuracy: 0.7688\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 1.3082 - accuracy: 0.7698\n",
            "363/469 [======================>.......] - ETA: 3s - loss: 1.3024 - accuracy: 0.7707\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 1.2960 - accuracy: 0.7717\n",
            "369/469 [======================>.......] - ETA: 3s - loss: 1.2842 - accuracy: 0.7735\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 1.2729 - accuracy: 0.7754\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 1.2617 - accuracy: 0.7770\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 1.2567 - accuracy: 0.7777\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 1.2460 - accuracy: 0.7793\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 1.2356 - accuracy: 0.7808\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 1.2259 - accuracy: 0.7821\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 1.2161 - accuracy: 0.7835\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 1.2114 - accuracy: 0.7842\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 1.2011 - accuracy: 0.7858\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 1.1919 - accuracy: 0.7872\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 1.1829 - accuracy: 0.7886\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 1.1737 - accuracy: 0.7900\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 1.1691 - accuracy: 0.7907\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 1.1596 - accuracy: 0.7922\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 1.1512 - accuracy: 0.7936\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 1.1421 - accuracy: 0.7950\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 1.1333 - accuracy: 0.7964\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 1.1287 - accuracy: 0.7971\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 1.1245 - accuracy: 0.7978\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 1.1204 - accuracy: 0.7984\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 1.1125 - accuracy: 0.7995\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 1.1043 - accuracy: 0.8007\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 1.0960 - accuracy: 0.8020\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 1.0920 - accuracy: 0.8026\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 1.0888 - accuracy: 0.8031\n",
            "455/469 [============================>.] - ETA: 0s - loss: 1.0850 - accuracy: 0.8037\n",
            "459/469 [============================>.] - ETA: 0s - loss: 1.0776 - accuracy: 0.8048\n",
            "463/469 [============================>.] - ETA: 0s - loss: 1.0703 - accuracy: 0.8059\n",
            "467/469 [============================>.] - ETA: 0s - loss: 1.0630 - accuracy: 0.8069\n",
            "469/469 [==============================] - ETA: 0s - loss: 1.0598 - accuracy: 0.8074\n",
            "469/469 [==============================] - 16s 32ms/step - loss: 1.0598 - accuracy: 0.8074 - val_loss: 0.1576 - val_accuracy: 0.9565\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 2/12\n",
            "  1/469 [..............................] - ETA: 13s - loss: 0.1577 - accuracy: 0.9609\n",
            "  5/469 [..............................] - ETA: 13s - loss: 0.2166 - accuracy: 0.9547\n",
            "  9/469 [..............................] - ETA: 13s - loss: 0.2099 - accuracy: 0.9505\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.2220 - accuracy: 0.9403\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.2179 - accuracy: 0.9375\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 0.2155 - accuracy: 0.9400\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 0.2222 - accuracy: 0.9395\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 0.2111 - accuracy: 0.9421\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.2127 - accuracy: 0.9410\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 0.2135 - accuracy: 0.9411\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 0.2090 - accuracy: 0.9417\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 0.2039 - accuracy: 0.9421\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1995 - accuracy: 0.9433\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.2023 - accuracy: 0.9425\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.2036 - accuracy: 0.9421\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 0.1987 - accuracy: 0.9436\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 0.1958 - accuracy: 0.9441\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 0.1961 - accuracy: 0.9441\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 0.1939 - accuracy: 0.9445\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.1925 - accuracy: 0.9441\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 0.1909 - accuracy: 0.9441\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 0.1887 - accuracy: 0.9449\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1900 - accuracy: 0.9446\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1880 - accuracy: 0.9453\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 0.1865 - accuracy: 0.9451\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 0.1854 - accuracy: 0.9447\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 0.1855 - accuracy: 0.9447\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 0.1847 - accuracy: 0.9449\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 0.1838 - accuracy: 0.9453\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1816 - accuracy: 0.9459\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 0.1817 - accuracy: 0.9453\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 0.1798 - accuracy: 0.9461\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1817 - accuracy: 0.9455\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1843 - accuracy: 0.9453\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1852 - accuracy: 0.9449\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 0.1861 - accuracy: 0.9446\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 0.1861 - accuracy: 0.9445\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 0.1851 - accuracy: 0.9444\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1856 - accuracy: 0.9442\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 0.1846 - accuracy: 0.9441\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 0.1854 - accuracy: 0.9439\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 0.1862 - accuracy: 0.9439\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 0.1855 - accuracy: 0.9440\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 0.1869 - accuracy: 0.9436\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 0.1854 - accuracy: 0.9438\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 0.1847 - accuracy: 0.9439\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 0.1853 - accuracy: 0.9437\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 0.1844 - accuracy: 0.9441\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1840 - accuracy: 0.9442\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 0.1847 - accuracy: 0.9442\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 0.1853 - accuracy: 0.9440\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 0.1850 - accuracy: 0.9442\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 0.1854 - accuracy: 0.9440\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 0.1857 - accuracy: 0.9441\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1860 - accuracy: 0.9440\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1862 - accuracy: 0.9439\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 0.1849 - accuracy: 0.9441\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1852 - accuracy: 0.9441\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 0.1855 - accuracy: 0.9438\n",
            "201/469 [===========>..................] - ETA: 8s - loss: 0.1856 - accuracy: 0.9438\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1859 - accuracy: 0.9438\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1861 - accuracy: 0.9439\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1858 - accuracy: 0.9441\n",
            "217/469 [============>.................] - ETA: 7s - loss: 0.1858 - accuracy: 0.9442\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1864 - accuracy: 0.9440\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1864 - accuracy: 0.9439\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1860 - accuracy: 0.9439\n",
            "231/469 [=============>................] - ETA: 7s - loss: 0.1860 - accuracy: 0.9440\n",
            "233/469 [=============>................] - ETA: 7s - loss: 0.1865 - accuracy: 0.9440\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1874 - accuracy: 0.9439\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1868 - accuracy: 0.9439\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1866 - accuracy: 0.9440\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 0.1873 - accuracy: 0.9438\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 0.1875 - accuracy: 0.9436\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 0.1873 - accuracy: 0.9438\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1874 - accuracy: 0.9439\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1882 - accuracy: 0.9436\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1887 - accuracy: 0.9436\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1888 - accuracy: 0.9435\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1884 - accuracy: 0.9436\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1897 - accuracy: 0.9433\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1901 - accuracy: 0.9433\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1903 - accuracy: 0.9433\n",
            "287/469 [=================>............] - ETA: 5s - loss: 0.1900 - accuracy: 0.9434\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1903 - accuracy: 0.9433\n",
            "291/469 [=================>............] - ETA: 5s - loss: 0.1902 - accuracy: 0.9433\n",
            "295/469 [=================>............] - ETA: 5s - loss: 0.1902 - accuracy: 0.9434\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1915 - accuracy: 0.9430\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1908 - accuracy: 0.9432\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1914 - accuracy: 0.9430\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.1916 - accuracy: 0.9430\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.1917 - accuracy: 0.9430\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.1917 - accuracy: 0.9430\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1916 - accuracy: 0.9431\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1920 - accuracy: 0.9430\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 0.1918 - accuracy: 0.9430\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1920 - accuracy: 0.9430\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 0.1922 - accuracy: 0.9430\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 0.1934 - accuracy: 0.9427\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.1942 - accuracy: 0.9424\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 0.1944 - accuracy: 0.9424\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1942 - accuracy: 0.9425\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.1947 - accuracy: 0.9424\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 0.1940 - accuracy: 0.9426\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 0.1949 - accuracy: 0.9424\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 0.1955 - accuracy: 0.9423\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 0.1957 - accuracy: 0.9422\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1956 - accuracy: 0.9423\n",
            "363/469 [======================>.......] - ETA: 3s - loss: 0.1954 - accuracy: 0.9424\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 0.1951 - accuracy: 0.9424\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 0.1947 - accuracy: 0.9425\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1953 - accuracy: 0.9425\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1952 - accuracy: 0.9425\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 0.1950 - accuracy: 0.9425\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 0.1954 - accuracy: 0.9425\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.1955 - accuracy: 0.9425\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.1953 - accuracy: 0.9426\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 0.1949 - accuracy: 0.9426\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1947 - accuracy: 0.9425\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.1947 - accuracy: 0.9424\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 0.1949 - accuracy: 0.9424\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 0.1952 - accuracy: 0.9424\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.1951 - accuracy: 0.9423\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1947 - accuracy: 0.9424\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 0.1945 - accuracy: 0.9425\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.1946 - accuracy: 0.9424\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.1943 - accuracy: 0.9425\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 0.1939 - accuracy: 0.9426\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.9425\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.1935 - accuracy: 0.9426\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.1934 - accuracy: 0.9425\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.1932 - accuracy: 0.9425\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.1933 - accuracy: 0.9425\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9424\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.1943 - accuracy: 0.9424\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.1937 - accuracy: 0.9425\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.1935 - accuracy: 0.9426\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9425\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1941 - accuracy: 0.9425 - val_loss: 0.1737 - val_accuracy: 0.9563\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 3/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 0.1579 - accuracy: 0.9479\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.1881 - accuracy: 0.9487\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.2064 - accuracy: 0.9382\n",
            " 13/469 [..............................] - ETA: 13s - loss: 0.2017 - accuracy: 0.9423\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 0.2102 - accuracy: 0.9407\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 0.1998 - accuracy: 0.9423\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.1913 - accuracy: 0.9444\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.1883 - accuracy: 0.9434\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 0.1856 - accuracy: 0.9446\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 0.1854 - accuracy: 0.9449\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 0.1845 - accuracy: 0.9459\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1795 - accuracy: 0.9464\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.1744 - accuracy: 0.9475\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 0.1772 - accuracy: 0.9472\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 0.1801 - accuracy: 0.9471\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 0.1830 - accuracy: 0.9464\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.1836 - accuracy: 0.9462\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 0.1810 - accuracy: 0.9468\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 0.1822 - accuracy: 0.9464\n",
            " 67/469 [===>..........................] - ETA: 12s - loss: 0.1808 - accuracy: 0.9474\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 0.1845 - accuracy: 0.9462\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.1848 - accuracy: 0.9458\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1853 - accuracy: 0.9465\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1837 - accuracy: 0.9467\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 0.1824 - accuracy: 0.9467\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 0.1800 - accuracy: 0.9466\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 0.1836 - accuracy: 0.9459\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1827 - accuracy: 0.9462\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 0.1837 - accuracy: 0.9462\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 0.1831 - accuracy: 0.9465\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 0.1844 - accuracy: 0.9460\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 0.1836 - accuracy: 0.9464\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1841 - accuracy: 0.9461\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1844 - accuracy: 0.9464\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 0.1841 - accuracy: 0.9463\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1839 - accuracy: 0.9463\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 0.1854 - accuracy: 0.9462\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 0.1847 - accuracy: 0.9465\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 0.1840 - accuracy: 0.9465\n",
            "135/469 [=======>......................] - ETA: 10s - loss: 0.1833 - accuracy: 0.9466\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1833 - accuracy: 0.9465 \n",
            "139/469 [=======>......................] - ETA: 9s - loss: 0.1831 - accuracy: 0.9466\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 0.1827 - accuracy: 0.9466\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 0.1818 - accuracy: 0.9469\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 0.1823 - accuracy: 0.9466\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 0.1814 - accuracy: 0.9467\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 0.1808 - accuracy: 0.9469\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 0.1815 - accuracy: 0.9466\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1820 - accuracy: 0.9463\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 0.1822 - accuracy: 0.9465\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 0.1828 - accuracy: 0.9463\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 0.1809 - accuracy: 0.9468\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 0.1806 - accuracy: 0.9467\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1799 - accuracy: 0.9468\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1786 - accuracy: 0.9471\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 0.1780 - accuracy: 0.9474\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 0.1776 - accuracy: 0.9475\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 0.1769 - accuracy: 0.9476\n",
            "201/469 [===========>..................] - ETA: 8s - loss: 0.1767 - accuracy: 0.9478\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 0.1766 - accuracy: 0.9476\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1761 - accuracy: 0.9478\n",
            "207/469 [============>.................] - ETA: 7s - loss: 0.1768 - accuracy: 0.9476\n",
            "211/469 [============>.................] - ETA: 7s - loss: 0.1771 - accuracy: 0.9473\n",
            "215/469 [============>.................] - ETA: 7s - loss: 0.1766 - accuracy: 0.9476\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1764 - accuracy: 0.9475\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1762 - accuracy: 0.9476\n",
            "225/469 [=============>................] - ETA: 7s - loss: 0.1766 - accuracy: 0.9475\n",
            "229/469 [=============>................] - ETA: 7s - loss: 0.1754 - accuracy: 0.9476\n",
            "233/469 [=============>................] - ETA: 7s - loss: 0.1749 - accuracy: 0.9478\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1755 - accuracy: 0.9480\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1755 - accuracy: 0.9479\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 0.1752 - accuracy: 0.9480\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 0.1749 - accuracy: 0.9479\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 0.1754 - accuracy: 0.9478\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 0.1749 - accuracy: 0.9478\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 0.1748 - accuracy: 0.9478\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1752 - accuracy: 0.9477\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 0.1747 - accuracy: 0.9479\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 0.1747 - accuracy: 0.9478\n",
            "269/469 [================>.............] - ETA: 5s - loss: 0.1757 - accuracy: 0.9475\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1757 - accuracy: 0.9476\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1764 - accuracy: 0.9475\n",
            "279/469 [================>.............] - ETA: 5s - loss: 0.1767 - accuracy: 0.9474\n",
            "283/469 [=================>............] - ETA: 5s - loss: 0.1769 - accuracy: 0.9472\n",
            "287/469 [=================>............] - ETA: 5s - loss: 0.1762 - accuracy: 0.9474\n",
            "291/469 [=================>............] - ETA: 5s - loss: 0.1760 - accuracy: 0.9474\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1759 - accuracy: 0.9474\n",
            "297/469 [=================>............] - ETA: 5s - loss: 0.1762 - accuracy: 0.9473\n",
            "301/469 [==================>...........] - ETA: 5s - loss: 0.1757 - accuracy: 0.9475\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 0.1756 - accuracy: 0.9475\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.1750 - accuracy: 0.9476\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1748 - accuracy: 0.9476\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.1747 - accuracy: 0.9477\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 0.1748 - accuracy: 0.9477\n",
            "323/469 [===================>..........] - ETA: 4s - loss: 0.1748 - accuracy: 0.9477\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 0.1744 - accuracy: 0.9478\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1741 - accuracy: 0.9479\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 0.1736 - accuracy: 0.9481\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.1746 - accuracy: 0.9479\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 0.1745 - accuracy: 0.9479\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1747 - accuracy: 0.9478\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.1756 - accuracy: 0.9475\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1764 - accuracy: 0.9473\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.1763 - accuracy: 0.9471\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 0.1766 - accuracy: 0.9471\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 0.1766 - accuracy: 0.9471\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1761 - accuracy: 0.9473\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1757 - accuracy: 0.9473\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.1765 - accuracy: 0.9470\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 0.1761 - accuracy: 0.9471\n",
            "377/469 [=======================>......] - ETA: 2s - loss: 0.1762 - accuracy: 0.9470\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 0.1764 - accuracy: 0.9469\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1763 - accuracy: 0.9469\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.1763 - accuracy: 0.9469\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.1761 - accuracy: 0.9469\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 0.1769 - accuracy: 0.9468\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 0.1780 - accuracy: 0.9465\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1779 - accuracy: 0.9466\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.1779 - accuracy: 0.9466\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.1775 - accuracy: 0.9468\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.1776 - accuracy: 0.9467\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 0.1775 - accuracy: 0.9466\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 0.1780 - accuracy: 0.9466\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1780 - accuracy: 0.9467\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 0.1782 - accuracy: 0.9467\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.1784 - accuracy: 0.9466\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.1785 - accuracy: 0.9466\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 0.1789 - accuracy: 0.9465\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 0.1785 - accuracy: 0.9465\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1791 - accuracy: 0.9465\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.1791 - accuracy: 0.9465\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.1792 - accuracy: 0.9464\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.1793 - accuracy: 0.9464\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.1797 - accuracy: 0.9463\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.9463\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1796 - accuracy: 0.9463\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9464\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.1789 - accuracy: 0.9466\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1786 - accuracy: 0.9466\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1786 - accuracy: 0.9466 - val_loss: 0.1557 - val_accuracy: 0.9618\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 4/12\n",
            "  1/469 [..............................] - ETA: 14s - loss: 0.1210 - accuracy: 0.9531\n",
            "  3/469 [..............................] - ETA: 14s - loss: 0.1029 - accuracy: 0.9609\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.1587 - accuracy: 0.9498\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.1668 - accuracy: 0.9474\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1616 - accuracy: 0.9484\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 0.1576 - accuracy: 0.9499\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 0.1531 - accuracy: 0.9509\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.1716 - accuracy: 0.9481\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.1728 - accuracy: 0.9467\n",
            " 31/469 [>.............................] - ETA: 13s - loss: 0.1742 - accuracy: 0.9463\n",
            " 35/469 [=>............................] - ETA: 13s - loss: 0.1756 - accuracy: 0.9455\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 0.1737 - accuracy: 0.9465\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1708 - accuracy: 0.9475\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.1618 - accuracy: 0.9500\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.1657 - accuracy: 0.9493\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 0.1667 - accuracy: 0.9493\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 0.1684 - accuracy: 0.9496\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.1660 - accuracy: 0.9498\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 0.1681 - accuracy: 0.9505\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.1675 - accuracy: 0.9505\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 0.1670 - accuracy: 0.9503\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.1667 - accuracy: 0.9505\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1660 - accuracy: 0.9507\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1641 - accuracy: 0.9510\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 0.1634 - accuracy: 0.9515\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 0.1617 - accuracy: 0.9517\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 0.1622 - accuracy: 0.9518\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 0.1627 - accuracy: 0.9517\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1639 - accuracy: 0.9514\n",
            "101/469 [=====>........................] - ETA: 11s - loss: 0.1633 - accuracy: 0.9511\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1659 - accuracy: 0.9508\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 0.1660 - accuracy: 0.9507\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 0.1663 - accuracy: 0.9505\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1676 - accuracy: 0.9503\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1688 - accuracy: 0.9502\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1681 - accuracy: 0.9502\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 0.1687 - accuracy: 0.9499\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 0.1691 - accuracy: 0.9499\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 0.1676 - accuracy: 0.9502\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1687 - accuracy: 0.9500 \n",
            "139/469 [=======>......................] - ETA: 9s - loss: 0.1684 - accuracy: 0.9500\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 0.1665 - accuracy: 0.9506\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 0.1667 - accuracy: 0.9505\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 0.1654 - accuracy: 0.9507\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 0.1665 - accuracy: 0.9503\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 0.1659 - accuracy: 0.9504\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 0.1653 - accuracy: 0.9505\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 0.1671 - accuracy: 0.9501\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1680 - accuracy: 0.9499\n",
            "169/469 [=========>....................] - ETA: 9s - loss: 0.1668 - accuracy: 0.9502\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 0.1674 - accuracy: 0.9501\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 0.1675 - accuracy: 0.9501\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 0.1683 - accuracy: 0.9504\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1694 - accuracy: 0.9498\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1695 - accuracy: 0.9496\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 0.1688 - accuracy: 0.9497\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 0.1694 - accuracy: 0.9496\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 0.1698 - accuracy: 0.9496\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 0.1703 - accuracy: 0.9498\n",
            "201/469 [===========>..................] - ETA: 8s - loss: 0.1705 - accuracy: 0.9498\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1714 - accuracy: 0.9497\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1712 - accuracy: 0.9496\n",
            "211/469 [============>.................] - ETA: 7s - loss: 0.1710 - accuracy: 0.9497\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1704 - accuracy: 0.9499\n",
            "215/469 [============>.................] - ETA: 7s - loss: 0.1703 - accuracy: 0.9499\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1703 - accuracy: 0.9498\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1714 - accuracy: 0.9496\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1708 - accuracy: 0.9495\n",
            "231/469 [=============>................] - ETA: 7s - loss: 0.1704 - accuracy: 0.9496\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 0.1705 - accuracy: 0.9496\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1705 - accuracy: 0.9496\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1709 - accuracy: 0.9495\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1709 - accuracy: 0.9496\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 0.1707 - accuracy: 0.9495\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 0.1702 - accuracy: 0.9494\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 0.1698 - accuracy: 0.9496\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1695 - accuracy: 0.9497\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1697 - accuracy: 0.9496\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1700 - accuracy: 0.9496\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1704 - accuracy: 0.9494\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1700 - accuracy: 0.9495\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1704 - accuracy: 0.9494\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1711 - accuracy: 0.9493\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1706 - accuracy: 0.9494\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1708 - accuracy: 0.9494\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1700 - accuracy: 0.9497\n",
            "295/469 [=================>............] - ETA: 5s - loss: 0.1702 - accuracy: 0.9495\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1708 - accuracy: 0.9495\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1704 - accuracy: 0.9495\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1700 - accuracy: 0.9496\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.1700 - accuracy: 0.9497\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.1702 - accuracy: 0.9497\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.1700 - accuracy: 0.9496\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1704 - accuracy: 0.9494\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1698 - accuracy: 0.9495\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1692 - accuracy: 0.9497\n",
            "330/469 [====================>.........] - ETA: 4s - loss: 0.1690 - accuracy: 0.9498\n",
            "332/469 [====================>.........] - ETA: 4s - loss: 0.1691 - accuracy: 0.9497\n",
            "334/469 [====================>.........] - ETA: 4s - loss: 0.1689 - accuracy: 0.9498\n",
            "338/469 [====================>.........] - ETA: 3s - loss: 0.1694 - accuracy: 0.9496\n",
            "342/469 [====================>.........] - ETA: 3s - loss: 0.1698 - accuracy: 0.9494\n",
            "346/469 [=====================>........] - ETA: 3s - loss: 0.1698 - accuracy: 0.9494\n",
            "350/469 [=====================>........] - ETA: 3s - loss: 0.1697 - accuracy: 0.9494\n",
            "354/469 [=====================>........] - ETA: 3s - loss: 0.1692 - accuracy: 0.9496\n",
            "356/469 [=====================>........] - ETA: 3s - loss: 0.1693 - accuracy: 0.9496\n",
            "360/469 [======================>.......] - ETA: 3s - loss: 0.1692 - accuracy: 0.9495\n",
            "364/469 [======================>.......] - ETA: 3s - loss: 0.1696 - accuracy: 0.9494\n",
            "368/469 [======================>.......] - ETA: 3s - loss: 0.1697 - accuracy: 0.9495\n",
            "370/469 [======================>.......] - ETA: 2s - loss: 0.1694 - accuracy: 0.9495\n",
            "374/469 [======================>.......] - ETA: 2s - loss: 0.1700 - accuracy: 0.9494\n",
            "378/469 [=======================>......] - ETA: 2s - loss: 0.1704 - accuracy: 0.9492\n",
            "382/469 [=======================>......] - ETA: 2s - loss: 0.1703 - accuracy: 0.9492\n",
            "386/469 [=======================>......] - ETA: 2s - loss: 0.1708 - accuracy: 0.9491\n",
            "390/469 [=======================>......] - ETA: 2s - loss: 0.1708 - accuracy: 0.9491\n",
            "392/469 [========================>.....] - ETA: 2s - loss: 0.1705 - accuracy: 0.9491\n",
            "396/469 [========================>.....] - ETA: 2s - loss: 0.1702 - accuracy: 0.9492\n",
            "400/469 [========================>.....] - ETA: 2s - loss: 0.1708 - accuracy: 0.9491\n",
            "404/469 [========================>.....] - ETA: 1s - loss: 0.1707 - accuracy: 0.9491\n",
            "408/469 [=========================>....] - ETA: 1s - loss: 0.1709 - accuracy: 0.9489\n",
            "410/469 [=========================>....] - ETA: 1s - loss: 0.1706 - accuracy: 0.9490\n",
            "414/469 [=========================>....] - ETA: 1s - loss: 0.1719 - accuracy: 0.9489\n",
            "418/469 [=========================>....] - ETA: 1s - loss: 0.1724 - accuracy: 0.9488\n",
            "420/469 [=========================>....] - ETA: 1s - loss: 0.1722 - accuracy: 0.9488\n",
            "424/469 [==========================>...] - ETA: 1s - loss: 0.1720 - accuracy: 0.9489\n",
            "428/469 [==========================>...] - ETA: 1s - loss: 0.1717 - accuracy: 0.9489\n",
            "432/469 [==========================>...] - ETA: 1s - loss: 0.1719 - accuracy: 0.9489\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.1724 - accuracy: 0.9488\n",
            "440/469 [===========================>..] - ETA: 0s - loss: 0.1723 - accuracy: 0.9488\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.1723 - accuracy: 0.9488\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.1718 - accuracy: 0.9490\n",
            "450/469 [===========================>..] - ETA: 0s - loss: 0.1718 - accuracy: 0.9490\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.1719 - accuracy: 0.9490\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9490\n",
            "462/469 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9489\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9489\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9489\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1725 - accuracy: 0.9489 - val_loss: 0.1639 - val_accuracy: 0.9560\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 5/12\n",
            "  3/469 [..............................] - ETA: 13s - loss: 0.1760 - accuracy: 0.9557\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.1598 - accuracy: 0.9598\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.1645 - accuracy: 0.9567\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1650 - accuracy: 0.9563\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 0.1572 - accuracy: 0.9577\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 0.1549 - accuracy: 0.9568\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.1479 - accuracy: 0.9584\n",
            " 29/469 [>.............................] - ETA: 12s - loss: 0.1477 - accuracy: 0.9582\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 0.1431 - accuracy: 0.9593\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 0.1449 - accuracy: 0.9592\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 0.1433 - accuracy: 0.9585\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1414 - accuracy: 0.9584\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.1410 - accuracy: 0.9584\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.1404 - accuracy: 0.9589\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 0.1405 - accuracy: 0.9587\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 0.1442 - accuracy: 0.9585\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 0.1460 - accuracy: 0.9583\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.1495 - accuracy: 0.9575\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 0.1465 - accuracy: 0.9573\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.1464 - accuracy: 0.9577\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 0.1459 - accuracy: 0.9582\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.1489 - accuracy: 0.9579\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1482 - accuracy: 0.9576\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 0.1482 - accuracy: 0.9571\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1484 - accuracy: 0.9573\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 0.1486 - accuracy: 0.9564\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 0.1488 - accuracy: 0.9559\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 0.1479 - accuracy: 0.9562\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1501 - accuracy: 0.9553\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 0.1514 - accuracy: 0.9552\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 0.1542 - accuracy: 0.9540\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 0.1557 - accuracy: 0.9536\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 0.1566 - accuracy: 0.9535\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1570 - accuracy: 0.9536\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 0.1561 - accuracy: 0.9537\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 0.1559 - accuracy: 0.9538\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 0.1554 - accuracy: 0.9541\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 0.1554 - accuracy: 0.9540\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 0.1549 - accuracy: 0.9542\n",
            "135/469 [=======>......................] - ETA: 9s - loss: 0.1555 - accuracy: 0.9541 \n",
            "139/469 [=======>......................] - ETA: 9s - loss: 0.1536 - accuracy: 0.9545\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 0.1527 - accuracy: 0.9549\n",
            "146/469 [========>.....................] - ETA: 10s - loss: 0.1531 - accuracy: 0.9547\n",
            "150/469 [========>.....................] - ETA: 10s - loss: 0.1529 - accuracy: 0.9545\n",
            "152/469 [========>.....................] - ETA: 9s - loss: 0.1541 - accuracy: 0.9543 \n",
            "156/469 [========>.....................] - ETA: 9s - loss: 0.1537 - accuracy: 0.9543\n",
            "160/469 [=========>....................] - ETA: 9s - loss: 0.1536 - accuracy: 0.9540\n",
            "164/469 [=========>....................] - ETA: 9s - loss: 0.1545 - accuracy: 0.9539\n",
            "166/469 [=========>....................] - ETA: 9s - loss: 0.1546 - accuracy: 0.9538\n",
            "168/469 [=========>....................] - ETA: 9s - loss: 0.1561 - accuracy: 0.9537\n",
            "170/469 [=========>....................] - ETA: 9s - loss: 0.1565 - accuracy: 0.9536\n",
            "174/469 [==========>...................] - ETA: 9s - loss: 0.1572 - accuracy: 0.9532\n",
            "178/469 [==========>...................] - ETA: 9s - loss: 0.1585 - accuracy: 0.9527\n",
            "182/469 [==========>...................] - ETA: 8s - loss: 0.1601 - accuracy: 0.9521\n",
            "184/469 [==========>...................] - ETA: 8s - loss: 0.1606 - accuracy: 0.9519\n",
            "188/469 [===========>..................] - ETA: 8s - loss: 0.1608 - accuracy: 0.9519\n",
            "192/469 [===========>..................] - ETA: 8s - loss: 0.1617 - accuracy: 0.9519\n",
            "196/469 [===========>..................] - ETA: 8s - loss: 0.1625 - accuracy: 0.9519\n",
            "200/469 [===========>..................] - ETA: 8s - loss: 0.1637 - accuracy: 0.9515\n",
            "202/469 [===========>..................] - ETA: 8s - loss: 0.1630 - accuracy: 0.9517\n",
            "206/469 [============>.................] - ETA: 8s - loss: 0.1632 - accuracy: 0.9515\n",
            "210/469 [============>.................] - ETA: 8s - loss: 0.1635 - accuracy: 0.9513\n",
            "214/469 [============>.................] - ETA: 7s - loss: 0.1631 - accuracy: 0.9516\n",
            "216/469 [============>.................] - ETA: 7s - loss: 0.1641 - accuracy: 0.9512\n",
            "220/469 [=============>................] - ETA: 7s - loss: 0.1661 - accuracy: 0.9511\n",
            "224/469 [=============>................] - ETA: 7s - loss: 0.1663 - accuracy: 0.9511\n",
            "228/469 [=============>................] - ETA: 7s - loss: 0.1670 - accuracy: 0.9508\n",
            "230/469 [=============>................] - ETA: 7s - loss: 0.1670 - accuracy: 0.9507\n",
            "232/469 [=============>................] - ETA: 7s - loss: 0.1670 - accuracy: 0.9508\n",
            "234/469 [=============>................] - ETA: 7s - loss: 0.1667 - accuracy: 0.9508\n",
            "238/469 [==============>...............] - ETA: 7s - loss: 0.1670 - accuracy: 0.9505\n",
            "242/469 [==============>...............] - ETA: 7s - loss: 0.1686 - accuracy: 0.9502\n",
            "246/469 [==============>...............] - ETA: 6s - loss: 0.1689 - accuracy: 0.9501\n",
            "248/469 [==============>...............] - ETA: 6s - loss: 0.1687 - accuracy: 0.9501\n",
            "252/469 [===============>..............] - ETA: 6s - loss: 0.1677 - accuracy: 0.9503\n",
            "256/469 [===============>..............] - ETA: 6s - loss: 0.1687 - accuracy: 0.9502\n",
            "260/469 [===============>..............] - ETA: 6s - loss: 0.1684 - accuracy: 0.9502\n",
            "264/469 [===============>..............] - ETA: 6s - loss: 0.1687 - accuracy: 0.9500\n",
            "268/469 [================>.............] - ETA: 6s - loss: 0.1687 - accuracy: 0.9500\n",
            "270/469 [================>.............] - ETA: 6s - loss: 0.1684 - accuracy: 0.9501\n",
            "274/469 [================>.............] - ETA: 6s - loss: 0.1687 - accuracy: 0.9501\n",
            "278/469 [================>.............] - ETA: 5s - loss: 0.1686 - accuracy: 0.9499\n",
            "282/469 [=================>............] - ETA: 5s - loss: 0.1688 - accuracy: 0.9498\n",
            "284/469 [=================>............] - ETA: 5s - loss: 0.1689 - accuracy: 0.9497\n",
            "288/469 [=================>............] - ETA: 5s - loss: 0.1705 - accuracy: 0.9497\n",
            "292/469 [=================>............] - ETA: 5s - loss: 0.1697 - accuracy: 0.9500\n",
            "296/469 [=================>............] - ETA: 5s - loss: 0.1704 - accuracy: 0.9498\n",
            "298/469 [==================>...........] - ETA: 5s - loss: 0.1705 - accuracy: 0.9498\n",
            "302/469 [==================>...........] - ETA: 5s - loss: 0.1718 - accuracy: 0.9495\n",
            "306/469 [==================>...........] - ETA: 5s - loss: 0.1713 - accuracy: 0.9498\n",
            "310/469 [==================>...........] - ETA: 4s - loss: 0.1723 - accuracy: 0.9496\n",
            "314/469 [===================>..........] - ETA: 4s - loss: 0.1727 - accuracy: 0.9494\n",
            "316/469 [===================>..........] - ETA: 4s - loss: 0.1729 - accuracy: 0.9493\n",
            "320/469 [===================>..........] - ETA: 4s - loss: 0.1725 - accuracy: 0.9493\n",
            "324/469 [===================>..........] - ETA: 4s - loss: 0.1719 - accuracy: 0.9494\n",
            "328/469 [===================>..........] - ETA: 4s - loss: 0.1725 - accuracy: 0.9491\n",
            "330/469 [====================>.........] - ETA: 4s - loss: 0.1725 - accuracy: 0.9491\n",
            "334/469 [====================>.........] - ETA: 4s - loss: 0.1724 - accuracy: 0.9491\n",
            "338/469 [====================>.........] - ETA: 4s - loss: 0.1725 - accuracy: 0.9491\n",
            "342/469 [====================>.........] - ETA: 3s - loss: 0.1726 - accuracy: 0.9492\n",
            "344/469 [=====================>........] - ETA: 3s - loss: 0.1729 - accuracy: 0.9491\n",
            "348/469 [=====================>........] - ETA: 3s - loss: 0.1726 - accuracy: 0.9490\n",
            "352/469 [=====================>........] - ETA: 3s - loss: 0.1726 - accuracy: 0.9490\n",
            "356/469 [=====================>........] - ETA: 3s - loss: 0.1727 - accuracy: 0.9489\n",
            "358/469 [=====================>........] - ETA: 3s - loss: 0.1732 - accuracy: 0.9489\n",
            "362/469 [======================>.......] - ETA: 3s - loss: 0.1733 - accuracy: 0.9490\n",
            "366/469 [======================>.......] - ETA: 3s - loss: 0.1736 - accuracy: 0.9489\n",
            "368/469 [======================>.......] - ETA: 3s - loss: 0.1734 - accuracy: 0.9490\n",
            "370/469 [======================>.......] - ETA: 3s - loss: 0.1733 - accuracy: 0.9490\n",
            "372/469 [======================>.......] - ETA: 2s - loss: 0.1735 - accuracy: 0.9490\n",
            "376/469 [=======================>......] - ETA: 2s - loss: 0.1735 - accuracy: 0.9490\n",
            "380/469 [=======================>......] - ETA: 2s - loss: 0.1733 - accuracy: 0.9491\n",
            "384/469 [=======================>......] - ETA: 2s - loss: 0.1730 - accuracy: 0.9492\n",
            "386/469 [=======================>......] - ETA: 2s - loss: 0.1728 - accuracy: 0.9493\n",
            "390/469 [=======================>......] - ETA: 2s - loss: 0.1727 - accuracy: 0.9493\n",
            "394/469 [========================>.....] - ETA: 2s - loss: 0.1727 - accuracy: 0.9493\n",
            "398/469 [========================>.....] - ETA: 2s - loss: 0.1730 - accuracy: 0.9492\n",
            "400/469 [========================>.....] - ETA: 2s - loss: 0.1730 - accuracy: 0.9493\n",
            "402/469 [========================>.....] - ETA: 2s - loss: 0.1729 - accuracy: 0.9493\n",
            "404/469 [========================>.....] - ETA: 1s - loss: 0.1728 - accuracy: 0.9493\n",
            "408/469 [=========================>....] - ETA: 1s - loss: 0.1725 - accuracy: 0.9493\n",
            "412/469 [=========================>....] - ETA: 1s - loss: 0.1723 - accuracy: 0.9494\n",
            "416/469 [=========================>....] - ETA: 1s - loss: 0.1725 - accuracy: 0.9494\n",
            "418/469 [=========================>....] - ETA: 1s - loss: 0.1721 - accuracy: 0.9495\n",
            "422/469 [=========================>....] - ETA: 1s - loss: 0.1722 - accuracy: 0.9496\n",
            "426/469 [==========================>...] - ETA: 1s - loss: 0.1735 - accuracy: 0.9495\n",
            "430/469 [==========================>...] - ETA: 1s - loss: 0.1740 - accuracy: 0.9494\n",
            "434/469 [==========================>...] - ETA: 1s - loss: 0.1745 - accuracy: 0.9493\n",
            "436/469 [==========================>...] - ETA: 1s - loss: 0.1745 - accuracy: 0.9493\n",
            "438/469 [===========================>..] - ETA: 0s - loss: 0.1745 - accuracy: 0.9492\n",
            "440/469 [===========================>..] - ETA: 0s - loss: 0.1747 - accuracy: 0.9491\n",
            "444/469 [===========================>..] - ETA: 0s - loss: 0.1748 - accuracy: 0.9491\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.1747 - accuracy: 0.9492\n",
            "452/469 [===========================>..] - ETA: 0s - loss: 0.1739 - accuracy: 0.9494\n",
            "456/469 [============================>.] - ETA: 0s - loss: 0.1737 - accuracy: 0.9494\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.1733 - accuracy: 0.9495\n",
            "462/469 [============================>.] - ETA: 0s - loss: 0.1740 - accuracy: 0.9494\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.1742 - accuracy: 0.9494\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.9493\n",
            "469/469 [==============================] - 15s 32ms/step - loss: 0.1746 - accuracy: 0.9492 - val_loss: 0.1373 - val_accuracy: 0.9655\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 6/12\n",
            "  1/469 [..............................] - ETA: 13s - loss: 0.2259 - accuracy: 0.9375\n",
            "  3/469 [..............................] - ETA: 13s - loss: 0.1971 - accuracy: 0.9479\n",
            "  5/469 [..............................] - ETA: 13s - loss: 0.1811 - accuracy: 0.9484\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.1723 - accuracy: 0.9453\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.1664 - accuracy: 0.9467\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1629 - accuracy: 0.9479\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 0.1634 - accuracy: 0.9502\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 0.1619 - accuracy: 0.9514\n",
            " 27/469 [>.............................] - ETA: 13s - loss: 0.1607 - accuracy: 0.9520\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.1581 - accuracy: 0.9534\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 0.1673 - accuracy: 0.9498\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 0.1658 - accuracy: 0.9504\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 0.1629 - accuracy: 0.9506\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 0.1617 - accuracy: 0.9500\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 0.1600 - accuracy: 0.9504\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.1572 - accuracy: 0.9508\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 0.1590 - accuracy: 0.9511\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 0.1586 - accuracy: 0.9510\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 0.1581 - accuracy: 0.9510\n",
            " 63/469 [===>..........................] - ETA: 11s - loss: 0.1557 - accuracy: 0.9510\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 0.1534 - accuracy: 0.9517\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.1555 - accuracy: 0.9513\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 0.1589 - accuracy: 0.9516\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 0.1584 - accuracy: 0.9517\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 0.1568 - accuracy: 0.9517\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 0.1597 - accuracy: 0.9511\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 0.1609 - accuracy: 0.9507\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 0.1607 - accuracy: 0.9504\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 0.1637 - accuracy: 0.9493\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 0.1633 - accuracy: 0.9495\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 0.1640 - accuracy: 0.9495\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 0.1644 - accuracy: 0.9495\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1634 - accuracy: 0.9496\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 0.1641 - accuracy: 0.9498\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 0.1648 - accuracy: 0.9498\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1642 - accuracy: 0.9499\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 0.1636 - accuracy: 0.9499\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1631 - accuracy: 0.9500\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1611 - accuracy: 0.9506\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 0.1618 - accuracy: 0.9508\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 0.1619 - accuracy: 0.9509\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 0.1626 - accuracy: 0.9507\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1613 - accuracy: 0.9511\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 0.1620 - accuracy: 0.9514\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 0.1630 - accuracy: 0.9516\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 0.1623 - accuracy: 0.9514\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 0.1623 - accuracy: 0.9515\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 0.1627 - accuracy: 0.9511\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 0.1621 - accuracy: 0.9515\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 0.1614 - accuracy: 0.9516\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1621 - accuracy: 0.9514\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 0.1616 - accuracy: 0.9514\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 0.1630 - accuracy: 0.9510\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 0.1634 - accuracy: 0.9507\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 0.1641 - accuracy: 0.9505\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1638 - accuracy: 0.9506\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1668 - accuracy: 0.9499\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 0.1693 - accuracy: 0.9494\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1706 - accuracy: 0.9486\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 0.1711 - accuracy: 0.9486\n",
            "201/469 [===========>..................] - ETA: 8s - loss: 0.1720 - accuracy: 0.9483\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1730 - accuracy: 0.9481\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1736 - accuracy: 0.9480\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1750 - accuracy: 0.9478\n",
            "215/469 [============>.................] - ETA: 7s - loss: 0.1762 - accuracy: 0.9476\n",
            "217/469 [============>.................] - ETA: 7s - loss: 0.1767 - accuracy: 0.9474\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1773 - accuracy: 0.9473\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1793 - accuracy: 0.9469\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1798 - accuracy: 0.9468\n",
            "231/469 [=============>................] - ETA: 7s - loss: 0.1800 - accuracy: 0.9468\n",
            "233/469 [=============>................] - ETA: 7s - loss: 0.1799 - accuracy: 0.9469\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 0.1796 - accuracy: 0.9469\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1794 - accuracy: 0.9469\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1803 - accuracy: 0.9468\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1802 - accuracy: 0.9468\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 0.1811 - accuracy: 0.9466\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 0.1803 - accuracy: 0.9467\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 0.1811 - accuracy: 0.9464\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1825 - accuracy: 0.9461\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1829 - accuracy: 0.9460\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1835 - accuracy: 0.9460\n",
            "269/469 [================>.............] - ETA: 5s - loss: 0.1841 - accuracy: 0.9459\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1856 - accuracy: 0.9457\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1864 - accuracy: 0.9456\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1873 - accuracy: 0.9452\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1884 - accuracy: 0.9449\n",
            "287/469 [=================>............] - ETA: 5s - loss: 0.1893 - accuracy: 0.9447\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1887 - accuracy: 0.9448\n",
            "291/469 [=================>............] - ETA: 5s - loss: 0.1884 - accuracy: 0.9448\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1880 - accuracy: 0.9448\n",
            "295/469 [=================>............] - ETA: 5s - loss: 0.1874 - accuracy: 0.9450\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1880 - accuracy: 0.9452\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1884 - accuracy: 0.9450\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1890 - accuracy: 0.9449\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1894 - accuracy: 0.9447\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.1891 - accuracy: 0.9448\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.1892 - accuracy: 0.9449\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1893 - accuracy: 0.9450\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1903 - accuracy: 0.9448\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1900 - accuracy: 0.9449\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 0.1902 - accuracy: 0.9449\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 0.1901 - accuracy: 0.9450\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.1897 - accuracy: 0.9453\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1896 - accuracy: 0.9453\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1897 - accuracy: 0.9452\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.1895 - accuracy: 0.9453\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 0.1893 - accuracy: 0.9454\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 0.1888 - accuracy: 0.9456\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1893 - accuracy: 0.9453\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1888 - accuracy: 0.9454\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.1883 - accuracy: 0.9456\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 0.1883 - accuracy: 0.9455\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1883 - accuracy: 0.9455\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1878 - accuracy: 0.9455\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1877 - accuracy: 0.9455\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.1869 - accuracy: 0.9456\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.1865 - accuracy: 0.9458\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.1863 - accuracy: 0.9458\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 0.1863 - accuracy: 0.9459\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1856 - accuracy: 0.9460\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.1855 - accuracy: 0.9459\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 0.1855 - accuracy: 0.9460\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 0.1863 - accuracy: 0.9459\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.1864 - accuracy: 0.9458\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1869 - accuracy: 0.9457\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.1861 - accuracy: 0.9459\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.1867 - accuracy: 0.9458\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.1864 - accuracy: 0.9459\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 0.1868 - accuracy: 0.9459\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1867 - accuracy: 0.9460\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.1876 - accuracy: 0.9459\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.1873 - accuracy: 0.9459\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.1875 - accuracy: 0.9458\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.1878 - accuracy: 0.9459\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9460\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9460\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9458\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9460\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9459\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1876 - accuracy: 0.9459 - val_loss: 0.1642 - val_accuracy: 0.9589\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 7/12\n",
            "  1/469 [..............................] - ETA: 14s - loss: 0.1579 - accuracy: 0.9375\n",
            "  3/469 [..............................] - ETA: 13s - loss: 0.1440 - accuracy: 0.9557\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.1542 - accuracy: 0.9531\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.1522 - accuracy: 0.9524\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1579 - accuracy: 0.9516\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 0.1647 - accuracy: 0.9494\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 0.1608 - accuracy: 0.9513\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.1625 - accuracy: 0.9503\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.1644 - accuracy: 0.9488\n",
            " 33/469 [=>............................] - ETA: 13s - loss: 0.1607 - accuracy: 0.9491\n",
            " 35/469 [=>............................] - ETA: 12s - loss: 0.1585 - accuracy: 0.9496\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 0.1548 - accuracy: 0.9511\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1499 - accuracy: 0.9519\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.1514 - accuracy: 0.9516\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.1540 - accuracy: 0.9522\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 0.1563 - accuracy: 0.9519\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 0.1536 - accuracy: 0.9530\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.1520 - accuracy: 0.9529\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 0.1498 - accuracy: 0.9536\n",
            " 67/469 [===>..........................] - ETA: 12s - loss: 0.1484 - accuracy: 0.9536\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.1491 - accuracy: 0.9535\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 0.1480 - accuracy: 0.9540\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.1461 - accuracy: 0.9546\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1426 - accuracy: 0.9558\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1410 - accuracy: 0.9559\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 0.1418 - accuracy: 0.9561\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 0.1414 - accuracy: 0.9565\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 0.1406 - accuracy: 0.9570\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1403 - accuracy: 0.9572\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 0.1428 - accuracy: 0.9568\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1411 - accuracy: 0.9575\n",
            "107/469 [=====>........................] - ETA: 10s - loss: 0.1405 - accuracy: 0.9574\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 0.1421 - accuracy: 0.9569\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1420 - accuracy: 0.9568\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1388 - accuracy: 0.9580\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1389 - accuracy: 0.9580\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 0.1390 - accuracy: 0.9580\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 0.1390 - accuracy: 0.9582\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 0.1413 - accuracy: 0.9574\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1429 - accuracy: 0.9572\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 0.1427 - accuracy: 0.9573\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 0.1441 - accuracy: 0.9571\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 0.1448 - accuracy: 0.9570\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 0.1446 - accuracy: 0.9571\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 0.1444 - accuracy: 0.9571\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 0.1444 - accuracy: 0.9571\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 0.1436 - accuracy: 0.9573\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1439 - accuracy: 0.9571\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 0.1442 - accuracy: 0.9574\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 0.1447 - accuracy: 0.9575\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 0.1460 - accuracy: 0.9568\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 0.1448 - accuracy: 0.9571\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1455 - accuracy: 0.9567\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1458 - accuracy: 0.9566\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 0.1461 - accuracy: 0.9565\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1470 - accuracy: 0.9564\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 0.1468 - accuracy: 0.9565\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 0.1468 - accuracy: 0.9563\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1478 - accuracy: 0.9563\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1490 - accuracy: 0.9560\n",
            "211/469 [============>.................] - ETA: 7s - loss: 0.1487 - accuracy: 0.9562\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1492 - accuracy: 0.9558\n",
            "215/469 [============>.................] - ETA: 7s - loss: 0.1496 - accuracy: 0.9557\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1495 - accuracy: 0.9557\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1493 - accuracy: 0.9558\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1514 - accuracy: 0.9555\n",
            "229/469 [=============>................] - ETA: 7s - loss: 0.1512 - accuracy: 0.9556\n",
            "233/469 [=============>................] - ETA: 7s - loss: 0.1516 - accuracy: 0.9554\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1522 - accuracy: 0.9554\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1528 - accuracy: 0.9551\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1541 - accuracy: 0.9547\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 0.1536 - accuracy: 0.9547\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 0.1534 - accuracy: 0.9548\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 0.1540 - accuracy: 0.9546\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1540 - accuracy: 0.9546\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1532 - accuracy: 0.9547\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1534 - accuracy: 0.9547\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1538 - accuracy: 0.9547\n",
            "275/469 [================>.............] - ETA: 5s - loss: 0.1541 - accuracy: 0.9547\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1545 - accuracy: 0.9547\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1549 - accuracy: 0.9545\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1553 - accuracy: 0.9546\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1549 - accuracy: 0.9547\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1554 - accuracy: 0.9544\n",
            "297/469 [=================>............] - ETA: 5s - loss: 0.1557 - accuracy: 0.9542\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1559 - accuracy: 0.9541\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 0.1555 - accuracy: 0.9542\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1553 - accuracy: 0.9542\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1561 - accuracy: 0.9541\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1563 - accuracy: 0.9542\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.1560 - accuracy: 0.9541\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.1566 - accuracy: 0.9540\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1577 - accuracy: 0.9536\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1576 - accuracy: 0.9536\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 0.1575 - accuracy: 0.9536\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 0.1575 - accuracy: 0.9535\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 0.1579 - accuracy: 0.9535\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.1578 - accuracy: 0.9535\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1575 - accuracy: 0.9535\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.1579 - accuracy: 0.9534\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1576 - accuracy: 0.9534\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 0.1581 - accuracy: 0.9535\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 0.1582 - accuracy: 0.9536\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 0.1585 - accuracy: 0.9535\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1587 - accuracy: 0.9534\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1600 - accuracy: 0.9531\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 0.1603 - accuracy: 0.9530\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.1603 - accuracy: 0.9530\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 0.1606 - accuracy: 0.9530\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1600 - accuracy: 0.9532\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1608 - accuracy: 0.9531\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1608 - accuracy: 0.9530\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.1615 - accuracy: 0.9527\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.1616 - accuracy: 0.9526\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.1615 - accuracy: 0.9527\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.1615 - accuracy: 0.9527\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 0.1620 - accuracy: 0.9526\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1620 - accuracy: 0.9526\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.1616 - accuracy: 0.9528\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.1616 - accuracy: 0.9527\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 0.1620 - accuracy: 0.9526\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.1635 - accuracy: 0.9524\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1647 - accuracy: 0.9521\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.1661 - accuracy: 0.9516\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.1665 - accuracy: 0.9515\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.1668 - accuracy: 0.9513\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 0.1670 - accuracy: 0.9513\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1672 - accuracy: 0.9512\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.1672 - accuracy: 0.9511\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.1667 - accuracy: 0.9512\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.1682 - accuracy: 0.9510\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.1694 - accuracy: 0.9508\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.9507\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9507\n",
            "461/469 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9505\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.1721 - accuracy: 0.9505\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.9503\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1728 - accuracy: 0.9503 - val_loss: 0.2154 - val_accuracy: 0.9512\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 8/12\n",
            "  1/469 [..............................] - ETA: 13s - loss: 0.2439 - accuracy: 0.9453\n",
            "  3/469 [..............................] - ETA: 13s - loss: 0.2568 - accuracy: 0.9453\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.2827 - accuracy: 0.9342\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.2684 - accuracy: 0.9318\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.2451 - accuracy: 0.9339\n",
            " 17/469 [>.............................] - ETA: 13s - loss: 0.2520 - accuracy: 0.9329\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 0.2519 - accuracy: 0.9322\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 0.2448 - accuracy: 0.9312\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.2319 - accuracy: 0.9341\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.2241 - accuracy: 0.9353\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 0.2204 - accuracy: 0.9349\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 0.2150 - accuracy: 0.9360\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 0.2097 - accuracy: 0.9377\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 0.2085 - accuracy: 0.9379\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.2064 - accuracy: 0.9391\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.2112 - accuracy: 0.9378\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.2070 - accuracy: 0.9384\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 0.2009 - accuracy: 0.9403\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 0.2043 - accuracy: 0.9400\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.2003 - accuracy: 0.9411\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 0.2012 - accuracy: 0.9415\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.2023 - accuracy: 0.9413\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 0.2012 - accuracy: 0.9412\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.2004 - accuracy: 0.9415\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1977 - accuracy: 0.9425\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1981 - accuracy: 0.9422\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 0.1971 - accuracy: 0.9419\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 0.1941 - accuracy: 0.9429\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 0.1955 - accuracy: 0.9425\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1941 - accuracy: 0.9430\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 0.1938 - accuracy: 0.9429\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1909 - accuracy: 0.9435\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 0.1925 - accuracy: 0.9436\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 0.1918 - accuracy: 0.9440\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1920 - accuracy: 0.9439\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1940 - accuracy: 0.9433\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1939 - accuracy: 0.9432\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 0.1938 - accuracy: 0.9436\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 0.1924 - accuracy: 0.9438\n",
            "133/469 [=======>......................] - ETA: 9s - loss: 0.1935 - accuracy: 0.9438 \n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1951 - accuracy: 0.9437\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 0.1957 - accuracy: 0.9438\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 0.1949 - accuracy: 0.9439\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 0.1927 - accuracy: 0.9443\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 0.1926 - accuracy: 0.9442\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 0.1921 - accuracy: 0.9444\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 0.1914 - accuracy: 0.9444\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 0.1900 - accuracy: 0.9447\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 0.1888 - accuracy: 0.9453\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 0.1881 - accuracy: 0.9458\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 0.1872 - accuracy: 0.9459\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 0.1867 - accuracy: 0.9460\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 0.1872 - accuracy: 0.9461\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 0.1863 - accuracy: 0.9463\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 0.1867 - accuracy: 0.9462\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 0.1862 - accuracy: 0.9461\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 0.1856 - accuracy: 0.9461\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1856 - accuracy: 0.9460\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 0.1850 - accuracy: 0.9463\n",
            "203/469 [===========>..................] - ETA: 7s - loss: 0.1853 - accuracy: 0.9462\n",
            "207/469 [============>.................] - ETA: 7s - loss: 0.1863 - accuracy: 0.9460\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1864 - accuracy: 0.9458\n",
            "211/469 [============>.................] - ETA: 7s - loss: 0.1862 - accuracy: 0.9458\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1863 - accuracy: 0.9458\n",
            "217/469 [============>.................] - ETA: 7s - loss: 0.1876 - accuracy: 0.9456\n",
            "221/469 [=============>................] - ETA: 7s - loss: 0.1870 - accuracy: 0.9456\n",
            "225/469 [=============>................] - ETA: 7s - loss: 0.1861 - accuracy: 0.9458\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1858 - accuracy: 0.9460\n",
            "231/469 [=============>................] - ETA: 7s - loss: 0.1861 - accuracy: 0.9462\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 0.1868 - accuracy: 0.9462\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 0.1863 - accuracy: 0.9465\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1862 - accuracy: 0.9465\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1858 - accuracy: 0.9466\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 0.1866 - accuracy: 0.9467\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 0.1868 - accuracy: 0.9467\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 0.1864 - accuracy: 0.9467\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1868 - accuracy: 0.9467\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1876 - accuracy: 0.9467\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1866 - accuracy: 0.9469\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1876 - accuracy: 0.9465\n",
            "275/469 [================>.............] - ETA: 5s - loss: 0.1885 - accuracy: 0.9460\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1889 - accuracy: 0.9460\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1902 - accuracy: 0.9458\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1899 - accuracy: 0.9459\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1893 - accuracy: 0.9460\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1898 - accuracy: 0.9461\n",
            "295/469 [=================>............] - ETA: 5s - loss: 0.1904 - accuracy: 0.9461\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1907 - accuracy: 0.9462\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1922 - accuracy: 0.9461\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1927 - accuracy: 0.9460\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.1930 - accuracy: 0.9459\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.1941 - accuracy: 0.9455\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.1944 - accuracy: 0.9454\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1944 - accuracy: 0.9454\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1948 - accuracy: 0.9456\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1952 - accuracy: 0.9456\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 0.1950 - accuracy: 0.9458\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.1954 - accuracy: 0.9457\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.1958 - accuracy: 0.9457\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1957 - accuracy: 0.9456\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1966 - accuracy: 0.9455\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.1961 - accuracy: 0.9457\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 0.1953 - accuracy: 0.9457\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 0.1956 - accuracy: 0.9457\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1959 - accuracy: 0.9457\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1956 - accuracy: 0.9456\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 0.1962 - accuracy: 0.9455\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 0.1962 - accuracy: 0.9454\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1964 - accuracy: 0.9453\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1966 - accuracy: 0.9453\n",
            "381/469 [=======================>......] - ETA: 2s - loss: 0.1961 - accuracy: 0.9453\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1959 - accuracy: 0.9453\n",
            "385/469 [=======================>......] - ETA: 2s - loss: 0.1962 - accuracy: 0.9453\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.1963 - accuracy: 0.9451\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.1959 - accuracy: 0.9451\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 0.1952 - accuracy: 0.9453\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 0.1952 - accuracy: 0.9453\n",
            "403/469 [========================>.....] - ETA: 1s - loss: 0.1945 - accuracy: 0.9455\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 0.1942 - accuracy: 0.9456\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 0.1940 - accuracy: 0.9456\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 0.1939 - accuracy: 0.9456\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.1940 - accuracy: 0.9457\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 0.1942 - accuracy: 0.9457\n",
            "421/469 [=========================>....] - ETA: 1s - loss: 0.1942 - accuracy: 0.9456\n",
            "425/469 [==========================>...] - ETA: 1s - loss: 0.1940 - accuracy: 0.9457\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.1933 - accuracy: 0.9458\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 0.1935 - accuracy: 0.9459\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 0.1935 - accuracy: 0.9459\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.1932 - accuracy: 0.9460\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.1929 - accuracy: 0.9461\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.1929 - accuracy: 0.9460\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.1930 - accuracy: 0.9459\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.1928 - accuracy: 0.9460\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9460\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1929 - accuracy: 0.9461\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9461\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.1925 - accuracy: 0.9462\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9462\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1923 - accuracy: 0.9462 - val_loss: 0.1704 - val_accuracy: 0.9503\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 9/12\n",
            "  1/469 [..............................] - ETA: 13s - loss: 0.0657 - accuracy: 0.9844\n",
            "  5/469 [..............................] - ETA: 14s - loss: 0.1690 - accuracy: 0.9563\n",
            "  7/469 [..............................] - ETA: 14s - loss: 0.1701 - accuracy: 0.9587\n",
            "  9/469 [..............................] - ETA: 14s - loss: 0.1671 - accuracy: 0.9557\n",
            " 11/469 [..............................] - ETA: 14s - loss: 0.1678 - accuracy: 0.9560\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1727 - accuracy: 0.9568\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 0.1602 - accuracy: 0.9585\n",
            " 23/469 [>.............................] - ETA: 13s - loss: 0.1600 - accuracy: 0.9596\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.1585 - accuracy: 0.9594\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.1562 - accuracy: 0.9593\n",
            " 33/469 [=>............................] - ETA: 13s - loss: 0.1569 - accuracy: 0.9593\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 0.1567 - accuracy: 0.9590\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 0.1596 - accuracy: 0.9571\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1580 - accuracy: 0.9566\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 0.1565 - accuracy: 0.9564\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.1589 - accuracy: 0.9561\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.1559 - accuracy: 0.9566\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 0.1608 - accuracy: 0.9555\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 0.1643 - accuracy: 0.9543\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.1682 - accuracy: 0.9527\n",
            " 63/469 [===>..........................] - ETA: 12s - loss: 0.1686 - accuracy: 0.9525\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 0.1700 - accuracy: 0.9519\n",
            " 69/469 [===>..........................] - ETA: 12s - loss: 0.1671 - accuracy: 0.9526\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 0.1682 - accuracy: 0.9527\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.1652 - accuracy: 0.9532\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1692 - accuracy: 0.9524\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1664 - accuracy: 0.9528\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 0.1658 - accuracy: 0.9528\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 0.1639 - accuracy: 0.9535\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 0.1659 - accuracy: 0.9530\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1676 - accuracy: 0.9524\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 0.1685 - accuracy: 0.9523\n",
            "103/469 [=====>........................] - ETA: 11s - loss: 0.1684 - accuracy: 0.9524\n",
            "107/469 [=====>........................] - ETA: 11s - loss: 0.1702 - accuracy: 0.9521\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 0.1711 - accuracy: 0.9518\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1706 - accuracy: 0.9519\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 0.1705 - accuracy: 0.9518\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 0.1709 - accuracy: 0.9514\n",
            "125/469 [======>.......................] - ETA: 10s - loss: 0.1702 - accuracy: 0.9517\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 0.1700 - accuracy: 0.9516\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 0.1687 - accuracy: 0.9514\n",
            "135/469 [=======>......................] - ETA: 10s - loss: 0.1695 - accuracy: 0.9513\n",
            "139/469 [=======>......................] - ETA: 10s - loss: 0.1693 - accuracy: 0.9513\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 0.1683 - accuracy: 0.9516\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 0.1686 - accuracy: 0.9517\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 0.1688 - accuracy: 0.9516\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 0.1688 - accuracy: 0.9515\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 0.1679 - accuracy: 0.9517\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 0.1682 - accuracy: 0.9516\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1700 - accuracy: 0.9508\n",
            "167/469 [=========>....................] - ETA: 9s - loss: 0.1704 - accuracy: 0.9506\n",
            "171/469 [=========>....................] - ETA: 9s - loss: 0.1705 - accuracy: 0.9507\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 0.1702 - accuracy: 0.9509\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 0.1704 - accuracy: 0.9504\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 0.1698 - accuracy: 0.9507\n",
            "185/469 [==========>...................] - ETA: 8s - loss: 0.1734 - accuracy: 0.9503\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 0.1752 - accuracy: 0.9502\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 0.1771 - accuracy: 0.9498\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1786 - accuracy: 0.9496\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 0.1787 - accuracy: 0.9493\n",
            "203/469 [===========>..................] - ETA: 8s - loss: 0.1789 - accuracy: 0.9490\n",
            "207/469 [============>.................] - ETA: 7s - loss: 0.1801 - accuracy: 0.9486\n",
            "211/469 [============>.................] - ETA: 7s - loss: 0.1809 - accuracy: 0.9484\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1803 - accuracy: 0.9485\n",
            "217/469 [============>.................] - ETA: 7s - loss: 0.1812 - accuracy: 0.9485\n",
            "221/469 [=============>................] - ETA: 7s - loss: 0.1815 - accuracy: 0.9484\n",
            "225/469 [=============>................] - ETA: 7s - loss: 0.1816 - accuracy: 0.9483\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1817 - accuracy: 0.9483\n",
            "231/469 [=============>................] - ETA: 7s - loss: 0.1799 - accuracy: 0.9486\n",
            "235/469 [==============>...............] - ETA: 7s - loss: 0.1803 - accuracy: 0.9486\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 0.1819 - accuracy: 0.9484\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 0.1812 - accuracy: 0.9487\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1811 - accuracy: 0.9485\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 0.1807 - accuracy: 0.9487\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 0.1803 - accuracy: 0.9487\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 0.1812 - accuracy: 0.9484\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 0.1802 - accuracy: 0.9487\n",
            "261/469 [===============>..............] - ETA: 6s - loss: 0.1804 - accuracy: 0.9488\n",
            "265/469 [===============>..............] - ETA: 6s - loss: 0.1803 - accuracy: 0.9489\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1803 - accuracy: 0.9489\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1811 - accuracy: 0.9488\n",
            "275/469 [================>.............] - ETA: 5s - loss: 0.1806 - accuracy: 0.9489\n",
            "279/469 [================>.............] - ETA: 5s - loss: 0.1802 - accuracy: 0.9491\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1802 - accuracy: 0.9492\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1809 - accuracy: 0.9492\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1807 - accuracy: 0.9492\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1807 - accuracy: 0.9492\n",
            "297/469 [=================>............] - ETA: 5s - loss: 0.1808 - accuracy: 0.9490\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1804 - accuracy: 0.9490\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1801 - accuracy: 0.9492\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1803 - accuracy: 0.9492\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1814 - accuracy: 0.9491\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.1815 - accuracy: 0.9490\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.1814 - accuracy: 0.9490\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 0.1814 - accuracy: 0.9490\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1816 - accuracy: 0.9490\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1809 - accuracy: 0.9492\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1811 - accuracy: 0.9491\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 0.1803 - accuracy: 0.9494\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.1806 - accuracy: 0.9493\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.1808 - accuracy: 0.9493\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 0.1811 - accuracy: 0.9495\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1810 - accuracy: 0.9496\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1807 - accuracy: 0.9497\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.1810 - accuracy: 0.9495\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 0.1814 - accuracy: 0.9493\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 0.1808 - accuracy: 0.9495\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1810 - accuracy: 0.9494\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1803 - accuracy: 0.9495\n",
            "369/469 [======================>.......] - ETA: 3s - loss: 0.1798 - accuracy: 0.9498\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 0.1792 - accuracy: 0.9499\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1786 - accuracy: 0.9501\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1786 - accuracy: 0.9500\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1785 - accuracy: 0.9500\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.1792 - accuracy: 0.9498\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.1795 - accuracy: 0.9498\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 0.1794 - accuracy: 0.9499\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 0.1792 - accuracy: 0.9499\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1787 - accuracy: 0.9499\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.1792 - accuracy: 0.9501\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.1790 - accuracy: 0.9501\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 0.1787 - accuracy: 0.9502\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.1791 - accuracy: 0.9502\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1786 - accuracy: 0.9503\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.1784 - accuracy: 0.9504\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.1782 - accuracy: 0.9503\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.1780 - accuracy: 0.9504\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 0.1778 - accuracy: 0.9504\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1777 - accuracy: 0.9504\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.1780 - accuracy: 0.9504\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.9502\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.1784 - accuracy: 0.9502\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.1794 - accuracy: 0.9501\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9501\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9500\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1791 - accuracy: 0.9501\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.1794 - accuracy: 0.9501\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9502\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1791 - accuracy: 0.9502 - val_loss: 0.1935 - val_accuracy: 0.9516\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 10/12\n",
            "  1/469 [..............................] - ETA: 13s - loss: 0.2426 - accuracy: 0.9062\n",
            "  5/469 [..............................] - ETA: 14s - loss: 0.2088 - accuracy: 0.9438\n",
            "  7/469 [..............................] - ETA: 14s - loss: 0.2064 - accuracy: 0.9408\n",
            " 11/469 [..............................] - ETA: 14s - loss: 0.1737 - accuracy: 0.9467\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1778 - accuracy: 0.9490\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 0.1731 - accuracy: 0.9494\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 0.1766 - accuracy: 0.9487\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.1742 - accuracy: 0.9481\n",
            " 29/469 [>.............................] - ETA: 13s - loss: 0.1771 - accuracy: 0.9480\n",
            " 33/469 [=>............................] - ETA: 13s - loss: 0.1840 - accuracy: 0.9470\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 0.1802 - accuracy: 0.9476\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 0.1784 - accuracy: 0.9477\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 0.1780 - accuracy: 0.9484\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1745 - accuracy: 0.9489\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.1750 - accuracy: 0.9488\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.1727 - accuracy: 0.9505\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 0.1718 - accuracy: 0.9499\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 0.1724 - accuracy: 0.9501\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.1674 - accuracy: 0.9509\n",
            " 65/469 [===>..........................] - ETA: 12s - loss: 0.1678 - accuracy: 0.9508\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.1681 - accuracy: 0.9514\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 0.1694 - accuracy: 0.9516\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 0.1688 - accuracy: 0.9518\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 0.1680 - accuracy: 0.9521\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1689 - accuracy: 0.9524\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 0.1712 - accuracy: 0.9523\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 0.1755 - accuracy: 0.9518\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 0.1779 - accuracy: 0.9514\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1812 - accuracy: 0.9505\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 0.1809 - accuracy: 0.9502\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 0.1801 - accuracy: 0.9505\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1821 - accuracy: 0.9499\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 0.1807 - accuracy: 0.9500\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 0.1812 - accuracy: 0.9497\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1808 - accuracy: 0.9498\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1791 - accuracy: 0.9501\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1808 - accuracy: 0.9499\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 0.1791 - accuracy: 0.9503\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 0.1805 - accuracy: 0.9500\n",
            "133/469 [=======>......................] - ETA: 10s - loss: 0.1799 - accuracy: 0.9503\n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1805 - accuracy: 0.9502\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 0.1788 - accuracy: 0.9506\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 0.1806 - accuracy: 0.9499\n",
            "147/469 [========>.....................] - ETA: 9s - loss: 0.1803 - accuracy: 0.9499\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 0.1805 - accuracy: 0.9499\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 0.1806 - accuracy: 0.9500\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 0.1818 - accuracy: 0.9499\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 0.1817 - accuracy: 0.9499\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 0.1813 - accuracy: 0.9498\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1814 - accuracy: 0.9498\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 0.1804 - accuracy: 0.9501\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 0.1807 - accuracy: 0.9503\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 0.1822 - accuracy: 0.9500\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 0.1826 - accuracy: 0.9498\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 0.1825 - accuracy: 0.9497\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1820 - accuracy: 0.9497\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1811 - accuracy: 0.9499\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 0.1820 - accuracy: 0.9496\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1819 - accuracy: 0.9494\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 0.1823 - accuracy: 0.9495\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 0.1827 - accuracy: 0.9493\n",
            "201/469 [===========>..................] - ETA: 8s - loss: 0.1829 - accuracy: 0.9494\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1827 - accuracy: 0.9493\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1819 - accuracy: 0.9495\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1805 - accuracy: 0.9497\n",
            "217/469 [============>.................] - ETA: 7s - loss: 0.1800 - accuracy: 0.9497\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1806 - accuracy: 0.9497\n",
            "221/469 [=============>................] - ETA: 7s - loss: 0.1811 - accuracy: 0.9496\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1806 - accuracy: 0.9497\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1804 - accuracy: 0.9499\n",
            "231/469 [=============>................] - ETA: 7s - loss: 0.1806 - accuracy: 0.9499\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 0.1797 - accuracy: 0.9499\n",
            "239/469 [==============>...............] - ETA: 6s - loss: 0.1801 - accuracy: 0.9496\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1799 - accuracy: 0.9498\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1786 - accuracy: 0.9500\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 0.1786 - accuracy: 0.9502\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 0.1780 - accuracy: 0.9504\n",
            "257/469 [===============>..............] - ETA: 6s - loss: 0.1789 - accuracy: 0.9501\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1790 - accuracy: 0.9501\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1787 - accuracy: 0.9500\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1790 - accuracy: 0.9497\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1794 - accuracy: 0.9497\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1792 - accuracy: 0.9499\n",
            "275/469 [================>.............] - ETA: 5s - loss: 0.1795 - accuracy: 0.9498\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1788 - accuracy: 0.9500\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1783 - accuracy: 0.9501\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1794 - accuracy: 0.9499\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1795 - accuracy: 0.9500\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1798 - accuracy: 0.9500\n",
            "295/469 [=================>............] - ETA: 5s - loss: 0.1798 - accuracy: 0.9500\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1799 - accuracy: 0.9500\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1813 - accuracy: 0.9497\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1814 - accuracy: 0.9496\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1830 - accuracy: 0.9494\n",
            "313/469 [===================>..........] - ETA: 4s - loss: 0.1835 - accuracy: 0.9493\n",
            "317/469 [===================>..........] - ETA: 4s - loss: 0.1830 - accuracy: 0.9493\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1837 - accuracy: 0.9491\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1836 - accuracy: 0.9491\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1845 - accuracy: 0.9490\n",
            "331/469 [====================>.........] - ETA: 4s - loss: 0.1842 - accuracy: 0.9491\n",
            "335/469 [====================>.........] - ETA: 3s - loss: 0.1839 - accuracy: 0.9493\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.1842 - accuracy: 0.9493\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1839 - accuracy: 0.9494\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.1838 - accuracy: 0.9495\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1841 - accuracy: 0.9494\n",
            "349/469 [=====================>........] - ETA: 3s - loss: 0.1838 - accuracy: 0.9494\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 0.1840 - accuracy: 0.9493\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 0.1846 - accuracy: 0.9492\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1855 - accuracy: 0.9489\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1851 - accuracy: 0.9488\n",
            "367/469 [======================>.......] - ETA: 3s - loss: 0.1848 - accuracy: 0.9490\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.1847 - accuracy: 0.9490\n",
            "371/469 [======================>.......] - ETA: 2s - loss: 0.1845 - accuracy: 0.9491\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 0.1848 - accuracy: 0.9492\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1845 - accuracy: 0.9492\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1845 - accuracy: 0.9493\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1840 - accuracy: 0.9494\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.1834 - accuracy: 0.9494\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.1834 - accuracy: 0.9495\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.1836 - accuracy: 0.9494\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.1832 - accuracy: 0.9495\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 0.1832 - accuracy: 0.9495\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1836 - accuracy: 0.9494\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.1845 - accuracy: 0.9493\n",
            "407/469 [=========================>....] - ETA: 1s - loss: 0.1846 - accuracy: 0.9493\n",
            "411/469 [=========================>....] - ETA: 1s - loss: 0.1842 - accuracy: 0.9493\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.1848 - accuracy: 0.9490\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1848 - accuracy: 0.9490\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.1854 - accuracy: 0.9489\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.1867 - accuracy: 0.9489\n",
            "429/469 [==========================>...] - ETA: 1s - loss: 0.1865 - accuracy: 0.9490\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 0.1864 - accuracy: 0.9490\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1867 - accuracy: 0.9489\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9488\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.1881 - accuracy: 0.9486\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.1880 - accuracy: 0.9486\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.1881 - accuracy: 0.9486\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1878 - accuracy: 0.9486\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9485\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9486\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.1877 - accuracy: 0.9487\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9486\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1876 - accuracy: 0.9486 - val_loss: 0.2128 - val_accuracy: 0.9499\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 11/12\n",
            "  1/469 [..............................] - ETA: 14s - loss: 0.2051 - accuracy: 0.9531\n",
            "  3/469 [..............................] - ETA: 14s - loss: 0.2436 - accuracy: 0.9531\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.1754 - accuracy: 0.9565\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.1974 - accuracy: 0.9432\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1809 - accuracy: 0.9458\n",
            " 19/469 [>.............................] - ETA: 13s - loss: 0.1705 - accuracy: 0.9486\n",
            " 21/469 [>.............................] - ETA: 13s - loss: 0.1757 - accuracy: 0.9472\n",
            " 25/469 [>.............................] - ETA: 13s - loss: 0.1765 - accuracy: 0.9472\n",
            " 29/469 [>.............................] - ETA: 12s - loss: 0.1718 - accuracy: 0.9480\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 0.1720 - accuracy: 0.9486\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 0.1770 - accuracy: 0.9491\n",
            " 39/469 [=>............................] - ETA: 12s - loss: 0.1733 - accuracy: 0.9503\n",
            " 43/469 [=>............................] - ETA: 12s - loss: 0.1753 - accuracy: 0.9515\n",
            " 47/469 [==>...........................] - ETA: 12s - loss: 0.1740 - accuracy: 0.9508\n",
            " 51/469 [==>...........................] - ETA: 12s - loss: 0.1696 - accuracy: 0.9519\n",
            " 55/469 [==>...........................] - ETA: 12s - loss: 0.1704 - accuracy: 0.9523\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 0.1669 - accuracy: 0.9531\n",
            " 61/469 [==>...........................] - ETA: 12s - loss: 0.1662 - accuracy: 0.9535\n",
            " 65/469 [===>..........................] - ETA: 11s - loss: 0.1644 - accuracy: 0.9541\n",
            " 69/469 [===>..........................] - ETA: 11s - loss: 0.1641 - accuracy: 0.9539\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 0.1658 - accuracy: 0.9539\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.1654 - accuracy: 0.9539\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 0.1627 - accuracy: 0.9546\n",
            " 79/469 [====>.........................] - ETA: 11s - loss: 0.1635 - accuracy: 0.9545\n",
            " 83/469 [====>.........................] - ETA: 11s - loss: 0.1634 - accuracy: 0.9548\n",
            " 87/469 [====>.........................] - ETA: 11s - loss: 0.1655 - accuracy: 0.9544\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 0.1657 - accuracy: 0.9543\n",
            " 93/469 [====>.........................] - ETA: 11s - loss: 0.1662 - accuracy: 0.9539\n",
            " 97/469 [=====>........................] - ETA: 11s - loss: 0.1667 - accuracy: 0.9534\n",
            "101/469 [=====>........................] - ETA: 10s - loss: 0.1679 - accuracy: 0.9531\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1728 - accuracy: 0.9527\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 0.1743 - accuracy: 0.9533\n",
            "111/469 [======>.......................] - ETA: 10s - loss: 0.1742 - accuracy: 0.9535\n",
            "115/469 [======>.......................] - ETA: 10s - loss: 0.1723 - accuracy: 0.9538\n",
            "119/469 [======>.......................] - ETA: 10s - loss: 0.1742 - accuracy: 0.9529\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1741 - accuracy: 0.9531\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 0.1742 - accuracy: 0.9529\n",
            "129/469 [=======>......................] - ETA: 10s - loss: 0.1743 - accuracy: 0.9529\n",
            "133/469 [=======>......................] - ETA: 9s - loss: 0.1751 - accuracy: 0.9526 \n",
            "137/469 [=======>......................] - ETA: 9s - loss: 0.1793 - accuracy: 0.9519\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 0.1805 - accuracy: 0.9516\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 0.1819 - accuracy: 0.9508\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 0.1816 - accuracy: 0.9511\n",
            "151/469 [========>.....................] - ETA: 9s - loss: 0.1825 - accuracy: 0.9508\n",
            "155/469 [========>.....................] - ETA: 9s - loss: 0.1839 - accuracy: 0.9503\n",
            "159/469 [=========>....................] - ETA: 9s - loss: 0.1865 - accuracy: 0.9496\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 0.1883 - accuracy: 0.9489\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 0.1884 - accuracy: 0.9489\n",
            "169/469 [=========>....................] - ETA: 8s - loss: 0.1897 - accuracy: 0.9486\n",
            "173/469 [==========>...................] - ETA: 8s - loss: 0.1900 - accuracy: 0.9483\n",
            "177/469 [==========>...................] - ETA: 8s - loss: 0.1886 - accuracy: 0.9488\n",
            "181/469 [==========>...................] - ETA: 8s - loss: 0.1878 - accuracy: 0.9489\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1876 - accuracy: 0.9490\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1879 - accuracy: 0.9492\n",
            "191/469 [===========>..................] - ETA: 8s - loss: 0.1877 - accuracy: 0.9492\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1865 - accuracy: 0.9494\n",
            "199/469 [===========>..................] - ETA: 8s - loss: 0.1857 - accuracy: 0.9497\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 0.1855 - accuracy: 0.9495\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1856 - accuracy: 0.9491\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1864 - accuracy: 0.9490\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1872 - accuracy: 0.9488\n",
            "217/469 [============>.................] - ETA: 7s - loss: 0.1872 - accuracy: 0.9487\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1868 - accuracy: 0.9488\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1871 - accuracy: 0.9489\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1870 - accuracy: 0.9487\n",
            "231/469 [=============>................] - ETA: 7s - loss: 0.1872 - accuracy: 0.9486\n",
            "235/469 [==============>...............] - ETA: 6s - loss: 0.1877 - accuracy: 0.9483\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1875 - accuracy: 0.9482\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1876 - accuracy: 0.9480\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1874 - accuracy: 0.9482\n",
            "249/469 [==============>...............] - ETA: 6s - loss: 0.1867 - accuracy: 0.9485\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 0.1867 - accuracy: 0.9486\n",
            "253/469 [===============>..............] - ETA: 6s - loss: 0.1870 - accuracy: 0.9486\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 0.1864 - accuracy: 0.9487\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1855 - accuracy: 0.9489\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1863 - accuracy: 0.9488\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1859 - accuracy: 0.9488\n",
            "271/469 [================>.............] - ETA: 5s - loss: 0.1861 - accuracy: 0.9488\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1861 - accuracy: 0.9487\n",
            "275/469 [================>.............] - ETA: 5s - loss: 0.1857 - accuracy: 0.9488\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1856 - accuracy: 0.9487\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1848 - accuracy: 0.9488\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1852 - accuracy: 0.9488\n",
            "289/469 [=================>............] - ETA: 5s - loss: 0.1847 - accuracy: 0.9490\n",
            "293/469 [=================>............] - ETA: 5s - loss: 0.1851 - accuracy: 0.9488\n",
            "297/469 [=================>............] - ETA: 5s - loss: 0.1846 - accuracy: 0.9489\n",
            "301/469 [==================>...........] - ETA: 4s - loss: 0.1840 - accuracy: 0.9489\n",
            "303/469 [==================>...........] - ETA: 4s - loss: 0.1838 - accuracy: 0.9489\n",
            "307/469 [==================>...........] - ETA: 4s - loss: 0.1828 - accuracy: 0.9493\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1833 - accuracy: 0.9492\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.1843 - accuracy: 0.9492\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 0.1842 - accuracy: 0.9492\n",
            "321/469 [===================>..........] - ETA: 4s - loss: 0.1844 - accuracy: 0.9492\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1852 - accuracy: 0.9491\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1851 - accuracy: 0.9490\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 0.1853 - accuracy: 0.9489\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.1840 - accuracy: 0.9491\n",
            "339/469 [====================>.........] - ETA: 3s - loss: 0.1852 - accuracy: 0.9491\n",
            "343/469 [====================>.........] - ETA: 3s - loss: 0.1853 - accuracy: 0.9490\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1859 - accuracy: 0.9488\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.1866 - accuracy: 0.9487\n",
            "353/469 [=====================>........] - ETA: 3s - loss: 0.1872 - accuracy: 0.9487\n",
            "357/469 [=====================>........] - ETA: 3s - loss: 0.1890 - accuracy: 0.9486\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1900 - accuracy: 0.9487\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1901 - accuracy: 0.9488\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.1907 - accuracy: 0.9485\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 0.1914 - accuracy: 0.9482\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1919 - accuracy: 0.9481\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1918 - accuracy: 0.9480\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1922 - accuracy: 0.9479\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.1918 - accuracy: 0.9480\n",
            "389/469 [=======================>......] - ETA: 2s - loss: 0.1922 - accuracy: 0.9478\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.1923 - accuracy: 0.9478\n",
            "393/469 [========================>.....] - ETA: 2s - loss: 0.1922 - accuracy: 0.9478\n",
            "397/469 [========================>.....] - ETA: 2s - loss: 0.1924 - accuracy: 0.9479\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1918 - accuracy: 0.9481\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.1927 - accuracy: 0.9480\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.1929 - accuracy: 0.9479\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 0.1923 - accuracy: 0.9481\n",
            "415/469 [=========================>....] - ETA: 1s - loss: 0.1921 - accuracy: 0.9480\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 0.1921 - accuracy: 0.9480\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1920 - accuracy: 0.9480\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.1917 - accuracy: 0.9480\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.1910 - accuracy: 0.9483\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 0.1921 - accuracy: 0.9482\n",
            "433/469 [==========================>...] - ETA: 1s - loss: 0.1921 - accuracy: 0.9481\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1918 - accuracy: 0.9481\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.1920 - accuracy: 0.9481\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.1916 - accuracy: 0.9482\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.1913 - accuracy: 0.9482\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.1915 - accuracy: 0.9481\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1912 - accuracy: 0.9482\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1915 - accuracy: 0.9483\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9483\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.1936 - accuracy: 0.9483\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9480\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1949 - accuracy: 0.9480 - val_loss: 0.1688 - val_accuracy: 0.9501\n",
            "\u001b[36m(train_mnist pid=3300066)\u001b[0m Epoch 12/12\n",
            "  1/469 [..............................] - ETA: 14s - loss: 0.2387 - accuracy: 0.9062\n",
            "  3/469 [..............................] - ETA: 12s - loss: 0.1929 - accuracy: 0.9349\n",
            "  5/469 [..............................] - ETA: 13s - loss: 0.1604 - accuracy: 0.9453\n",
            "  7/469 [..............................] - ETA: 13s - loss: 0.1657 - accuracy: 0.9453\n",
            " 11/469 [..............................] - ETA: 13s - loss: 0.1812 - accuracy: 0.9361\n",
            " 15/469 [..............................] - ETA: 13s - loss: 0.1922 - accuracy: 0.9385\n",
            " 19/469 [>.............................] - ETA: 12s - loss: 0.1885 - accuracy: 0.9408\n",
            " 23/469 [>.............................] - ETA: 12s - loss: 0.1833 - accuracy: 0.9426\n",
            " 27/469 [>.............................] - ETA: 12s - loss: 0.1824 - accuracy: 0.9450\n",
            " 31/469 [>.............................] - ETA: 12s - loss: 0.1767 - accuracy: 0.9476\n",
            " 33/469 [=>............................] - ETA: 12s - loss: 0.1711 - accuracy: 0.9491\n",
            " 37/469 [=>............................] - ETA: 12s - loss: 0.1718 - accuracy: 0.9500\n",
            " 41/469 [=>............................] - ETA: 12s - loss: 0.1680 - accuracy: 0.9503\n",
            " 45/469 [=>............................] - ETA: 12s - loss: 0.1705 - accuracy: 0.9507\n",
            " 49/469 [==>...........................] - ETA: 12s - loss: 0.1726 - accuracy: 0.9503\n",
            " 53/469 [==>...........................] - ETA: 12s - loss: 0.1709 - accuracy: 0.9502\n",
            " 57/469 [==>...........................] - ETA: 12s - loss: 0.1706 - accuracy: 0.9501\n",
            " 59/469 [==>...........................] - ETA: 12s - loss: 0.1694 - accuracy: 0.9505\n",
            " 63/469 [===>..........................] - ETA: 11s - loss: 0.1700 - accuracy: 0.9513\n",
            " 67/469 [===>..........................] - ETA: 11s - loss: 0.1688 - accuracy: 0.9513\n",
            " 71/469 [===>..........................] - ETA: 11s - loss: 0.1711 - accuracy: 0.9510\n",
            " 73/469 [===>..........................] - ETA: 11s - loss: 0.1714 - accuracy: 0.9510\n",
            " 75/469 [===>..........................] - ETA: 11s - loss: 0.1719 - accuracy: 0.9516\n",
            " 77/469 [===>..........................] - ETA: 11s - loss: 0.1745 - accuracy: 0.9506\n",
            " 81/469 [====>.........................] - ETA: 11s - loss: 0.1808 - accuracy: 0.9496\n",
            " 85/469 [====>.........................] - ETA: 11s - loss: 0.1840 - accuracy: 0.9493\n",
            " 89/469 [====>.........................] - ETA: 11s - loss: 0.1828 - accuracy: 0.9494\n",
            " 91/469 [====>.........................] - ETA: 11s - loss: 0.1842 - accuracy: 0.9493\n",
            " 95/469 [=====>........................] - ETA: 11s - loss: 0.1856 - accuracy: 0.9494\n",
            " 99/469 [=====>........................] - ETA: 11s - loss: 0.1853 - accuracy: 0.9493\n",
            "103/469 [=====>........................] - ETA: 10s - loss: 0.1845 - accuracy: 0.9496\n",
            "105/469 [=====>........................] - ETA: 10s - loss: 0.1860 - accuracy: 0.9493\n",
            "109/469 [=====>........................] - ETA: 10s - loss: 0.1874 - accuracy: 0.9492\n",
            "113/469 [======>.......................] - ETA: 10s - loss: 0.1876 - accuracy: 0.9491\n",
            "117/469 [======>.......................] - ETA: 10s - loss: 0.1870 - accuracy: 0.9489\n",
            "121/469 [======>.......................] - ETA: 10s - loss: 0.1903 - accuracy: 0.9486\n",
            "123/469 [======>.......................] - ETA: 10s - loss: 0.1915 - accuracy: 0.9487\n",
            "127/469 [=======>......................] - ETA: 10s - loss: 0.1897 - accuracy: 0.9489\n",
            "131/469 [=======>......................] - ETA: 10s - loss: 0.1898 - accuracy: 0.9488\n",
            "135/469 [=======>......................] - ETA: 9s - loss: 0.1894 - accuracy: 0.9489\n",
            "139/469 [=======>......................] - ETA: 9s - loss: 0.1894 - accuracy: 0.9489\n",
            "141/469 [========>.....................] - ETA: 9s - loss: 0.1893 - accuracy: 0.9487\n",
            "143/469 [========>.....................] - ETA: 9s - loss: 0.1887 - accuracy: 0.9489\n",
            "145/469 [========>.....................] - ETA: 9s - loss: 0.1877 - accuracy: 0.9491\n",
            "149/469 [========>.....................] - ETA: 9s - loss: 0.1864 - accuracy: 0.9497\n",
            "153/469 [========>.....................] - ETA: 9s - loss: 0.1870 - accuracy: 0.9496\n",
            "157/469 [=========>....................] - ETA: 9s - loss: 0.1870 - accuracy: 0.9498\n",
            "161/469 [=========>....................] - ETA: 9s - loss: 0.1881 - accuracy: 0.9496\n",
            "163/469 [=========>....................] - ETA: 9s - loss: 0.1879 - accuracy: 0.9498\n",
            "165/469 [=========>....................] - ETA: 9s - loss: 0.1885 - accuracy: 0.9497\n",
            "167/469 [=========>....................] - ETA: 8s - loss: 0.1884 - accuracy: 0.9498\n",
            "171/469 [=========>....................] - ETA: 8s - loss: 0.1878 - accuracy: 0.9499\n",
            "175/469 [==========>...................] - ETA: 8s - loss: 0.1892 - accuracy: 0.9497\n",
            "179/469 [==========>...................] - ETA: 8s - loss: 0.1893 - accuracy: 0.9497\n",
            "183/469 [==========>...................] - ETA: 8s - loss: 0.1909 - accuracy: 0.9493\n",
            "187/469 [==========>...................] - ETA: 8s - loss: 0.1913 - accuracy: 0.9493\n",
            "189/469 [===========>..................] - ETA: 8s - loss: 0.1913 - accuracy: 0.9493\n",
            "193/469 [===========>..................] - ETA: 8s - loss: 0.1913 - accuracy: 0.9491\n",
            "195/469 [===========>..................] - ETA: 8s - loss: 0.1918 - accuracy: 0.9490\n",
            "197/469 [===========>..................] - ETA: 8s - loss: 0.1922 - accuracy: 0.9488\n",
            "201/469 [===========>..................] - ETA: 7s - loss: 0.1926 - accuracy: 0.9485\n",
            "205/469 [============>.................] - ETA: 7s - loss: 0.1917 - accuracy: 0.9488\n",
            "209/469 [============>.................] - ETA: 7s - loss: 0.1920 - accuracy: 0.9486\n",
            "213/469 [============>.................] - ETA: 7s - loss: 0.1916 - accuracy: 0.9485\n",
            "217/469 [============>.................] - ETA: 7s - loss: 0.1902 - accuracy: 0.9489\n",
            "219/469 [=============>................] - ETA: 7s - loss: 0.1898 - accuracy: 0.9488\n",
            "223/469 [=============>................] - ETA: 7s - loss: 0.1896 - accuracy: 0.9488\n",
            "227/469 [=============>................] - ETA: 7s - loss: 0.1892 - accuracy: 0.9489\n",
            "229/469 [=============>................] - ETA: 7s - loss: 0.1888 - accuracy: 0.9491\n",
            "233/469 [=============>................] - ETA: 7s - loss: 0.1880 - accuracy: 0.9492\n",
            "237/469 [==============>...............] - ETA: 6s - loss: 0.1873 - accuracy: 0.9493\n",
            "241/469 [==============>...............] - ETA: 6s - loss: 0.1869 - accuracy: 0.9495\n",
            "243/469 [==============>...............] - ETA: 6s - loss: 0.1870 - accuracy: 0.9494\n",
            "245/469 [==============>...............] - ETA: 6s - loss: 0.1873 - accuracy: 0.9494\n",
            "247/469 [==============>...............] - ETA: 6s - loss: 0.1864 - accuracy: 0.9495\n",
            "251/469 [===============>..............] - ETA: 6s - loss: 0.1865 - accuracy: 0.9494\n",
            "255/469 [===============>..............] - ETA: 6s - loss: 0.1868 - accuracy: 0.9493\n",
            "259/469 [===============>..............] - ETA: 6s - loss: 0.1871 - accuracy: 0.9494\n",
            "263/469 [===============>..............] - ETA: 6s - loss: 0.1868 - accuracy: 0.9494\n",
            "267/469 [================>.............] - ETA: 6s - loss: 0.1865 - accuracy: 0.9495\n",
            "269/469 [================>.............] - ETA: 5s - loss: 0.1860 - accuracy: 0.9497\n",
            "273/469 [================>.............] - ETA: 5s - loss: 0.1880 - accuracy: 0.9492\n",
            "277/469 [================>.............] - ETA: 5s - loss: 0.1882 - accuracy: 0.9491\n",
            "281/469 [================>.............] - ETA: 5s - loss: 0.1900 - accuracy: 0.9488\n",
            "285/469 [=================>............] - ETA: 5s - loss: 0.1907 - accuracy: 0.9488\n",
            "287/469 [=================>............] - ETA: 5s - loss: 0.1917 - accuracy: 0.9486\n",
            "291/469 [=================>............] - ETA: 5s - loss: 0.1932 - accuracy: 0.9483\n",
            "295/469 [=================>............] - ETA: 5s - loss: 0.1936 - accuracy: 0.9481\n",
            "299/469 [==================>...........] - ETA: 5s - loss: 0.1940 - accuracy: 0.9479\n",
            "301/469 [==================>...........] - ETA: 5s - loss: 0.1940 - accuracy: 0.9479\n",
            "305/469 [==================>...........] - ETA: 4s - loss: 0.1934 - accuracy: 0.9479\n",
            "309/469 [==================>...........] - ETA: 4s - loss: 0.1930 - accuracy: 0.9480\n",
            "311/469 [==================>...........] - ETA: 4s - loss: 0.1936 - accuracy: 0.9480\n",
            "315/469 [===================>..........] - ETA: 4s - loss: 0.1938 - accuracy: 0.9479\n",
            "319/469 [===================>..........] - ETA: 4s - loss: 0.1945 - accuracy: 0.9477\n",
            "323/469 [===================>..........] - ETA: 4s - loss: 0.1942 - accuracy: 0.9479\n",
            "325/469 [===================>..........] - ETA: 4s - loss: 0.1939 - accuracy: 0.9480\n",
            "327/469 [===================>..........] - ETA: 4s - loss: 0.1939 - accuracy: 0.9479\n",
            "329/469 [====================>.........] - ETA: 4s - loss: 0.1943 - accuracy: 0.9479\n",
            "333/469 [====================>.........] - ETA: 4s - loss: 0.1942 - accuracy: 0.9478\n",
            "337/469 [====================>.........] - ETA: 3s - loss: 0.1949 - accuracy: 0.9477\n",
            "341/469 [====================>.........] - ETA: 3s - loss: 0.1950 - accuracy: 0.9477\n",
            "345/469 [=====================>........] - ETA: 3s - loss: 0.1950 - accuracy: 0.9478\n",
            "347/469 [=====================>........] - ETA: 3s - loss: 0.1947 - accuracy: 0.9478\n",
            "351/469 [=====================>........] - ETA: 3s - loss: 0.1946 - accuracy: 0.9476\n",
            "355/469 [=====================>........] - ETA: 3s - loss: 0.1949 - accuracy: 0.9477\n",
            "359/469 [=====================>........] - ETA: 3s - loss: 0.1958 - accuracy: 0.9476\n",
            "361/469 [======================>.......] - ETA: 3s - loss: 0.1959 - accuracy: 0.9474\n",
            "365/469 [======================>.......] - ETA: 3s - loss: 0.1964 - accuracy: 0.9473\n",
            "369/469 [======================>.......] - ETA: 2s - loss: 0.1966 - accuracy: 0.9473\n",
            "373/469 [======================>.......] - ETA: 2s - loss: 0.1970 - accuracy: 0.9472\n",
            "375/469 [======================>.......] - ETA: 2s - loss: 0.1972 - accuracy: 0.9471\n",
            "379/469 [=======================>......] - ETA: 2s - loss: 0.1976 - accuracy: 0.9469\n",
            "383/469 [=======================>......] - ETA: 2s - loss: 0.1974 - accuracy: 0.9470\n",
            "387/469 [=======================>......] - ETA: 2s - loss: 0.1975 - accuracy: 0.9469\n",
            "391/469 [========================>.....] - ETA: 2s - loss: 0.1976 - accuracy: 0.9470\n",
            "395/469 [========================>.....] - ETA: 2s - loss: 0.1974 - accuracy: 0.9470\n",
            "399/469 [========================>.....] - ETA: 2s - loss: 0.1971 - accuracy: 0.9471\n",
            "401/469 [========================>.....] - ETA: 2s - loss: 0.1969 - accuracy: 0.9471\n",
            "405/469 [========================>.....] - ETA: 1s - loss: 0.1965 - accuracy: 0.9471\n",
            "409/469 [=========================>....] - ETA: 1s - loss: 0.1971 - accuracy: 0.9471\n",
            "413/469 [=========================>....] - ETA: 1s - loss: 0.1970 - accuracy: 0.9472\n",
            "417/469 [=========================>....] - ETA: 1s - loss: 0.1965 - accuracy: 0.9473\n",
            "419/469 [=========================>....] - ETA: 1s - loss: 0.1962 - accuracy: 0.9474\n",
            "423/469 [==========================>...] - ETA: 1s - loss: 0.1957 - accuracy: 0.9475\n",
            "427/469 [==========================>...] - ETA: 1s - loss: 0.1955 - accuracy: 0.9477\n",
            "431/469 [==========================>...] - ETA: 1s - loss: 0.1957 - accuracy: 0.9475\n",
            "435/469 [==========================>...] - ETA: 1s - loss: 0.1958 - accuracy: 0.9475\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.1965 - accuracy: 0.9474\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.1965 - accuracy: 0.9473\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.1964 - accuracy: 0.9474\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.1962 - accuracy: 0.9475\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.1958 - accuracy: 0.9474\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.1954 - accuracy: 0.9475\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1953 - accuracy: 0.9475\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1947 - accuracy: 0.9476\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9477\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9477\n",
            "469/469 [==============================] - 15s 31ms/step - loss: 0.1939 - accuracy: 0.9477 - val_loss: 0.1891 - val_accuracy: 0.9597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3300944)\u001b[0m 2023-12-05 01:54:27.057413: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3300944)\u001b[0m 2023-12-05 01:54:27.106056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3300944)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3300944)\u001b[0m 2023-12-05 01:54:27.105539: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3300944)\u001b[0m 2023-12-05 01:54:28.005124: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3300944)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3300944)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3300944)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3300944)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3300944)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3300944)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3300944)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3300944)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m 2023-12-05 01:54:29.655621: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 1/12\n",
            "  7/469 [..............................] - ETA: 4s - loss: 3.0643 - accuracy: 0.2690  \n",
            " 19/469 [>.............................] - ETA: 3s - loss: 1.8464 - accuracy: 0.5041\n",
            " 25/469 [>.............................] - ETA: 3s - loss: 1.5434 - accuracy: 0.5806\n",
            " 37/469 [=>............................] - ETA: 3s - loss: 1.2191 - accuracy: 0.6603\n",
            " 49/469 [==>...........................] - ETA: 3s - loss: 1.0301 - accuracy: 0.7124\n",
            " 61/469 [==>...........................] - ETA: 3s - loss: 0.9072 - accuracy: 0.7459\n",
            " 73/469 [===>..........................] - ETA: 3s - loss: 0.8172 - accuracy: 0.7708\n",
            " 85/469 [====>.........................] - ETA: 3s - loss: 0.7476 - accuracy: 0.7888\n",
            " 97/469 [=====>........................] - ETA: 3s - loss: 0.6976 - accuracy: 0.8024\n",
            "109/469 [=====>........................] - ETA: 3s - loss: 0.6553 - accuracy: 0.8126\n",
            "121/469 [======>.......................] - ETA: 2s - loss: 0.6158 - accuracy: 0.8232\n",
            "135/469 [=======>......................] - ETA: 2s - loss: 0.5851 - accuracy: 0.8323\n",
            "147/469 [========>.....................] - ETA: 2s - loss: 0.5589 - accuracy: 0.8396\n",
            "159/469 [=========>....................] - ETA: 2s - loss: 0.5383 - accuracy: 0.8454\n",
            "171/469 [=========>....................] - ETA: 2s - loss: 0.5212 - accuracy: 0.8503\n",
            "183/469 [==========>...................] - ETA: 2s - loss: 0.5043 - accuracy: 0.8548\n",
            "195/469 [===========>..................] - ETA: 2s - loss: 0.4881 - accuracy: 0.8590\n",
            "201/469 [===========>..................] - ETA: 2s - loss: 0.4805 - accuracy: 0.8608\n",
            "213/469 [============>.................] - ETA: 2s - loss: 0.4681 - accuracy: 0.8640\n",
            "225/469 [=============>................] - ETA: 2s - loss: 0.4583 - accuracy: 0.8662\n",
            "238/469 [==============>...............] - ETA: 1s - loss: 0.4475 - accuracy: 0.8697\n",
            "250/469 [==============>...............] - ETA: 1s - loss: 0.4384 - accuracy: 0.8719\n",
            "263/469 [===============>..............] - ETA: 1s - loss: 0.4279 - accuracy: 0.8751\n",
            "275/469 [================>.............] - ETA: 1s - loss: 0.4186 - accuracy: 0.8779\n",
            "289/469 [=================>............] - ETA: 1s - loss: 0.4095 - accuracy: 0.8804\n",
            "301/469 [==================>...........] - ETA: 1s - loss: 0.4036 - accuracy: 0.8824\n",
            "314/469 [===================>..........] - ETA: 1s - loss: 0.3970 - accuracy: 0.8842\n",
            "327/469 [===================>..........] - ETA: 1s - loss: 0.3917 - accuracy: 0.8858\n",
            "333/469 [====================>.........] - ETA: 1s - loss: 0.3893 - accuracy: 0.8864\n",
            "340/469 [====================>.........] - ETA: 1s - loss: 0.3869 - accuracy: 0.8869\n",
            "352/469 [=====================>........] - ETA: 0s - loss: 0.3815 - accuracy: 0.8884\n",
            "365/469 [======================>.......] - ETA: 0s - loss: 0.3766 - accuracy: 0.8900\n",
            "377/469 [=======================>......] - ETA: 0s - loss: 0.3714 - accuracy: 0.8916\n",
            "389/469 [=======================>......] - ETA: 0s - loss: 0.3674 - accuracy: 0.8927\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.3632 - accuracy: 0.8939\n",
            "413/469 [=========================>....] - ETA: 0s - loss: 0.3584 - accuracy: 0.8953\n",
            "425/469 [==========================>...] - ETA: 0s - loss: 0.3546 - accuracy: 0.8963\n",
            "438/469 [===========================>..] - ETA: 0s - loss: 0.3496 - accuracy: 0.8976\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.3467 - accuracy: 0.8983\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.3438 - accuracy: 0.8993\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.8999\n",
            "469/469 [==============================] - 5s 9ms/step - loss: 0.3417 - accuracy: 0.8999 - val_loss: 0.0919 - val_accuracy: 0.9723\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 2/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.2358 - accuracy: 0.9219\n",
            " 14/469 [..............................] - ETA: 3s - loss: 0.2045 - accuracy: 0.9336\n",
            " 26/469 [>.............................] - ETA: 3s - loss: 0.2194 - accuracy: 0.9306\n",
            " 38/469 [=>............................] - ETA: 3s - loss: 0.2122 - accuracy: 0.9313\n",
            " 50/469 [==>...........................] - ETA: 3s - loss: 0.2152 - accuracy: 0.9311\n",
            " 62/469 [==>...........................] - ETA: 3s - loss: 0.2147 - accuracy: 0.9315\n",
            " 74/469 [===>..........................] - ETA: 3s - loss: 0.2149 - accuracy: 0.9315\n",
            " 86/469 [====>.........................] - ETA: 3s - loss: 0.2115 - accuracy: 0.9326\n",
            " 98/469 [=====>........................] - ETA: 3s - loss: 0.2100 - accuracy: 0.9342\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.2057 - accuracy: 0.9360\n",
            "128/469 [=======>......................] - ETA: 2s - loss: 0.2064 - accuracy: 0.9364\n",
            "140/469 [=======>......................] - ETA: 2s - loss: 0.2068 - accuracy: 0.9366\n",
            "152/469 [========>.....................] - ETA: 2s - loss: 0.2067 - accuracy: 0.9363\n",
            "164/469 [=========>....................] - ETA: 2s - loss: 0.2072 - accuracy: 0.9359\n",
            "176/469 [==========>...................] - ETA: 2s - loss: 0.2083 - accuracy: 0.9358\n",
            "188/469 [===========>..................] - ETA: 2s - loss: 0.2097 - accuracy: 0.9360\n",
            "200/469 [===========>..................] - ETA: 2s - loss: 0.2106 - accuracy: 0.9367\n",
            "213/469 [============>.................] - ETA: 2s - loss: 0.2086 - accuracy: 0.9373\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.2096 - accuracy: 0.9373\n",
            "238/469 [==============>...............] - ETA: 1s - loss: 0.2097 - accuracy: 0.9373\n",
            "251/469 [===============>..............] - ETA: 1s - loss: 0.2085 - accuracy: 0.9377\n",
            "264/469 [===============>..............] - ETA: 1s - loss: 0.2089 - accuracy: 0.9377\n",
            "276/469 [================>.............] - ETA: 1s - loss: 0.2090 - accuracy: 0.9374\n",
            "288/469 [=================>............] - ETA: 1s - loss: 0.2077 - accuracy: 0.9377\n",
            "300/469 [==================>...........] - ETA: 1s - loss: 0.2095 - accuracy: 0.9375\n",
            "312/469 [==================>...........] - ETA: 1s - loss: 0.2087 - accuracy: 0.9378\n",
            "324/469 [===================>..........] - ETA: 1s - loss: 0.2079 - accuracy: 0.9379\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.2078 - accuracy: 0.9379\n",
            "342/469 [====================>.........] - ETA: 1s - loss: 0.2083 - accuracy: 0.9379\n",
            "354/469 [=====================>........] - ETA: 0s - loss: 0.2097 - accuracy: 0.9377\n",
            "366/469 [======================>.......] - ETA: 0s - loss: 0.2091 - accuracy: 0.9376\n",
            "378/469 [=======================>......] - ETA: 0s - loss: 0.2087 - accuracy: 0.9379\n",
            "390/469 [=======================>......] - ETA: 0s - loss: 0.2072 - accuracy: 0.9384\n",
            "402/469 [========================>.....] - ETA: 0s - loss: 0.2067 - accuracy: 0.9385\n",
            "415/469 [=========================>....] - ETA: 0s - loss: 0.2066 - accuracy: 0.9384\n",
            "428/469 [==========================>...] - ETA: 0s - loss: 0.2058 - accuracy: 0.9383\n",
            "440/469 [===========================>..] - ETA: 0s - loss: 0.2059 - accuracy: 0.9383\n",
            "452/469 [===========================>..] - ETA: 0s - loss: 0.2052 - accuracy: 0.9385\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9386\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.2047 - accuracy: 0.9385 - val_loss: 0.0828 - val_accuracy: 0.9741\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 3/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.1476 - accuracy: 0.9453\n",
            " 14/469 [..............................] - ETA: 3s - loss: 0.1779 - accuracy: 0.9464\n",
            " 26/469 [>.............................] - ETA: 3s - loss: 0.1799 - accuracy: 0.9474\n",
            " 39/469 [=>............................] - ETA: 3s - loss: 0.1826 - accuracy: 0.9467\n",
            " 52/469 [==>...........................] - ETA: 3s - loss: 0.1753 - accuracy: 0.9489\n",
            " 65/469 [===>..........................] - ETA: 3s - loss: 0.1757 - accuracy: 0.9487\n",
            " 78/469 [===>..........................] - ETA: 3s - loss: 0.1738 - accuracy: 0.9491\n",
            " 90/469 [====>.........................] - ETA: 3s - loss: 0.1737 - accuracy: 0.9491\n",
            "103/469 [=====>........................] - ETA: 3s - loss: 0.1718 - accuracy: 0.9495\n",
            "115/469 [======>.......................] - ETA: 2s - loss: 0.1740 - accuracy: 0.9485\n",
            "127/469 [=======>......................] - ETA: 2s - loss: 0.1732 - accuracy: 0.9478\n",
            "139/469 [=======>......................] - ETA: 2s - loss: 0.1753 - accuracy: 0.9478\n",
            "151/469 [========>.....................] - ETA: 2s - loss: 0.1750 - accuracy: 0.9475\n",
            "164/469 [=========>....................] - ETA: 2s - loss: 0.1752 - accuracy: 0.9475\n",
            "177/469 [==========>...................] - ETA: 2s - loss: 0.1793 - accuracy: 0.9469\n",
            "190/469 [===========>..................] - ETA: 2s - loss: 0.1823 - accuracy: 0.9458\n",
            "203/469 [===========>..................] - ETA: 2s - loss: 0.1844 - accuracy: 0.9458\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.1860 - accuracy: 0.9456\n",
            "228/469 [=============>................] - ETA: 2s - loss: 0.1874 - accuracy: 0.9453\n",
            "240/469 [==============>...............] - ETA: 1s - loss: 0.1863 - accuracy: 0.9458\n",
            "252/469 [===============>..............] - ETA: 1s - loss: 0.1842 - accuracy: 0.9465\n",
            "258/469 [===============>..............] - ETA: 1s - loss: 0.1837 - accuracy: 0.9464\n",
            "265/469 [===============>..............] - ETA: 1s - loss: 0.1834 - accuracy: 0.9462\n",
            "271/469 [================>.............] - ETA: 1s - loss: 0.1840 - accuracy: 0.9460\n",
            "283/469 [=================>............] - ETA: 1s - loss: 0.1850 - accuracy: 0.9458\n",
            "295/469 [=================>............] - ETA: 1s - loss: 0.1858 - accuracy: 0.9458\n",
            "308/469 [==================>...........] - ETA: 1s - loss: 0.1851 - accuracy: 0.9460\n",
            "320/469 [===================>..........] - ETA: 1s - loss: 0.1854 - accuracy: 0.9460\n",
            "332/469 [====================>.........] - ETA: 1s - loss: 0.1863 - accuracy: 0.9458\n",
            "344/469 [=====================>........] - ETA: 1s - loss: 0.1861 - accuracy: 0.9458\n",
            "357/469 [=====================>........] - ETA: 0s - loss: 0.1858 - accuracy: 0.9461\n",
            "370/469 [======================>.......] - ETA: 0s - loss: 0.1857 - accuracy: 0.9459\n",
            "382/469 [=======================>......] - ETA: 0s - loss: 0.1857 - accuracy: 0.9460\n",
            "395/469 [========================>.....] - ETA: 0s - loss: 0.1852 - accuracy: 0.9463\n",
            "409/469 [=========================>....] - ETA: 0s - loss: 0.1838 - accuracy: 0.9464\n",
            "422/469 [=========================>....] - ETA: 0s - loss: 0.1841 - accuracy: 0.9464\n",
            "434/469 [==========================>...] - ETA: 0s - loss: 0.1838 - accuracy: 0.9464\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.1834 - accuracy: 0.9465\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.1843 - accuracy: 0.9463\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.1844 - accuracy: 0.9462\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1843 - accuracy: 0.9462 - val_loss: 0.0793 - val_accuracy: 0.9764\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 4/12\n",
            "  7/469 [..............................] - ETA: 3s - loss: 0.1572 - accuracy: 0.9509\n",
            " 19/469 [>.............................] - ETA: 3s - loss: 0.1634 - accuracy: 0.9527\n",
            " 31/469 [>.............................] - ETA: 3s - loss: 0.1688 - accuracy: 0.9521\n",
            " 43/469 [=>............................] - ETA: 3s - loss: 0.1634 - accuracy: 0.9524\n",
            " 55/469 [==>...........................] - ETA: 3s - loss: 0.1653 - accuracy: 0.9514\n",
            " 67/469 [===>..........................] - ETA: 3s - loss: 0.1680 - accuracy: 0.9501\n",
            " 79/469 [====>.........................] - ETA: 3s - loss: 0.1704 - accuracy: 0.9503\n",
            " 91/469 [====>.........................] - ETA: 3s - loss: 0.1660 - accuracy: 0.9511\n",
            "103/469 [=====>........................] - ETA: 3s - loss: 0.1660 - accuracy: 0.9502\n",
            "109/469 [=====>........................] - ETA: 3s - loss: 0.1663 - accuracy: 0.9508\n",
            "115/469 [======>.......................] - ETA: 3s - loss: 0.1648 - accuracy: 0.9512\n",
            "122/469 [======>.......................] - ETA: 2s - loss: 0.1657 - accuracy: 0.9510\n",
            "129/469 [=======>......................] - ETA: 2s - loss: 0.1683 - accuracy: 0.9503\n",
            "135/469 [=======>......................] - ETA: 2s - loss: 0.1689 - accuracy: 0.9503\n",
            "142/469 [========>.....................] - ETA: 2s - loss: 0.1708 - accuracy: 0.9499\n",
            "148/469 [========>.....................] - ETA: 2s - loss: 0.1706 - accuracy: 0.9500\n",
            "160/469 [=========>....................] - ETA: 2s - loss: 0.1715 - accuracy: 0.9500\n",
            "172/469 [==========>...................] - ETA: 2s - loss: 0.1711 - accuracy: 0.9500\n",
            "185/469 [==========>...................] - ETA: 2s - loss: 0.1701 - accuracy: 0.9499\n",
            "198/469 [===========>..................] - ETA: 2s - loss: 0.1706 - accuracy: 0.9499\n",
            "211/469 [============>.................] - ETA: 2s - loss: 0.1718 - accuracy: 0.9496\n",
            "223/469 [=============>................] - ETA: 2s - loss: 0.1713 - accuracy: 0.9499\n",
            "237/469 [==============>...............] - ETA: 1s - loss: 0.1693 - accuracy: 0.9502\n",
            "250/469 [==============>...............] - ETA: 1s - loss: 0.1697 - accuracy: 0.9502\n",
            "257/469 [===============>..............] - ETA: 1s - loss: 0.1687 - accuracy: 0.9504\n",
            "269/469 [================>.............] - ETA: 1s - loss: 0.1684 - accuracy: 0.9505\n",
            "283/469 [=================>............] - ETA: 1s - loss: 0.1688 - accuracy: 0.9503\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.1689 - accuracy: 0.9501\n",
            "309/469 [==================>...........] - ETA: 1s - loss: 0.1693 - accuracy: 0.9501\n",
            "322/469 [===================>..........] - ETA: 1s - loss: 0.1695 - accuracy: 0.9499\n",
            "334/469 [====================>.........] - ETA: 1s - loss: 0.1702 - accuracy: 0.9495\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.1701 - accuracy: 0.9494\n",
            "360/469 [======================>.......] - ETA: 0s - loss: 0.1697 - accuracy: 0.9497\n",
            "373/469 [======================>.......] - ETA: 0s - loss: 0.1694 - accuracy: 0.9500\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.1698 - accuracy: 0.9497\n",
            "398/469 [========================>.....] - ETA: 0s - loss: 0.1693 - accuracy: 0.9497\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.1710 - accuracy: 0.9494\n",
            "423/469 [==========================>...] - ETA: 0s - loss: 0.1717 - accuracy: 0.9492\n",
            "436/469 [==========================>...] - ETA: 0s - loss: 0.1722 - accuracy: 0.9489\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.1718 - accuracy: 0.9489\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.1720 - accuracy: 0.9491\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9493\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1711 - accuracy: 0.9493 - val_loss: 0.0743 - val_accuracy: 0.9779\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 5/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.1767 - accuracy: 0.9297\n",
            " 13/469 [..............................] - ETA: 3s - loss: 0.1534 - accuracy: 0.9513\n",
            " 26/469 [>.............................] - ETA: 3s - loss: 0.1553 - accuracy: 0.9534\n",
            " 38/469 [=>............................] - ETA: 3s - loss: 0.1604 - accuracy: 0.9509\n",
            " 51/469 [==>...........................] - ETA: 3s - loss: 0.1722 - accuracy: 0.9484\n",
            " 63/469 [===>..........................] - ETA: 3s - loss: 0.1763 - accuracy: 0.9485\n",
            " 76/469 [===>..........................] - ETA: 3s - loss: 0.1762 - accuracy: 0.9477\n",
            " 89/469 [====>.........................] - ETA: 3s - loss: 0.1759 - accuracy: 0.9477\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.1800 - accuracy: 0.9473\n",
            "114/469 [======>.......................] - ETA: 2s - loss: 0.1812 - accuracy: 0.9465\n",
            "127/469 [=======>......................] - ETA: 2s - loss: 0.1815 - accuracy: 0.9466\n",
            "139/469 [=======>......................] - ETA: 2s - loss: 0.1822 - accuracy: 0.9462\n",
            "152/469 [========>.....................] - ETA: 2s - loss: 0.1803 - accuracy: 0.9469\n",
            "164/469 [=========>....................] - ETA: 2s - loss: 0.1765 - accuracy: 0.9479\n",
            "177/469 [==========>...................] - ETA: 2s - loss: 0.1742 - accuracy: 0.9484\n",
            "190/469 [===========>..................] - ETA: 2s - loss: 0.1748 - accuracy: 0.9482\n",
            "202/469 [===========>..................] - ETA: 2s - loss: 0.1735 - accuracy: 0.9487\n",
            "214/469 [============>.................] - ETA: 2s - loss: 0.1754 - accuracy: 0.9481\n",
            "228/469 [=============>................] - ETA: 2s - loss: 0.1739 - accuracy: 0.9479\n",
            "241/469 [==============>...............] - ETA: 1s - loss: 0.1749 - accuracy: 0.9477\n",
            "254/469 [===============>..............] - ETA: 1s - loss: 0.1764 - accuracy: 0.9473\n",
            "266/469 [================>.............] - ETA: 1s - loss: 0.1762 - accuracy: 0.9474\n",
            "278/469 [================>.............] - ETA: 1s - loss: 0.1751 - accuracy: 0.9474\n",
            "290/469 [=================>............] - ETA: 1s - loss: 0.1748 - accuracy: 0.9476\n",
            "302/469 [==================>...........] - ETA: 1s - loss: 0.1758 - accuracy: 0.9471\n",
            "315/469 [===================>..........] - ETA: 1s - loss: 0.1748 - accuracy: 0.9474\n",
            "327/469 [===================>..........] - ETA: 1s - loss: 0.1758 - accuracy: 0.9472\n",
            "339/469 [====================>.........] - ETA: 1s - loss: 0.1756 - accuracy: 0.9473\n",
            "352/469 [=====================>........] - ETA: 0s - loss: 0.1767 - accuracy: 0.9473\n",
            "358/469 [=====================>........] - ETA: 0s - loss: 0.1760 - accuracy: 0.9476\n",
            "370/469 [======================>.......] - ETA: 0s - loss: 0.1762 - accuracy: 0.9474\n",
            "383/469 [=======================>......] - ETA: 0s - loss: 0.1756 - accuracy: 0.9476\n",
            "395/469 [========================>.....] - ETA: 0s - loss: 0.1763 - accuracy: 0.9475\n",
            "407/469 [=========================>....] - ETA: 0s - loss: 0.1769 - accuracy: 0.9472\n",
            "419/469 [=========================>....] - ETA: 0s - loss: 0.1765 - accuracy: 0.9473\n",
            "431/469 [==========================>...] - ETA: 0s - loss: 0.1764 - accuracy: 0.9474\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.1770 - accuracy: 0.9472\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.9472\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9474\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1766 - accuracy: 0.9474 - val_loss: 0.0804 - val_accuracy: 0.9767\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 6/12\n",
            "  7/469 [..............................] - ETA: 3s - loss: 0.1407 - accuracy: 0.9632\n",
            " 20/469 [>.............................] - ETA: 3s - loss: 0.1380 - accuracy: 0.9625\n",
            " 33/469 [=>............................] - ETA: 3s - loss: 0.1545 - accuracy: 0.9619\n",
            " 45/469 [=>............................] - ETA: 3s - loss: 0.1534 - accuracy: 0.9597\n",
            " 58/469 [==>...........................] - ETA: 3s - loss: 0.1566 - accuracy: 0.9573\n",
            " 70/469 [===>..........................] - ETA: 3s - loss: 0.1614 - accuracy: 0.9569\n",
            " 82/469 [====>.........................] - ETA: 3s - loss: 0.1656 - accuracy: 0.9549\n",
            " 95/469 [=====>........................] - ETA: 3s - loss: 0.1624 - accuracy: 0.9547\n",
            "108/469 [=====>........................] - ETA: 3s - loss: 0.1658 - accuracy: 0.9544\n",
            "120/469 [======>.......................] - ETA: 2s - loss: 0.1653 - accuracy: 0.9539\n",
            "133/469 [=======>......................] - ETA: 2s - loss: 0.1652 - accuracy: 0.9539\n",
            "145/469 [========>.....................] - ETA: 2s - loss: 0.1664 - accuracy: 0.9532\n",
            "157/469 [=========>....................] - ETA: 2s - loss: 0.1675 - accuracy: 0.9524\n",
            "169/469 [=========>....................] - ETA: 2s - loss: 0.1671 - accuracy: 0.9521\n",
            "181/469 [==========>...................] - ETA: 2s - loss: 0.1666 - accuracy: 0.9515\n",
            "194/469 [===========>..................] - ETA: 2s - loss: 0.1671 - accuracy: 0.9512\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.1668 - accuracy: 0.9511\n",
            "219/469 [=============>................] - ETA: 2s - loss: 0.1707 - accuracy: 0.9506\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.1718 - accuracy: 0.9508\n",
            "243/469 [==============>...............] - ETA: 1s - loss: 0.1707 - accuracy: 0.9505\n",
            "256/469 [===============>..............] - ETA: 1s - loss: 0.1701 - accuracy: 0.9505\n",
            "268/469 [================>.............] - ETA: 1s - loss: 0.1694 - accuracy: 0.9508\n",
            "274/469 [================>.............] - ETA: 1s - loss: 0.1684 - accuracy: 0.9510\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.1685 - accuracy: 0.9512\n",
            "299/469 [==================>...........] - ETA: 1s - loss: 0.1681 - accuracy: 0.9512\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.1680 - accuracy: 0.9513\n",
            "323/469 [===================>..........] - ETA: 1s - loss: 0.1685 - accuracy: 0.9508\n",
            "335/469 [====================>.........] - ETA: 1s - loss: 0.1691 - accuracy: 0.9505\n",
            "349/469 [=====================>........] - ETA: 1s - loss: 0.1689 - accuracy: 0.9504\n",
            "361/469 [======================>.......] - ETA: 0s - loss: 0.1674 - accuracy: 0.9507\n",
            "373/469 [======================>.......] - ETA: 0s - loss: 0.1685 - accuracy: 0.9505\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.1681 - accuracy: 0.9506\n",
            "398/469 [========================>.....] - ETA: 0s - loss: 0.1681 - accuracy: 0.9506\n",
            "410/469 [=========================>....] - ETA: 0s - loss: 0.1694 - accuracy: 0.9504\n",
            "423/469 [==========================>...] - ETA: 0s - loss: 0.1687 - accuracy: 0.9506\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.1694 - accuracy: 0.9503\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.1688 - accuracy: 0.9504\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9503\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9502\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1692 - accuracy: 0.9502 - val_loss: 0.0879 - val_accuracy: 0.9754\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 7/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.2195 - accuracy: 0.9453\n",
            " 14/469 [..............................] - ETA: 3s - loss: 0.1583 - accuracy: 0.9537\n",
            " 21/469 [>.............................] - ETA: 3s - loss: 0.1527 - accuracy: 0.9535\n",
            " 27/469 [>.............................] - ETA: 3s - loss: 0.1587 - accuracy: 0.9525\n",
            " 33/469 [=>............................] - ETA: 3s - loss: 0.1599 - accuracy: 0.9527\n",
            " 40/469 [=>............................] - ETA: 3s - loss: 0.1655 - accuracy: 0.9508\n",
            " 46/469 [=>............................] - ETA: 3s - loss: 0.1658 - accuracy: 0.9504\n",
            " 53/469 [==>...........................] - ETA: 3s - loss: 0.1611 - accuracy: 0.9518\n",
            " 66/469 [===>..........................] - ETA: 3s - loss: 0.1684 - accuracy: 0.9497\n",
            " 72/469 [===>..........................] - ETA: 3s - loss: 0.1717 - accuracy: 0.9486\n",
            " 79/469 [====>.........................] - ETA: 3s - loss: 0.1690 - accuracy: 0.9497\n",
            " 86/469 [====>.........................] - ETA: 3s - loss: 0.1671 - accuracy: 0.9499\n",
            " 92/469 [====>.........................] - ETA: 3s - loss: 0.1665 - accuracy: 0.9496\n",
            " 98/469 [=====>........................] - ETA: 3s - loss: 0.1650 - accuracy: 0.9501\n",
            "105/469 [=====>........................] - ETA: 3s - loss: 0.1670 - accuracy: 0.9496\n",
            "118/469 [======>.......................] - ETA: 2s - loss: 0.1645 - accuracy: 0.9494\n",
            "130/469 [=======>......................] - ETA: 2s - loss: 0.1611 - accuracy: 0.9506\n",
            "142/469 [========>.....................] - ETA: 2s - loss: 0.1583 - accuracy: 0.9512\n",
            "154/469 [========>.....................] - ETA: 2s - loss: 0.1587 - accuracy: 0.9517\n",
            "166/469 [=========>....................] - ETA: 2s - loss: 0.1558 - accuracy: 0.9524\n",
            "178/469 [==========>...................] - ETA: 2s - loss: 0.1558 - accuracy: 0.9529\n",
            "190/469 [===========>..................] - ETA: 2s - loss: 0.1529 - accuracy: 0.9535\n",
            "203/469 [===========>..................] - ETA: 2s - loss: 0.1536 - accuracy: 0.9533\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.1537 - accuracy: 0.9532\n",
            "228/469 [=============>................] - ETA: 2s - loss: 0.1538 - accuracy: 0.9531\n",
            "240/469 [==============>...............] - ETA: 1s - loss: 0.1531 - accuracy: 0.9535\n",
            "252/469 [===============>..............] - ETA: 1s - loss: 0.1518 - accuracy: 0.9540\n",
            "264/469 [===============>..............] - ETA: 1s - loss: 0.1513 - accuracy: 0.9545\n",
            "270/469 [================>.............] - ETA: 1s - loss: 0.1521 - accuracy: 0.9544\n",
            "276/469 [================>.............] - ETA: 1s - loss: 0.1513 - accuracy: 0.9544\n",
            "283/469 [=================>............] - ETA: 1s - loss: 0.1515 - accuracy: 0.9543\n",
            "295/469 [=================>............] - ETA: 1s - loss: 0.1524 - accuracy: 0.9538\n",
            "307/469 [==================>...........] - ETA: 1s - loss: 0.1532 - accuracy: 0.9535\n",
            "319/469 [===================>..........] - ETA: 1s - loss: 0.1536 - accuracy: 0.9532\n",
            "331/469 [====================>.........] - ETA: 1s - loss: 0.1525 - accuracy: 0.9533\n",
            "343/469 [====================>.........] - ETA: 1s - loss: 0.1532 - accuracy: 0.9531\n",
            "355/469 [=====================>........] - ETA: 0s - loss: 0.1540 - accuracy: 0.9530\n",
            "368/469 [======================>.......] - ETA: 0s - loss: 0.1543 - accuracy: 0.9529\n",
            "381/469 [=======================>......] - ETA: 0s - loss: 0.1540 - accuracy: 0.9532\n",
            "393/469 [========================>.....] - ETA: 0s - loss: 0.1536 - accuracy: 0.9532\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.1540 - accuracy: 0.9533\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.1554 - accuracy: 0.9532\n",
            "431/469 [==========================>...] - ETA: 0s - loss: 0.1558 - accuracy: 0.9530\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.1550 - accuracy: 0.9534\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.1565 - accuracy: 0.9532\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.1573 - accuracy: 0.9530\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1580 - accuracy: 0.9528 - val_loss: 0.0825 - val_accuracy: 0.9772\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 8/12\n",
            "  7/469 [..............................] - ETA: 3s - loss: 0.1784 - accuracy: 0.9342\n",
            " 19/469 [>.............................] - ETA: 3s - loss: 0.1948 - accuracy: 0.9408\n",
            " 32/469 [=>............................] - ETA: 3s - loss: 0.1821 - accuracy: 0.9448\n",
            " 45/469 [=>............................] - ETA: 3s - loss: 0.1832 - accuracy: 0.9439\n",
            " 58/469 [==>...........................] - ETA: 3s - loss: 0.1721 - accuracy: 0.9469\n",
            " 70/469 [===>..........................] - ETA: 3s - loss: 0.1649 - accuracy: 0.9499\n",
            " 82/469 [====>.........................] - ETA: 3s - loss: 0.1655 - accuracy: 0.9508\n",
            " 94/469 [=====>........................] - ETA: 3s - loss: 0.1638 - accuracy: 0.9505\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.1611 - accuracy: 0.9514\n",
            "118/469 [======>.......................] - ETA: 2s - loss: 0.1604 - accuracy: 0.9517\n",
            "130/469 [=======>......................] - ETA: 2s - loss: 0.1599 - accuracy: 0.9516\n",
            "142/469 [========>.....................] - ETA: 2s - loss: 0.1590 - accuracy: 0.9516\n",
            "154/469 [========>.....................] - ETA: 2s - loss: 0.1599 - accuracy: 0.9520\n",
            "166/469 [=========>....................] - ETA: 2s - loss: 0.1613 - accuracy: 0.9517\n",
            "178/469 [==========>...................] - ETA: 2s - loss: 0.1602 - accuracy: 0.9520\n",
            "191/469 [===========>..................] - ETA: 2s - loss: 0.1616 - accuracy: 0.9516\n",
            "203/469 [===========>..................] - ETA: 2s - loss: 0.1634 - accuracy: 0.9512\n",
            "215/469 [============>.................] - ETA: 2s - loss: 0.1642 - accuracy: 0.9512\n",
            "228/469 [=============>................] - ETA: 2s - loss: 0.1639 - accuracy: 0.9512\n",
            "241/469 [==============>...............] - ETA: 1s - loss: 0.1678 - accuracy: 0.9505\n",
            "247/469 [==============>...............] - ETA: 1s - loss: 0.1676 - accuracy: 0.9505\n",
            "260/469 [===============>..............] - ETA: 1s - loss: 0.1686 - accuracy: 0.9504\n",
            "266/469 [================>.............] - ETA: 1s - loss: 0.1675 - accuracy: 0.9507\n",
            "273/469 [================>.............] - ETA: 1s - loss: 0.1669 - accuracy: 0.9507\n",
            "279/469 [================>.............] - ETA: 1s - loss: 0.1672 - accuracy: 0.9506\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.1667 - accuracy: 0.9505\n",
            "298/469 [==================>...........] - ETA: 1s - loss: 0.1675 - accuracy: 0.9506\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.1683 - accuracy: 0.9504\n",
            "324/469 [===================>..........] - ETA: 1s - loss: 0.1686 - accuracy: 0.9505\n",
            "331/469 [====================>.........] - ETA: 1s - loss: 0.1680 - accuracy: 0.9507\n",
            "337/469 [====================>.........] - ETA: 1s - loss: 0.1683 - accuracy: 0.9507\n",
            "350/469 [=====================>........] - ETA: 1s - loss: 0.1666 - accuracy: 0.9510\n",
            "357/469 [=====================>........] - ETA: 0s - loss: 0.1668 - accuracy: 0.9510\n",
            "363/469 [======================>.......] - ETA: 0s - loss: 0.1671 - accuracy: 0.9510\n",
            "369/469 [======================>.......] - ETA: 0s - loss: 0.1672 - accuracy: 0.9509\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.1672 - accuracy: 0.9509\n",
            "382/469 [=======================>......] - ETA: 0s - loss: 0.1670 - accuracy: 0.9509\n",
            "395/469 [========================>.....] - ETA: 0s - loss: 0.1674 - accuracy: 0.9508\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.1673 - accuracy: 0.9508\n",
            "408/469 [=========================>....] - ETA: 0s - loss: 0.1668 - accuracy: 0.9508\n",
            "414/469 [=========================>....] - ETA: 0s - loss: 0.1667 - accuracy: 0.9508\n",
            "426/469 [==========================>...] - ETA: 0s - loss: 0.1665 - accuracy: 0.9510\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.1648 - accuracy: 0.9514\n",
            "452/469 [===========================>..] - ETA: 0s - loss: 0.1654 - accuracy: 0.9513\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9514\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1656 - accuracy: 0.9513 - val_loss: 0.0913 - val_accuracy: 0.9742\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 9/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.1676 - accuracy: 0.9609\n",
            " 14/469 [..............................] - ETA: 3s - loss: 0.1244 - accuracy: 0.9570\n",
            " 28/469 [>.............................] - ETA: 3s - loss: 0.1447 - accuracy: 0.9559\n",
            " 42/469 [=>............................] - ETA: 3s - loss: 0.1460 - accuracy: 0.9567\n",
            " 55/469 [==>...........................] - ETA: 3s - loss: 0.1482 - accuracy: 0.9555\n",
            " 62/469 [==>...........................] - ETA: 3s - loss: 0.1486 - accuracy: 0.9551\n",
            " 75/469 [===>..........................] - ETA: 3s - loss: 0.1572 - accuracy: 0.9535\n",
            " 81/469 [====>.........................] - ETA: 3s - loss: 0.1560 - accuracy: 0.9534\n",
            " 88/469 [====>.........................] - ETA: 3s - loss: 0.1578 - accuracy: 0.9533\n",
            " 94/469 [=====>........................] - ETA: 3s - loss: 0.1579 - accuracy: 0.9524\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.1569 - accuracy: 0.9528\n",
            "115/469 [======>.......................] - ETA: 2s - loss: 0.1573 - accuracy: 0.9532\n",
            "129/469 [=======>......................] - ETA: 2s - loss: 0.1540 - accuracy: 0.9537\n",
            "142/469 [========>.....................] - ETA: 2s - loss: 0.1530 - accuracy: 0.9543\n",
            "156/469 [========>.....................] - ETA: 2s - loss: 0.1555 - accuracy: 0.9540\n",
            "170/469 [=========>....................] - ETA: 2s - loss: 0.1566 - accuracy: 0.9537\n",
            "182/469 [==========>...................] - ETA: 2s - loss: 0.1568 - accuracy: 0.9535\n",
            "195/469 [===========>..................] - ETA: 2s - loss: 0.1546 - accuracy: 0.9537\n",
            "209/469 [============>.................] - ETA: 2s - loss: 0.1551 - accuracy: 0.9536\n",
            "222/469 [=============>................] - ETA: 2s - loss: 0.1536 - accuracy: 0.9538\n",
            "229/469 [=============>................] - ETA: 1s - loss: 0.1533 - accuracy: 0.9538\n",
            "235/469 [==============>...............] - ETA: 1s - loss: 0.1532 - accuracy: 0.9538\n",
            "241/469 [==============>...............] - ETA: 1s - loss: 0.1529 - accuracy: 0.9540\n",
            "248/469 [==============>...............] - ETA: 1s - loss: 0.1530 - accuracy: 0.9538\n",
            "261/469 [===============>..............] - ETA: 1s - loss: 0.1537 - accuracy: 0.9536\n",
            "268/469 [================>.............] - ETA: 1s - loss: 0.1534 - accuracy: 0.9539\n",
            "274/469 [================>.............] - ETA: 1s - loss: 0.1552 - accuracy: 0.9537\n",
            "281/469 [================>.............] - ETA: 1s - loss: 0.1552 - accuracy: 0.9535\n",
            "287/469 [=================>............] - ETA: 1s - loss: 0.1544 - accuracy: 0.9536\n",
            "299/469 [==================>...........] - ETA: 1s - loss: 0.1546 - accuracy: 0.9534\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.1558 - accuracy: 0.9532\n",
            "323/469 [===================>..........] - ETA: 1s - loss: 0.1557 - accuracy: 0.9533\n",
            "335/469 [====================>.........] - ETA: 1s - loss: 0.1555 - accuracy: 0.9534\n",
            "347/469 [=====================>........] - ETA: 1s - loss: 0.1555 - accuracy: 0.9533\n",
            "359/469 [=====================>........] - ETA: 0s - loss: 0.1552 - accuracy: 0.9533\n",
            "371/469 [======================>.......] - ETA: 0s - loss: 0.1558 - accuracy: 0.9532\n",
            "383/469 [=======================>......] - ETA: 0s - loss: 0.1560 - accuracy: 0.9532\n",
            "395/469 [========================>.....] - ETA: 0s - loss: 0.1572 - accuracy: 0.9529\n",
            "407/469 [=========================>....] - ETA: 0s - loss: 0.1566 - accuracy: 0.9529\n",
            "420/469 [=========================>....] - ETA: 0s - loss: 0.1571 - accuracy: 0.9529\n",
            "432/469 [==========================>...] - ETA: 0s - loss: 0.1566 - accuracy: 0.9529\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.1564 - accuracy: 0.9527\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.9527\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9529\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1562 - accuracy: 0.9528 - val_loss: 0.0882 - val_accuracy: 0.9765\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 10/12\n",
            "  7/469 [..............................] - ETA: 3s - loss: 0.1520 - accuracy: 0.9509\n",
            " 20/469 [>.............................] - ETA: 3s - loss: 0.1602 - accuracy: 0.9461\n",
            " 33/469 [=>............................] - ETA: 3s - loss: 0.1679 - accuracy: 0.9482\n",
            " 45/469 [=>............................] - ETA: 3s - loss: 0.1558 - accuracy: 0.9510\n",
            " 58/469 [==>...........................] - ETA: 3s - loss: 0.1575 - accuracy: 0.9518\n",
            " 71/469 [===>..........................] - ETA: 3s - loss: 0.1564 - accuracy: 0.9529\n",
            " 83/469 [====>.........................] - ETA: 3s - loss: 0.1576 - accuracy: 0.9535\n",
            " 95/469 [=====>........................] - ETA: 3s - loss: 0.1577 - accuracy: 0.9532\n",
            "107/469 [=====>........................] - ETA: 3s - loss: 0.1574 - accuracy: 0.9531\n",
            "119/469 [======>.......................] - ETA: 2s - loss: 0.1532 - accuracy: 0.9535\n",
            "131/469 [=======>......................] - ETA: 2s - loss: 0.1514 - accuracy: 0.9548\n",
            "137/469 [=======>......................] - ETA: 2s - loss: 0.1491 - accuracy: 0.9553\n",
            "149/469 [========>.....................] - ETA: 2s - loss: 0.1477 - accuracy: 0.9561\n",
            "161/469 [=========>....................] - ETA: 2s - loss: 0.1485 - accuracy: 0.9562\n",
            "173/469 [==========>...................] - ETA: 2s - loss: 0.1513 - accuracy: 0.9556\n",
            "185/469 [==========>...................] - ETA: 2s - loss: 0.1550 - accuracy: 0.9546\n",
            "197/469 [===========>..................] - ETA: 2s - loss: 0.1555 - accuracy: 0.9542\n",
            "209/469 [============>.................] - ETA: 2s - loss: 0.1577 - accuracy: 0.9536\n",
            "221/469 [=============>................] - ETA: 2s - loss: 0.1593 - accuracy: 0.9529\n",
            "234/469 [=============>................] - ETA: 1s - loss: 0.1596 - accuracy: 0.9531\n",
            "246/469 [==============>...............] - ETA: 1s - loss: 0.1600 - accuracy: 0.9529\n",
            "258/469 [===============>..............] - ETA: 1s - loss: 0.1595 - accuracy: 0.9527\n",
            "271/469 [================>.............] - ETA: 1s - loss: 0.1593 - accuracy: 0.9525\n",
            "284/469 [=================>............] - ETA: 1s - loss: 0.1592 - accuracy: 0.9526\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.1583 - accuracy: 0.9529\n",
            "309/469 [==================>...........] - ETA: 1s - loss: 0.1577 - accuracy: 0.9529\n",
            "322/469 [===================>..........] - ETA: 1s - loss: 0.1584 - accuracy: 0.9528\n",
            "334/469 [====================>.........] - ETA: 1s - loss: 0.1569 - accuracy: 0.9531\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.1564 - accuracy: 0.9531\n",
            "358/469 [=====================>........] - ETA: 0s - loss: 0.1561 - accuracy: 0.9532\n",
            "370/469 [======================>.......] - ETA: 0s - loss: 0.1554 - accuracy: 0.9533\n",
            "382/469 [=======================>......] - ETA: 0s - loss: 0.1538 - accuracy: 0.9538\n",
            "388/469 [=======================>......] - ETA: 0s - loss: 0.1534 - accuracy: 0.9539\n",
            "400/469 [========================>.....] - ETA: 0s - loss: 0.1536 - accuracy: 0.9538\n",
            "413/469 [=========================>....] - ETA: 0s - loss: 0.1531 - accuracy: 0.9538\n",
            "425/469 [==========================>...] - ETA: 0s - loss: 0.1541 - accuracy: 0.9538\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.1541 - accuracy: 0.9538\n",
            "449/469 [===========================>..] - ETA: 0s - loss: 0.1539 - accuracy: 0.9537\n",
            "462/469 [============================>.] - ETA: 0s - loss: 0.1539 - accuracy: 0.9538\n",
            "468/469 [============================>.] - ETA: 0s - loss: 0.1539 - accuracy: 0.9538\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1537 - accuracy: 0.9538 - val_loss: 0.0880 - val_accuracy: 0.9774\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 11/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.1515 - accuracy: 0.9531\n",
            " 13/469 [..............................] - ETA: 3s - loss: 0.1561 - accuracy: 0.9525\n",
            " 25/469 [>.............................] - ETA: 3s - loss: 0.1364 - accuracy: 0.9566\n",
            " 37/469 [=>............................] - ETA: 3s - loss: 0.1438 - accuracy: 0.9540\n",
            " 49/469 [==>...........................] - ETA: 3s - loss: 0.1413 - accuracy: 0.9544\n",
            " 61/469 [==>...........................] - ETA: 3s - loss: 0.1459 - accuracy: 0.9535\n",
            " 73/469 [===>..........................] - ETA: 3s - loss: 0.1484 - accuracy: 0.9536\n",
            " 85/469 [====>.........................] - ETA: 3s - loss: 0.1431 - accuracy: 0.9547\n",
            " 97/469 [=====>........................] - ETA: 3s - loss: 0.1456 - accuracy: 0.9555\n",
            "109/469 [=====>........................] - ETA: 3s - loss: 0.1457 - accuracy: 0.9551\n",
            "121/469 [======>.......................] - ETA: 3s - loss: 0.1442 - accuracy: 0.9554\n",
            "133/469 [=======>......................] - ETA: 2s - loss: 0.1443 - accuracy: 0.9551\n",
            "145/469 [========>.....................] - ETA: 2s - loss: 0.1435 - accuracy: 0.9553\n",
            "157/469 [=========>....................] - ETA: 2s - loss: 0.1464 - accuracy: 0.9551\n",
            "170/469 [=========>....................] - ETA: 2s - loss: 0.1471 - accuracy: 0.9553\n",
            "182/469 [==========>...................] - ETA: 2s - loss: 0.1453 - accuracy: 0.9562\n",
            "194/469 [===========>..................] - ETA: 2s - loss: 0.1469 - accuracy: 0.9557\n",
            "206/469 [============>.................] - ETA: 2s - loss: 0.1466 - accuracy: 0.9559\n",
            "219/469 [=============>................] - ETA: 2s - loss: 0.1463 - accuracy: 0.9561\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.1471 - accuracy: 0.9558\n",
            "243/469 [==============>...............] - ETA: 1s - loss: 0.1456 - accuracy: 0.9560\n",
            "256/469 [===============>..............] - ETA: 1s - loss: 0.1454 - accuracy: 0.9557\n",
            "268/469 [================>.............] - ETA: 1s - loss: 0.1452 - accuracy: 0.9558\n",
            "280/469 [================>.............] - ETA: 1s - loss: 0.1453 - accuracy: 0.9556\n",
            "293/469 [=================>............] - ETA: 1s - loss: 0.1454 - accuracy: 0.9554\n",
            "299/469 [==================>...........] - ETA: 1s - loss: 0.1452 - accuracy: 0.9556\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.1457 - accuracy: 0.9559\n",
            "323/469 [===================>..........] - ETA: 1s - loss: 0.1460 - accuracy: 0.9560\n",
            "335/469 [====================>.........] - ETA: 1s - loss: 0.1465 - accuracy: 0.9559\n",
            "347/469 [=====================>........] - ETA: 1s - loss: 0.1472 - accuracy: 0.9559\n",
            "359/469 [=====================>........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9554\n",
            "372/469 [======================>.......] - ETA: 0s - loss: 0.1477 - accuracy: 0.9556\n",
            "384/469 [=======================>......] - ETA: 0s - loss: 0.1473 - accuracy: 0.9554\n",
            "396/469 [========================>.....] - ETA: 0s - loss: 0.1465 - accuracy: 0.9554\n",
            "408/469 [=========================>....] - ETA: 0s - loss: 0.1476 - accuracy: 0.9553\n",
            "420/469 [=========================>....] - ETA: 0s - loss: 0.1486 - accuracy: 0.9551\n",
            "432/469 [==========================>...] - ETA: 0s - loss: 0.1481 - accuracy: 0.9551\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.1488 - accuracy: 0.9552\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.1492 - accuracy: 0.9553\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.1489 - accuracy: 0.9553\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1487 - accuracy: 0.9553 - val_loss: 0.1021 - val_accuracy: 0.9746\n",
            "\u001b[36m(train_mnist pid=3300944)\u001b[0m Epoch 12/12\n",
            "  7/469 [..............................] - ETA: 4s - loss: 0.1771 - accuracy: 0.9554\n",
            " 19/469 [>.............................] - ETA: 3s - loss: 0.1594 - accuracy: 0.9556\n",
            " 31/469 [>.............................] - ETA: 3s - loss: 0.1711 - accuracy: 0.9544\n",
            " 43/469 [=>............................] - ETA: 3s - loss: 0.1656 - accuracy: 0.9548\n",
            " 56/469 [==>...........................] - ETA: 3s - loss: 0.1691 - accuracy: 0.9555\n",
            " 68/469 [===>..........................] - ETA: 3s - loss: 0.1689 - accuracy: 0.9544\n",
            " 81/469 [====>.........................] - ETA: 3s - loss: 0.1630 - accuracy: 0.9546\n",
            " 93/469 [====>.........................] - ETA: 3s - loss: 0.1599 - accuracy: 0.9549\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.1645 - accuracy: 0.9543\n",
            "119/469 [======>.......................] - ETA: 2s - loss: 0.1659 - accuracy: 0.9532\n",
            "132/469 [=======>......................] - ETA: 2s - loss: 0.1678 - accuracy: 0.9528\n",
            "144/469 [========>.....................] - ETA: 2s - loss: 0.1660 - accuracy: 0.9536\n",
            "157/469 [=========>....................] - ETA: 2s - loss: 0.1677 - accuracy: 0.9530\n",
            "170/469 [=========>....................] - ETA: 2s - loss: 0.1662 - accuracy: 0.9532\n",
            "182/469 [==========>...................] - ETA: 2s - loss: 0.1662 - accuracy: 0.9534\n",
            "194/469 [===========>..................] - ETA: 2s - loss: 0.1636 - accuracy: 0.9540\n",
            "207/469 [============>.................] - ETA: 2s - loss: 0.1614 - accuracy: 0.9544\n",
            "220/469 [=============>................] - ETA: 2s - loss: 0.1594 - accuracy: 0.9549\n",
            "232/469 [=============>................] - ETA: 2s - loss: 0.1583 - accuracy: 0.9552\n",
            "244/469 [==============>...............] - ETA: 1s - loss: 0.1600 - accuracy: 0.9549\n",
            "256/469 [===============>..............] - ETA: 1s - loss: 0.1599 - accuracy: 0.9546\n",
            "262/469 [===============>..............] - ETA: 1s - loss: 0.1596 - accuracy: 0.9547\n",
            "268/469 [================>.............] - ETA: 1s - loss: 0.1611 - accuracy: 0.9546\n",
            "275/469 [================>.............] - ETA: 1s - loss: 0.1610 - accuracy: 0.9548\n",
            "287/469 [=================>............] - ETA: 1s - loss: 0.1601 - accuracy: 0.9549\n",
            "299/469 [==================>...........] - ETA: 1s - loss: 0.1597 - accuracy: 0.9549\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.1595 - accuracy: 0.9550\n",
            "323/469 [===================>..........] - ETA: 1s - loss: 0.1593 - accuracy: 0.9547\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.1600 - accuracy: 0.9545\n",
            "349/469 [=====================>........] - ETA: 1s - loss: 0.1592 - accuracy: 0.9545\n",
            "361/469 [======================>.......] - ETA: 0s - loss: 0.1584 - accuracy: 0.9546\n",
            "373/469 [======================>.......] - ETA: 0s - loss: 0.1581 - accuracy: 0.9544\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.1579 - accuracy: 0.9545\n",
            "398/469 [========================>.....] - ETA: 0s - loss: 0.1579 - accuracy: 0.9545\n",
            "410/469 [=========================>....] - ETA: 0s - loss: 0.1574 - accuracy: 0.9546\n",
            "422/469 [=========================>....] - ETA: 0s - loss: 0.1574 - accuracy: 0.9546\n",
            "434/469 [==========================>...] - ETA: 0s - loss: 0.1564 - accuracy: 0.9547\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.1555 - accuracy: 0.9551\n",
            "459/469 [============================>.] - ETA: 0s - loss: 0.1554 - accuracy: 0.9551\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9549\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.1554 - accuracy: 0.9549 - val_loss: 0.0815 - val_accuracy: 0.9793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3301447)\u001b[0m 2023-12-05 01:55:25.084528: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3301447)\u001b[0m 2023-12-05 01:55:25.133737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3301447)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3301447)\u001b[0m 2023-12-05 01:55:25.133233: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3301447)\u001b[0m 2023-12-05 01:55:26.028646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3301447)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3301447)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3301447)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3301447)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3301447)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3301447)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3301447)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3301447)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m 2023-12-05 01:55:27.608507: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 1/12\n",
            "  1/938 [..............................] - ETA: 9:54 - loss: 2.3186 - accuracy: 0.1250\n",
            "  7/938 [..............................] - ETA: 17s - loss: 164.5546 - accuracy: 0.1049\n",
            " 13/938 [..............................] - ETA: 17s - loss: 89.6986 - accuracy: 0.1154 \n",
            " 19/938 [..............................] - ETA: 17s - loss: 62.1062 - accuracy: 0.1127\n",
            " 25/938 [..............................] - ETA: 17s - loss: 47.7578 - accuracy: 0.1075\n",
            " 31/938 [..............................] - ETA: 16s - loss: 38.9607 - accuracy: 0.1033\n",
            " 37/938 [>.............................] - ETA: 16s - loss: 33.0175 - accuracy: 0.1001\n",
            " 40/938 [>.............................] - ETA: 16s - loss: 30.7146 - accuracy: 0.0980\n",
            " 43/938 [>.............................] - ETA: 16s - loss: 28.7328 - accuracy: 0.0988\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 27.0085 - accuracy: 0.1036\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 24.1586 - accuracy: 0.1043\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 21.8971 - accuracy: 0.1070\n",
            " 64/938 [=>............................] - ETA: 16s - loss: 20.0602 - accuracy: 0.1057\n",
            " 70/938 [=>............................] - ETA: 16s - loss: 18.5388 - accuracy: 0.1058\n",
            " 76/938 [=>............................] - ETA: 16s - loss: 17.2574 - accuracy: 0.1069\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 16.1637 - accuracy: 0.1046\n",
            " 88/938 [=>............................] - ETA: 15s - loss: 15.2192 - accuracy: 0.1042\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 14.3949 - accuracy: 0.1032\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 13.6697 - accuracy: 0.1052\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 13.0264 - accuracy: 0.1060\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 12.4521 - accuracy: 0.1067\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 12.1879 - accuracy: 0.1063\n",
            "118/938 [==>...........................] - ETA: 15s - loss: 11.9367 - accuracy: 0.1062\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 11.6977 - accuracy: 0.1065\n",
            "124/938 [==>...........................] - ETA: 15s - loss: 11.4707 - accuracy: 0.1051\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 11.2543 - accuracy: 0.1046\n",
            "133/938 [===>..........................] - ETA: 15s - loss: 10.8505 - accuracy: 0.1048\n",
            "139/938 [===>..........................] - ETA: 14s - loss: 10.4816 - accuracy: 0.1042\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 10.1429 - accuracy: 0.1063\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 9.8323 - accuracy: 0.1062\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 9.5445 - accuracy: 0.1067\n",
            "163/938 [====>.........................] - ETA: 14s - loss: 9.2784 - accuracy: 0.1058\n",
            "169/938 [====>.........................] - ETA: 14s - loss: 9.0310 - accuracy: 0.1054\n",
            "175/938 [====>.........................] - ETA: 14s - loss: 8.8004 - accuracy: 0.1063\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 8.5856 - accuracy: 0.1050\n",
            "187/938 [====>.........................] - ETA: 14s - loss: 8.3840 - accuracy: 0.1048\n",
            "190/938 [=====>........................] - ETA: 13s - loss: 8.2881 - accuracy: 0.1051\n",
            "193/938 [=====>........................] - ETA: 13s - loss: 8.1951 - accuracy: 0.1049\n",
            "196/938 [=====>........................] - ETA: 13s - loss: 8.1053 - accuracy: 0.1047\n",
            "199/938 [=====>........................] - ETA: 13s - loss: 8.0179 - accuracy: 0.1044\n",
            "202/938 [=====>........................] - ETA: 13s - loss: 7.9332 - accuracy: 0.1038\n",
            "208/938 [=====>........................] - ETA: 13s - loss: 7.7707 - accuracy: 0.1041\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 7.6176 - accuracy: 0.1041\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 7.4728 - accuracy: 0.1050\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 7.3359 - accuracy: 0.1056\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 7.2059 - accuracy: 0.1054\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 7.0824 - accuracy: 0.1054\n",
            "244/938 [======>.......................] - ETA: 12s - loss: 6.9649 - accuracy: 0.1057\n",
            "250/938 [======>.......................] - ETA: 12s - loss: 6.8532 - accuracy: 0.1056\n",
            "253/938 [=======>......................] - ETA: 12s - loss: 6.7993 - accuracy: 0.1054\n",
            "259/938 [=======>......................] - ETA: 12s - loss: 6.6952 - accuracy: 0.1057\n",
            "262/938 [=======>......................] - ETA: 12s - loss: 6.6450 - accuracy: 0.1056\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 6.5958 - accuracy: 0.1060\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 6.5007 - accuracy: 0.1059\n",
            "277/938 [=======>......................] - ETA: 12s - loss: 6.4101 - accuracy: 0.1059\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 6.3231 - accuracy: 0.1053\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 6.2397 - accuracy: 0.1050\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 6.1596 - accuracy: 0.1056\n",
            "301/938 [========>.....................] - ETA: 11s - loss: 6.0830 - accuracy: 0.1051\n",
            "307/938 [========>.....................] - ETA: 11s - loss: 6.0092 - accuracy: 0.1049\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 5.9382 - accuracy: 0.1051\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 5.8698 - accuracy: 0.1051\n",
            "322/938 [=========>....................] - ETA: 11s - loss: 5.8367 - accuracy: 0.1051\n",
            "325/938 [=========>....................] - ETA: 11s - loss: 5.8041 - accuracy: 0.1053\n",
            "328/938 [=========>....................] - ETA: 11s - loss: 5.7722 - accuracy: 0.1051\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 5.7102 - accuracy: 0.1045\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 5.6501 - accuracy: 0.1048\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 5.5921 - accuracy: 0.1049\n",
            "352/938 [==========>...................] - ETA: 10s - loss: 5.5361 - accuracy: 0.1051\n",
            "358/938 [==========>...................] - ETA: 10s - loss: 5.4821 - accuracy: 0.1049\n",
            "364/938 [==========>...................] - ETA: 10s - loss: 5.4297 - accuracy: 0.1046\n",
            "370/938 [==========>...................] - ETA: 10s - loss: 5.3791 - accuracy: 0.1046\n",
            "376/938 [===========>..................] - ETA: 10s - loss: 5.3302 - accuracy: 0.1046\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 5.2827 - accuracy: 0.1047\n",
            "388/938 [===========>..................] - ETA: 10s - loss: 5.2367 - accuracy: 0.1047\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 5.2142 - accuracy: 0.1047\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 5.1703 - accuracy: 0.1047\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 5.1276 - accuracy: 0.1048\n",
            "409/938 [============>.................] - ETA: 9s - loss: 5.0861 - accuracy: 0.1049\n",
            "415/938 [============>.................] - ETA: 9s - loss: 5.0459 - accuracy: 0.1049\n",
            "421/938 [============>.................] - ETA: 9s - loss: 5.0068 - accuracy: 0.1048\n",
            "427/938 [============>.................] - ETA: 9s - loss: 4.9690 - accuracy: 0.1045\n",
            "433/938 [============>.................] - ETA: 9s - loss: 4.9321 - accuracy: 0.1044\n",
            "439/938 [=============>................] - ETA: 9s - loss: 4.8962 - accuracy: 0.1044\n",
            "445/938 [=============>................] - ETA: 9s - loss: 4.8612 - accuracy: 0.1044\n",
            "448/938 [=============>................] - ETA: 9s - loss: 4.8441 - accuracy: 0.1043\n",
            "451/938 [=============>................] - ETA: 9s - loss: 4.8273 - accuracy: 0.1044\n",
            "454/938 [=============>................] - ETA: 9s - loss: 4.8107 - accuracy: 0.1044\n",
            "457/938 [=============>................] - ETA: 8s - loss: 4.7943 - accuracy: 0.1043\n",
            "460/938 [=============>................] - ETA: 8s - loss: 4.7781 - accuracy: 0.1042\n",
            "463/938 [=============>................] - ETA: 8s - loss: 4.7621 - accuracy: 0.1042\n",
            "466/938 [=============>................] - ETA: 8s - loss: 4.7463 - accuracy: 0.1043\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 4.7152 - accuracy: 0.1042\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 4.6848 - accuracy: 0.1044\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 4.6554 - accuracy: 0.1043\n",
            "490/938 [==============>...............] - ETA: 8s - loss: 4.6267 - accuracy: 0.1044\n",
            "496/938 [==============>...............] - ETA: 8s - loss: 4.5986 - accuracy: 0.1045\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 4.5713 - accuracy: 0.1042\n",
            "508/938 [===============>..............] - ETA: 8s - loss: 4.5445 - accuracy: 0.1041\n",
            "514/938 [===============>..............] - ETA: 7s - loss: 4.5183 - accuracy: 0.1042\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 4.5055 - accuracy: 0.1042\n",
            "520/938 [===============>..............] - ETA: 7s - loss: 4.4928 - accuracy: 0.1043\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 4.4803 - accuracy: 0.1042\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 4.4557 - accuracy: 0.1045\n",
            "535/938 [================>.............] - ETA: 7s - loss: 4.4315 - accuracy: 0.1048\n",
            "541/938 [================>.............] - ETA: 7s - loss: 4.4081 - accuracy: 0.1049\n",
            "547/938 [================>.............] - ETA: 7s - loss: 4.3850 - accuracy: 0.1053\n",
            "553/938 [================>.............] - ETA: 7s - loss: 4.3625 - accuracy: 0.1055\n",
            "559/938 [================>.............] - ETA: 7s - loss: 4.3405 - accuracy: 0.1053\n",
            "565/938 [=================>............] - ETA: 6s - loss: 4.3190 - accuracy: 0.1049\n",
            "571/938 [=================>............] - ETA: 6s - loss: 4.2978 - accuracy: 0.1050\n",
            "574/938 [=================>............] - ETA: 6s - loss: 4.2875 - accuracy: 0.1050\n",
            "580/938 [=================>............] - ETA: 6s - loss: 4.2670 - accuracy: 0.1050\n",
            "586/938 [=================>............] - ETA: 6s - loss: 4.2469 - accuracy: 0.1048\n",
            "592/938 [=================>............] - ETA: 6s - loss: 4.2273 - accuracy: 0.1047\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 4.2080 - accuracy: 0.1047\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 4.1891 - accuracy: 0.1048\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 4.1706 - accuracy: 0.1049\n",
            "616/938 [==================>...........] - ETA: 6s - loss: 4.1525 - accuracy: 0.1047\n",
            "622/938 [==================>...........] - ETA: 5s - loss: 4.1346 - accuracy: 0.1045\n",
            "628/938 [===================>..........] - ETA: 5s - loss: 4.1172 - accuracy: 0.1047\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 4.1087 - accuracy: 0.1045\n",
            "637/938 [===================>..........] - ETA: 5s - loss: 4.0918 - accuracy: 0.1043\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 4.0750 - accuracy: 0.1041\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 4.0588 - accuracy: 0.1042\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 4.0428 - accuracy: 0.1043\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 4.0270 - accuracy: 0.1043\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 4.0113 - accuracy: 0.1046\n",
            "673/938 [====================>.........] - ETA: 4s - loss: 3.9962 - accuracy: 0.1045\n",
            "679/938 [====================>.........] - ETA: 4s - loss: 3.9814 - accuracy: 0.1045\n",
            "685/938 [====================>.........] - ETA: 4s - loss: 3.9667 - accuracy: 0.1044\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 3.9595 - accuracy: 0.1043\n",
            "691/938 [=====================>........] - ETA: 4s - loss: 3.9523 - accuracy: 0.1042\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 3.9452 - accuracy: 0.1041\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 3.9312 - accuracy: 0.1042\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 3.9174 - accuracy: 0.1041\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 3.9038 - accuracy: 0.1041\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 3.8904 - accuracy: 0.1043\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 3.8773 - accuracy: 0.1041\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 3.8644 - accuracy: 0.1040\n",
            "736/938 [======================>.......] - ETA: 3s - loss: 3.8517 - accuracy: 0.1039\n",
            "742/938 [======================>.......] - ETA: 3s - loss: 3.8392 - accuracy: 0.1039\n",
            "748/938 [======================>.......] - ETA: 3s - loss: 3.8269 - accuracy: 0.1039\n",
            "754/938 [=======================>......] - ETA: 3s - loss: 3.8148 - accuracy: 0.1039\n",
            "760/938 [=======================>......] - ETA: 3s - loss: 3.8029 - accuracy: 0.1041\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 3.7971 - accuracy: 0.1040\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 3.7855 - accuracy: 0.1040\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 3.7741 - accuracy: 0.1040\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 3.7628 - accuracy: 0.1040\n",
            "787/938 [========================>.....] - ETA: 2s - loss: 3.7517 - accuracy: 0.1040\n",
            "793/938 [========================>.....] - ETA: 2s - loss: 3.7408 - accuracy: 0.1040\n",
            "799/938 [========================>.....] - ETA: 2s - loss: 3.7300 - accuracy: 0.1042\n",
            "805/938 [========================>.....] - ETA: 2s - loss: 3.7194 - accuracy: 0.1042\n",
            "811/938 [========================>.....] - ETA: 2s - loss: 3.7090 - accuracy: 0.1042\n",
            "817/938 [=========================>....] - ETA: 2s - loss: 3.6988 - accuracy: 0.1043\n",
            "823/938 [=========================>....] - ETA: 2s - loss: 3.6886 - accuracy: 0.1043\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 3.6836 - accuracy: 0.1041\n",
            "829/938 [=========================>....] - ETA: 2s - loss: 3.6786 - accuracy: 0.1041\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 3.6737 - accuracy: 0.1040\n",
            "835/938 [=========================>....] - ETA: 1s - loss: 3.6687 - accuracy: 0.1040\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 3.6638 - accuracy: 0.1042\n",
            "844/938 [=========================>....] - ETA: 1s - loss: 3.6542 - accuracy: 0.1043\n",
            "850/938 [==========================>...] - ETA: 1s - loss: 3.6446 - accuracy: 0.1045\n",
            "856/938 [==========================>...] - ETA: 1s - loss: 3.6352 - accuracy: 0.1046\n",
            "862/938 [==========================>...] - ETA: 1s - loss: 3.6260 - accuracy: 0.1047\n",
            "868/938 [==========================>...] - ETA: 1s - loss: 3.6168 - accuracy: 0.1048\n",
            "874/938 [==========================>...] - ETA: 1s - loss: 3.6078 - accuracy: 0.1049\n",
            "880/938 [===========================>..] - ETA: 1s - loss: 3.5988 - accuracy: 0.1049\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 3.5944 - accuracy: 0.1049\n",
            "886/938 [===========================>..] - ETA: 0s - loss: 3.5900 - accuracy: 0.1049\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 3.5857 - accuracy: 0.1048\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 3.5771 - accuracy: 0.1048\n",
            "901/938 [===========================>..] - ETA: 0s - loss: 3.5687 - accuracy: 0.1049\n",
            "907/938 [============================>.] - ETA: 0s - loss: 3.5604 - accuracy: 0.1047\n",
            "913/938 [============================>.] - ETA: 0s - loss: 3.5522 - accuracy: 0.1047\n",
            "919/938 [============================>.] - ETA: 0s - loss: 3.5441 - accuracy: 0.1048\n",
            "925/938 [============================>.] - ETA: 0s - loss: 3.5361 - accuracy: 0.1046\n",
            "931/938 [============================>.] - ETA: 0s - loss: 3.5282 - accuracy: 0.1045\n",
            "937/938 [============================>.] - ETA: 0s - loss: 3.5204 - accuracy: 0.1045\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 3.5198 - accuracy: 0.1044 - val_loss: 2.3108 - val_accuracy: 0.0892\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 2/12\n",
            "  4/938 [..............................] - ETA: 18s - loss: 2.3110 - accuracy: 0.0977\n",
            "  7/938 [..............................] - ETA: 17s - loss: 2.3078 - accuracy: 0.1205\n",
            " 13/938 [..............................] - ETA: 17s - loss: 2.3067 - accuracy: 0.1178\n",
            " 19/938 [..............................] - ETA: 17s - loss: 2.3089 - accuracy: 0.1118\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3072 - accuracy: 0.1181\n",
            " 31/938 [..............................] - ETA: 17s - loss: 2.3064 - accuracy: 0.1149\n",
            " 37/938 [>.............................] - ETA: 16s - loss: 2.3051 - accuracy: 0.1136\n",
            " 43/938 [>.............................] - ETA: 16s - loss: 2.3058 - accuracy: 0.1083\n",
            " 49/938 [>.............................] - ETA: 16s - loss: 2.3064 - accuracy: 0.1059\n",
            " 55/938 [>.............................] - ETA: 16s - loss: 2.3072 - accuracy: 0.1051\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3065 - accuracy: 0.1080\n",
            " 61/938 [>.............................] - ETA: 16s - loss: 2.3070 - accuracy: 0.1078\n",
            " 64/938 [=>............................] - ETA: 16s - loss: 2.3071 - accuracy: 0.1077\n",
            " 67/938 [=>............................] - ETA: 16s - loss: 2.3066 - accuracy: 0.1066\n",
            " 70/938 [=>............................] - ETA: 16s - loss: 2.3071 - accuracy: 0.1067\n",
            " 76/938 [=>............................] - ETA: 16s - loss: 2.3080 - accuracy: 0.1055\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 2.3079 - accuracy: 0.1046\n",
            " 88/938 [=>............................] - ETA: 15s - loss: 2.3075 - accuracy: 0.1062\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 2.3077 - accuracy: 0.1061\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 2.3076 - accuracy: 0.1066\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 2.3078 - accuracy: 0.1051\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 2.3075 - accuracy: 0.1052\n",
            "118/938 [==>...........................] - ETA: 15s - loss: 2.3073 - accuracy: 0.1042\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 2.3076 - accuracy: 0.1042\n",
            "124/938 [==>...........................] - ETA: 15s - loss: 2.3073 - accuracy: 0.1041\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 2.3076 - accuracy: 0.1040\n",
            "133/938 [===>..........................] - ETA: 15s - loss: 2.3079 - accuracy: 0.1041\n",
            "139/938 [===>..........................] - ETA: 15s - loss: 2.3073 - accuracy: 0.1045\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 2.3080 - accuracy: 0.1040\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 2.3081 - accuracy: 0.1043\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.1041\n",
            "163/938 [====>.........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.1044\n",
            "169/938 [====>.........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.1050\n",
            "175/938 [====>.........................] - ETA: 14s - loss: 2.3076 - accuracy: 0.1066\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3076 - accuracy: 0.1066\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 2.3077 - accuracy: 0.1062\n",
            "184/938 [====>.........................] - ETA: 14s - loss: 2.3078 - accuracy: 0.1057\n",
            "190/938 [=====>........................] - ETA: 14s - loss: 2.3076 - accuracy: 0.1058\n",
            "196/938 [=====>........................] - ETA: 13s - loss: 2.3078 - accuracy: 0.1056\n",
            "202/938 [=====>........................] - ETA: 13s - loss: 2.3078 - accuracy: 0.1054\n",
            "208/938 [=====>........................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1055\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 2.3078 - accuracy: 0.1058\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 2.3080 - accuracy: 0.1058\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1058\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1057\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 2.3079 - accuracy: 0.1051\n",
            "244/938 [======>.......................] - ETA: 13s - loss: 2.3080 - accuracy: 0.1047\n",
            "247/938 [======>.......................] - ETA: 12s - loss: 2.3079 - accuracy: 0.1050\n",
            "253/938 [=======>......................] - ETA: 12s - loss: 2.3077 - accuracy: 0.1047\n",
            "259/938 [=======>......................] - ETA: 12s - loss: 2.3077 - accuracy: 0.1049\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 2.3076 - accuracy: 0.1051\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 2.3077 - accuracy: 0.1055\n",
            "277/938 [=======>......................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1051\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1043\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1043\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1047\n",
            "301/938 [========>.....................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1050\n",
            "307/938 [========>.....................] - ETA: 11s - loss: 2.3081 - accuracy: 0.1044\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1046\n",
            "316/938 [=========>....................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1042\n",
            "322/938 [=========>....................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1041\n",
            "328/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1036\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1035\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1033\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1033\n",
            "352/938 [==========>...................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1033\n",
            "358/938 [==========>...................] - ETA: 10s - loss: 2.3079 - accuracy: 0.1033\n",
            "364/938 [==========>...................] - ETA: 10s - loss: 2.3081 - accuracy: 0.1029\n",
            "367/938 [==========>...................] - ETA: 10s - loss: 2.3080 - accuracy: 0.1031\n",
            "373/938 [==========>...................] - ETA: 10s - loss: 2.3079 - accuracy: 0.1031\n",
            "379/938 [===========>..................] - ETA: 10s - loss: 2.3081 - accuracy: 0.1030\n",
            "385/938 [===========>..................] - ETA: 10s - loss: 2.3081 - accuracy: 0.1027\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3080 - accuracy: 0.1027\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3080 - accuracy: 0.1027\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3080 - accuracy: 0.1027\n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1028\n",
            "415/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1025\n",
            "421/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1024\n",
            "424/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1023\n",
            "430/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1020\n",
            "433/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1019\n",
            "439/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1020\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1019\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1021\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3080 - accuracy: 0.1021\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3079 - accuracy: 0.1026\n",
            "460/938 [=============>................] - ETA: 8s - loss: 2.3079 - accuracy: 0.1025\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3079 - accuracy: 0.1029\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 2.3079 - accuracy: 0.1030\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 2.3079 - accuracy: 0.1027\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1032\n",
            "490/938 [==============>...............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1035\n",
            "496/938 [==============>...............] - ETA: 8s - loss: 2.3077 - accuracy: 0.1035\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1034\n",
            "505/938 [===============>..............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1034\n",
            "511/938 [===============>..............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1033\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3078 - accuracy: 0.1031\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3078 - accuracy: 0.1032\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1032\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1032\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3078 - accuracy: 0.1034\n",
            "547/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1035\n",
            "553/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1035\n",
            "556/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1034\n",
            "559/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1034\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1032\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3078 - accuracy: 0.1032\n",
            "571/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1030\n",
            "574/938 [=================>............] - ETA: 6s - loss: 2.3081 - accuracy: 0.1028\n",
            "580/938 [=================>............] - ETA: 6s - loss: 2.3081 - accuracy: 0.1027\n",
            "586/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1028\n",
            "592/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1027\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1028\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1029\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1032\n",
            "616/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1032\n",
            "619/938 [==================>...........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1033\n",
            "622/938 [==================>...........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1033\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1035\n",
            "628/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1034\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1035\n",
            "634/938 [===================>..........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1035\n",
            "637/938 [===================>..........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1035\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1036\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1036\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 2.3077 - accuracy: 0.1041\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1041\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1042\n",
            "673/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1043\n",
            "679/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1043\n",
            "685/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1043\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 2.3078 - accuracy: 0.1043\n",
            "691/938 [=====================>........] - ETA: 4s - loss: 2.3078 - accuracy: 0.1042\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1041\n",
            "697/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1041\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1041\n",
            "703/938 [=====================>........] - ETA: 4s - loss: 2.3078 - accuracy: 0.1044\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3078 - accuracy: 0.1044\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1042\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1043\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3080 - accuracy: 0.1045\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1045\n",
            "736/938 [======================>.......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1046\n",
            "742/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1045\n",
            "748/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1045\n",
            "754/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1046\n",
            "760/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1045\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1046\n",
            "766/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1046\n",
            "772/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1047\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1047\n",
            "778/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1048\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 2.3079 - accuracy: 0.1048\n",
            "787/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1047\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1048\n",
            "793/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1048\n",
            "799/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1046\n",
            "805/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1046\n",
            "811/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1046\n",
            "817/938 [=========================>....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1045\n",
            "823/938 [=========================>....] - ETA: 2s - loss: 2.3079 - accuracy: 0.1045\n",
            "829/938 [=========================>....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1045\n",
            "835/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1044\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3081 - accuracy: 0.1043\n",
            "844/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1042\n",
            "850/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1042\n",
            "856/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1041\n",
            "862/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1041\n",
            "868/938 [==========================>...] - ETA: 1s - loss: 2.3079 - accuracy: 0.1042\n",
            "874/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1041\n",
            "880/938 [===========================>..] - ETA: 1s - loss: 2.3080 - accuracy: 0.1040\n",
            "886/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1039\n",
            "892/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1039\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 2.3079 - accuracy: 0.1039\n",
            "901/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1039\n",
            "907/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1039\n",
            "913/938 [============================>.] - ETA: 0s - loss: 2.3079 - accuracy: 0.1041\n",
            "919/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1041\n",
            "925/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1041\n",
            "931/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1041\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1041\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3080 - accuracy: 0.1041 - val_loss: 2.3025 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 3/12\n",
            "  1/938 [..............................] - ETA: 17s - loss: 2.2834 - accuracy: 0.1875\n",
            "  7/938 [..............................] - ETA: 17s - loss: 2.3106 - accuracy: 0.1027\n",
            " 13/938 [..............................] - ETA: 17s - loss: 2.3066 - accuracy: 0.1070\n",
            " 19/938 [..............................] - ETA: 17s - loss: 2.3080 - accuracy: 0.1020\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3090 - accuracy: 0.0988\n",
            " 31/938 [..............................] - ETA: 17s - loss: 2.3080 - accuracy: 0.1028\n",
            " 37/938 [>.............................] - ETA: 17s - loss: 2.3109 - accuracy: 0.0971\n",
            " 40/938 [>.............................] - ETA: 17s - loss: 2.3106 - accuracy: 0.0961\n",
            " 43/938 [>.............................] - ETA: 17s - loss: 2.3111 - accuracy: 0.0927\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 2.3115 - accuracy: 0.0897\n",
            " 49/938 [>.............................] - ETA: 16s - loss: 2.3118 - accuracy: 0.0899\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 2.3115 - accuracy: 0.0928\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3119 - accuracy: 0.0929\n",
            " 64/938 [=>............................] - ETA: 16s - loss: 2.3113 - accuracy: 0.0940\n",
            " 70/938 [=>............................] - ETA: 16s - loss: 2.3123 - accuracy: 0.0940\n",
            " 76/938 [=>............................] - ETA: 16s - loss: 2.3118 - accuracy: 0.0952\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 2.3114 - accuracy: 0.0974\n",
            " 88/938 [=>............................] - ETA: 16s - loss: 2.3113 - accuracy: 0.0971\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 2.3108 - accuracy: 0.0977\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 2.3109 - accuracy: 0.0969\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 2.3106 - accuracy: 0.0976\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 2.3105 - accuracy: 0.0986\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 2.3107 - accuracy: 0.0984\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 2.3106 - accuracy: 0.0996\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 2.3104 - accuracy: 0.0990\n",
            "133/938 [===>..........................] - ETA: 15s - loss: 2.3101 - accuracy: 0.0989\n",
            "139/938 [===>..........................] - ETA: 15s - loss: 2.3097 - accuracy: 0.0990\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 2.3098 - accuracy: 0.0989\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 2.3092 - accuracy: 0.0993\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 2.3092 - accuracy: 0.1001\n",
            "163/938 [====>.........................] - ETA: 14s - loss: 2.3095 - accuracy: 0.1000\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3094 - accuracy: 0.1004\n",
            "169/938 [====>.........................] - ETA: 14s - loss: 2.3093 - accuracy: 0.1000\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3095 - accuracy: 0.0997\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3100 - accuracy: 0.0995\n",
            "184/938 [====>.........................] - ETA: 14s - loss: 2.3099 - accuracy: 0.0998\n",
            "190/938 [=====>........................] - ETA: 14s - loss: 2.3097 - accuracy: 0.1001\n",
            "196/938 [=====>........................] - ETA: 13s - loss: 2.3098 - accuracy: 0.1004\n",
            "202/938 [=====>........................] - ETA: 13s - loss: 2.3097 - accuracy: 0.1005\n",
            "208/938 [=====>........................] - ETA: 13s - loss: 2.3096 - accuracy: 0.1008\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 2.3095 - accuracy: 0.1011\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 2.3093 - accuracy: 0.1012\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 2.3094 - accuracy: 0.1015\n",
            "229/938 [======>.......................] - ETA: 13s - loss: 2.3096 - accuracy: 0.1014\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 2.3094 - accuracy: 0.1015\n",
            "235/938 [======>.......................] - ETA: 13s - loss: 2.3094 - accuracy: 0.1019\n",
            "241/938 [======>.......................] - ETA: 13s - loss: 2.3092 - accuracy: 0.1026\n",
            "247/938 [======>.......................] - ETA: 12s - loss: 2.3092 - accuracy: 0.1026\n",
            "253/938 [=======>......................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1030\n",
            "259/938 [=======>......................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1030\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1029\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1029\n",
            "277/938 [=======>......................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1025\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1031\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3086 - accuracy: 0.1035\n",
            "292/938 [========>.....................] - ETA: 12s - loss: 2.3087 - accuracy: 0.1035\n",
            "298/938 [========>.....................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1034\n",
            "304/938 [========>.....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1037\n",
            "310/938 [========>.....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1034\n",
            "316/938 [=========>....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1034\n",
            "322/938 [=========>....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1033\n",
            "328/938 [=========>....................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1035\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 2.3086 - accuracy: 0.1039\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3085 - accuracy: 0.1041\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3084 - accuracy: 0.1044\n",
            "349/938 [==========>...................] - ETA: 11s - loss: 2.3086 - accuracy: 0.1039\n",
            "355/938 [==========>...................] - ETA: 10s - loss: 2.3085 - accuracy: 0.1041\n",
            "361/938 [==========>...................] - ETA: 10s - loss: 2.3086 - accuracy: 0.1039\n",
            "367/938 [==========>...................] - ETA: 10s - loss: 2.3085 - accuracy: 0.1041\n",
            "373/938 [==========>...................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1039\n",
            "379/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1036\n",
            "385/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1034\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3091 - accuracy: 0.1033\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3091 - accuracy: 0.1036\n",
            "400/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1039\n",
            "406/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1041\n",
            "412/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1041\n",
            "418/938 [============>.................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1038\n",
            "424/938 [============>.................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1040\n",
            "430/938 [============>.................] - ETA: 9s - loss: 2.3090 - accuracy: 0.1034\n",
            "436/938 [============>.................] - ETA: 9s - loss: 2.3090 - accuracy: 0.1035\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1034\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3090 - accuracy: 0.1033\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1035\n",
            "460/938 [=============>................] - ETA: 8s - loss: 2.3089 - accuracy: 0.1038\n",
            "463/938 [=============>................] - ETA: 8s - loss: 2.3089 - accuracy: 0.1038\n",
            "469/938 [==============>...............] - ETA: 8s - loss: 2.3089 - accuracy: 0.1035\n",
            "475/938 [==============>...............] - ETA: 8s - loss: 2.3089 - accuracy: 0.1035\n",
            "481/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1031\n",
            "487/938 [==============>...............] - ETA: 8s - loss: 2.3087 - accuracy: 0.1033\n",
            "493/938 [==============>...............] - ETA: 8s - loss: 2.3086 - accuracy: 0.1034\n",
            "499/938 [==============>...............] - ETA: 8s - loss: 2.3087 - accuracy: 0.1033\n",
            "505/938 [===============>..............] - ETA: 8s - loss: 2.3087 - accuracy: 0.1032\n",
            "511/938 [===============>..............] - ETA: 8s - loss: 2.3086 - accuracy: 0.1033\n",
            "514/938 [===============>..............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1032\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1033\n",
            "520/938 [===============>..............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1033\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1032\n",
            "526/938 [===============>..............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1030\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1032\n",
            "532/938 [================>.............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1033\n",
            "538/938 [================>.............] - ETA: 7s - loss: 2.3086 - accuracy: 0.1038\n",
            "544/938 [================>.............] - ETA: 7s - loss: 2.3085 - accuracy: 0.1043\n",
            "550/938 [================>.............] - ETA: 7s - loss: 2.3085 - accuracy: 0.1044\n",
            "556/938 [================>.............] - ETA: 7s - loss: 2.3084 - accuracy: 0.1048\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3084 - accuracy: 0.1048\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3084 - accuracy: 0.1048\n",
            "574/938 [=================>............] - ETA: 6s - loss: 2.3085 - accuracy: 0.1045\n",
            "580/938 [=================>............] - ETA: 6s - loss: 2.3085 - accuracy: 0.1045\n",
            "586/938 [=================>............] - ETA: 6s - loss: 2.3084 - accuracy: 0.1047\n",
            "589/938 [=================>............] - ETA: 6s - loss: 2.3084 - accuracy: 0.1046\n",
            "595/938 [==================>...........] - ETA: 6s - loss: 2.3084 - accuracy: 0.1044\n",
            "601/938 [==================>...........] - ETA: 6s - loss: 2.3083 - accuracy: 0.1047\n",
            "607/938 [==================>...........] - ETA: 6s - loss: 2.3081 - accuracy: 0.1051\n",
            "613/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1051\n",
            "619/938 [==================>...........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1052\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1052\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1050\n",
            "634/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1049\n",
            "637/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1050\n",
            "640/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1049\n",
            "646/938 [===================>..........] - ETA: 5s - loss: 2.3082 - accuracy: 0.1047\n",
            "652/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1047\n",
            "658/938 [====================>.........] - ETA: 5s - loss: 2.3082 - accuracy: 0.1045\n",
            "664/938 [====================>.........] - ETA: 5s - loss: 2.3083 - accuracy: 0.1042\n",
            "670/938 [====================>.........] - ETA: 5s - loss: 2.3083 - accuracy: 0.1041\n",
            "676/938 [====================>.........] - ETA: 4s - loss: 2.3082 - accuracy: 0.1041\n",
            "682/938 [====================>.........] - ETA: 4s - loss: 2.3082 - accuracy: 0.1044\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 2.3082 - accuracy: 0.1042\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 2.3083 - accuracy: 0.1040\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3083 - accuracy: 0.1038\n",
            "703/938 [=====================>........] - ETA: 4s - loss: 2.3083 - accuracy: 0.1038\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3083 - accuracy: 0.1037\n",
            "709/938 [=====================>........] - ETA: 4s - loss: 2.3083 - accuracy: 0.1037\n",
            "715/938 [=====================>........] - ETA: 4s - loss: 2.3083 - accuracy: 0.1037\n",
            "721/938 [======================>.......] - ETA: 4s - loss: 2.3083 - accuracy: 0.1038\n",
            "727/938 [======================>.......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1039\n",
            "733/938 [======================>.......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1038\n",
            "739/938 [======================>.......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1039\n",
            "745/938 [======================>.......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1039\n",
            "751/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1039\n",
            "757/938 [=======================>......] - ETA: 3s - loss: 2.3084 - accuracy: 0.1040\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1042\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1045\n",
            "772/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1044\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1045\n",
            "778/938 [=======================>......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1045\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1047\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1048\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1049\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1050\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1051\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1053\n",
            "820/938 [=========================>....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1050\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1052\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3082 - accuracy: 0.1051\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3082 - accuracy: 0.1054\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3082 - accuracy: 0.1053\n",
            "844/938 [=========================>....] - ETA: 1s - loss: 2.3082 - accuracy: 0.1053\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3082 - accuracy: 0.1053\n",
            "850/938 [==========================>...] - ETA: 1s - loss: 2.3082 - accuracy: 0.1053\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3082 - accuracy: 0.1052\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1053\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3082 - accuracy: 0.1052\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3082 - accuracy: 0.1053\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3082 - accuracy: 0.1052\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3082 - accuracy: 0.1053\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1052\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1051\n",
            "898/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1050\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1049\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1049\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3082 - accuracy: 0.1050\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3082 - accuracy: 0.1052\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3082 - accuracy: 0.1051\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1050\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1051\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3083 - accuracy: 0.1051 - val_loss: 2.3046 - val_accuracy: 0.1010\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 4/12\n",
            "  1/938 [..............................] - ETA: 17s - loss: 2.3001 - accuracy: 0.1406\n",
            "  4/938 [..............................] - ETA: 18s - loss: 2.3079 - accuracy: 0.1016\n",
            " 10/938 [..............................] - ETA: 17s - loss: 2.3059 - accuracy: 0.1172\n",
            " 16/938 [..............................] - ETA: 17s - loss: 2.3066 - accuracy: 0.1094\n",
            " 22/938 [..............................] - ETA: 17s - loss: 2.3054 - accuracy: 0.1172\n",
            " 28/938 [..............................] - ETA: 17s - loss: 2.3048 - accuracy: 0.1116\n",
            " 34/938 [>.............................] - ETA: 16s - loss: 2.3052 - accuracy: 0.1094\n",
            " 40/938 [>.............................] - ETA: 16s - loss: 2.3060 - accuracy: 0.1086\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 2.3053 - accuracy: 0.1124\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 2.3053 - accuracy: 0.1124\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3061 - accuracy: 0.1107\n",
            " 61/938 [>.............................] - ETA: 16s - loss: 2.3066 - accuracy: 0.1089\n",
            " 67/938 [=>............................] - ETA: 16s - loss: 2.3067 - accuracy: 0.1082\n",
            " 73/938 [=>............................] - ETA: 16s - loss: 2.3060 - accuracy: 0.1085\n",
            " 79/938 [=>............................] - ETA: 16s - loss: 2.3062 - accuracy: 0.1080\n",
            " 85/938 [=>............................] - ETA: 15s - loss: 2.3069 - accuracy: 0.1064\n",
            " 91/938 [=>............................] - ETA: 15s - loss: 2.3067 - accuracy: 0.1070\n",
            " 97/938 [==>...........................] - ETA: 15s - loss: 2.3067 - accuracy: 0.1065\n",
            "103/938 [==>...........................] - ETA: 15s - loss: 2.3069 - accuracy: 0.1054\n",
            "109/938 [==>...........................] - ETA: 15s - loss: 2.3063 - accuracy: 0.1061\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 2.3060 - accuracy: 0.1073\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 2.3066 - accuracy: 0.1061\n",
            "124/938 [==>...........................] - ETA: 15s - loss: 2.3067 - accuracy: 0.1061\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 2.3067 - accuracy: 0.1070\n",
            "130/938 [===>..........................] - ETA: 15s - loss: 2.3068 - accuracy: 0.1064\n",
            "136/938 [===>..........................] - ETA: 15s - loss: 2.3077 - accuracy: 0.1051\n",
            "142/938 [===>..........................] - ETA: 14s - loss: 2.3077 - accuracy: 0.1049\n",
            "148/938 [===>..........................] - ETA: 14s - loss: 2.3075 - accuracy: 0.1050\n",
            "154/938 [===>..........................] - ETA: 14s - loss: 2.3077 - accuracy: 0.1055\n",
            "160/938 [====>.........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.1051\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.1044\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3077 - accuracy: 0.1041\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.1040\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 2.3078 - accuracy: 0.1039\n",
            "187/938 [====>.........................] - ETA: 14s - loss: 2.3074 - accuracy: 0.1044\n",
            "193/938 [=====>........................] - ETA: 13s - loss: 2.3076 - accuracy: 0.1047\n",
            "199/938 [=====>........................] - ETA: 13s - loss: 2.3074 - accuracy: 0.1035\n",
            "205/938 [=====>........................] - ETA: 13s - loss: 2.3075 - accuracy: 0.1038\n",
            "211/938 [=====>........................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1033\n",
            "217/938 [=====>........................] - ETA: 13s - loss: 2.3076 - accuracy: 0.1030\n",
            "223/938 [======>.......................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1031\n",
            "229/938 [======>.......................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1038\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 2.3076 - accuracy: 0.1043\n",
            "235/938 [======>.......................] - ETA: 13s - loss: 2.3078 - accuracy: 0.1041\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 2.3077 - accuracy: 0.1041\n",
            "244/938 [======>.......................] - ETA: 13s - loss: 2.3076 - accuracy: 0.1044\n",
            "250/938 [======>.......................] - ETA: 12s - loss: 2.3079 - accuracy: 0.1042\n",
            "256/938 [=======>......................] - ETA: 12s - loss: 2.3079 - accuracy: 0.1041\n",
            "262/938 [=======>......................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1034\n",
            "268/938 [=======>......................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1041\n",
            "274/938 [=======>......................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1041\n",
            "280/938 [=======>......................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1042\n",
            "286/938 [========>.....................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1039\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3079 - accuracy: 0.1042\n",
            "292/938 [========>.....................] - ETA: 12s - loss: 2.3079 - accuracy: 0.1046\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1050\n",
            "301/938 [========>.....................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1047\n",
            "307/938 [========>.....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1049\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 2.3077 - accuracy: 0.1053\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 2.3077 - accuracy: 0.1056\n",
            "325/938 [=========>....................] - ETA: 11s - loss: 2.3074 - accuracy: 0.1058\n",
            "331/938 [=========>....................] - ETA: 11s - loss: 2.3078 - accuracy: 0.1055\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1056\n",
            "337/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1053\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1050\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1052\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1048\n",
            "352/938 [==========>...................] - ETA: 11s - loss: 2.3083 - accuracy: 0.1043\n",
            "358/938 [==========>...................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1043\n",
            "364/938 [==========>...................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1041\n",
            "370/938 [==========>...................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1040\n",
            "376/938 [===========>..................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1042\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1045\n",
            "385/938 [===========>..................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1046\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1045\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1044\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1049\n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1051 \n",
            "415/938 [============>.................] - ETA: 9s - loss: 2.3080 - accuracy: 0.1053\n",
            "421/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1052\n",
            "427/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1050\n",
            "433/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1052\n",
            "436/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1053\n",
            "439/938 [=============>................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1052\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1054\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1053\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1053\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1051\n",
            "460/938 [=============>................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1052\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3082 - accuracy: 0.1052\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 2.3082 - accuracy: 0.1054\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 2.3080 - accuracy: 0.1056\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 2.3080 - accuracy: 0.1057\n",
            "490/938 [==============>...............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1059\n",
            "496/938 [==============>...............] - ETA: 8s - loss: 2.3079 - accuracy: 0.1057\n",
            "499/938 [==============>...............] - ETA: 8s - loss: 2.3079 - accuracy: 0.1056\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 2.3079 - accuracy: 0.1056\n",
            "508/938 [===============>..............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1057\n",
            "511/938 [===============>..............] - ETA: 8s - loss: 2.3077 - accuracy: 0.1057\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3077 - accuracy: 0.1055\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3078 - accuracy: 0.1055\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3077 - accuracy: 0.1057\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1053\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1050\n",
            "547/938 [================>.............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1049\n",
            "553/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1049\n",
            "559/938 [================>.............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1048\n",
            "565/938 [=================>............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1047\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3081 - accuracy: 0.1047\n",
            "571/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1051\n",
            "574/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1050\n",
            "577/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1049\n",
            "580/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1049\n",
            "583/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1049\n",
            "586/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1050\n",
            "589/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1050\n",
            "592/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1049\n",
            "595/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1049\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1049\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1048\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1049\n",
            "616/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1049\n",
            "622/938 [==================>...........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1048\n",
            "628/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1046\n",
            "634/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1045\n",
            "640/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1047\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1049\n",
            "646/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1048\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1048\n",
            "652/938 [===================>..........] - ETA: 5s - loss: 2.3078 - accuracy: 0.1048\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1049\n",
            "658/938 [====================>.........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1051\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1050\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1048\n",
            "673/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1050\n",
            "679/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1051\n",
            "685/938 [====================>.........] - ETA: 4s - loss: 2.3078 - accuracy: 0.1055\n",
            "691/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1054\n",
            "697/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1053\n",
            "703/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1051\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1051\n",
            "709/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1051\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1051\n",
            "715/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1054\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1056\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3081 - accuracy: 0.1054\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1054\n",
            "736/938 [======================>.......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1053\n",
            "742/938 [======================>.......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1052\n",
            "748/938 [======================>.......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1051\n",
            "754/938 [=======================>......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1051\n",
            "760/938 [=======================>......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1050\n",
            "766/938 [=======================>......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1049\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1049\n",
            "772/938 [=======================>......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1049\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1048\n",
            "778/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1048\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 2.3083 - accuracy: 0.1049\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1049\n",
            "787/938 [========================>.....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1049\n",
            "793/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1051\n",
            "799/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1052\n",
            "805/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1053\n",
            "811/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1053\n",
            "817/938 [=========================>....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1052\n",
            "823/938 [=========================>....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1052\n",
            "829/938 [=========================>....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1050\n",
            "835/938 [=========================>....] - ETA: 1s - loss: 2.3083 - accuracy: 0.1050\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3083 - accuracy: 0.1050\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3083 - accuracy: 0.1049\n",
            "844/938 [=========================>....] - ETA: 1s - loss: 2.3083 - accuracy: 0.1049\n",
            "850/938 [==========================>...] - ETA: 1s - loss: 2.3082 - accuracy: 0.1049\n",
            "856/938 [==========================>...] - ETA: 1s - loss: 2.3082 - accuracy: 0.1050\n",
            "862/938 [==========================>...] - ETA: 1s - loss: 2.3084 - accuracy: 0.1050\n",
            "868/938 [==========================>...] - ETA: 1s - loss: 2.3083 - accuracy: 0.1049\n",
            "874/938 [==========================>...] - ETA: 1s - loss: 2.3083 - accuracy: 0.1048\n",
            "880/938 [===========================>..] - ETA: 1s - loss: 2.3084 - accuracy: 0.1047\n",
            "886/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1046\n",
            "892/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1045\n",
            "898/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1046\n",
            "901/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1047\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3082 - accuracy: 0.1048\n",
            "907/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1049\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1048\n",
            "913/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1049\n",
            "919/938 [============================>.] - ETA: 0s - loss: 2.3082 - accuracy: 0.1049\n",
            "925/938 [============================>.] - ETA: 0s - loss: 2.3082 - accuracy: 0.1049\n",
            "931/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1048\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3082 - accuracy: 0.1050\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3082 - accuracy: 0.1050 - val_loss: 2.3129 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 5/12\n",
            "  4/938 [..............................] - ETA: 17s - loss: 2.3149 - accuracy: 0.1328\n",
            " 10/938 [..............................] - ETA: 17s - loss: 2.3137 - accuracy: 0.1031\n",
            " 16/938 [..............................] - ETA: 17s - loss: 2.3096 - accuracy: 0.1064\n",
            " 22/938 [..............................] - ETA: 17s - loss: 2.3069 - accuracy: 0.1065\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3096 - accuracy: 0.1063\n",
            " 31/938 [..............................] - ETA: 17s - loss: 2.3098 - accuracy: 0.1023\n",
            " 37/938 [>.............................] - ETA: 16s - loss: 2.3079 - accuracy: 0.1060\n",
            " 43/938 [>.............................] - ETA: 16s - loss: 2.3084 - accuracy: 0.1079\n",
            " 49/938 [>.............................] - ETA: 16s - loss: 2.3087 - accuracy: 0.1040\n",
            " 55/938 [>.............................] - ETA: 16s - loss: 2.3076 - accuracy: 0.1057\n",
            " 61/938 [>.............................] - ETA: 16s - loss: 2.3071 - accuracy: 0.1027\n",
            " 67/938 [=>............................] - ETA: 16s - loss: 2.3068 - accuracy: 0.1056\n",
            " 73/938 [=>............................] - ETA: 16s - loss: 2.3079 - accuracy: 0.1068\n",
            " 79/938 [=>............................] - ETA: 16s - loss: 2.3077 - accuracy: 0.1068\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 2.3075 - accuracy: 0.1077\n",
            " 88/938 [=>............................] - ETA: 15s - loss: 2.3073 - accuracy: 0.1064\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 2.3076 - accuracy: 0.1052\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 2.3078 - accuracy: 0.1042\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 2.3081 - accuracy: 0.1032\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 2.3078 - accuracy: 0.1044\n",
            "118/938 [==>...........................] - ETA: 15s - loss: 2.3081 - accuracy: 0.1038\n",
            "124/938 [==>...........................] - ETA: 15s - loss: 2.3082 - accuracy: 0.1035\n",
            "130/938 [===>..........................] - ETA: 15s - loss: 2.3080 - accuracy: 0.1031\n",
            "136/938 [===>..........................] - ETA: 15s - loss: 2.3077 - accuracy: 0.1035\n",
            "142/938 [===>..........................] - ETA: 14s - loss: 2.3079 - accuracy: 0.1024\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 2.3080 - accuracy: 0.1020\n",
            "148/938 [===>..........................] - ETA: 14s - loss: 2.3076 - accuracy: 0.1022\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 2.3076 - accuracy: 0.1020\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 2.3078 - accuracy: 0.1031\n",
            "163/938 [====>.........................] - ETA: 14s - loss: 2.3078 - accuracy: 0.1031\n",
            "169/938 [====>.........................] - ETA: 14s - loss: 2.3078 - accuracy: 0.1024\n",
            "175/938 [====>.........................] - ETA: 14s - loss: 2.3077 - accuracy: 0.1022\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 2.3077 - accuracy: 0.1026\n",
            "187/938 [====>.........................] - ETA: 14s - loss: 2.3076 - accuracy: 0.1019\n",
            "193/938 [=====>........................] - ETA: 14s - loss: 2.3077 - accuracy: 0.1014\n",
            "199/938 [=====>........................] - ETA: 13s - loss: 2.3080 - accuracy: 0.1016\n",
            "205/938 [=====>........................] - ETA: 13s - loss: 2.3078 - accuracy: 0.1021\n",
            "208/938 [=====>........................] - ETA: 13s - loss: 2.3080 - accuracy: 0.1019\n",
            "211/938 [=====>........................] - ETA: 13s - loss: 2.3083 - accuracy: 0.1015\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 2.3083 - accuracy: 0.1011\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 2.3083 - accuracy: 0.1006\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 2.3081 - accuracy: 0.1017\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 2.3081 - accuracy: 0.1018\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 2.3079 - accuracy: 0.1019\n",
            "244/938 [======>.......................] - ETA: 13s - loss: 2.3082 - accuracy: 0.1017\n",
            "250/938 [======>.......................] - ETA: 12s - loss: 2.3081 - accuracy: 0.1018\n",
            "256/938 [=======>......................] - ETA: 12s - loss: 2.3082 - accuracy: 0.1017\n",
            "259/938 [=======>......................] - ETA: 12s - loss: 2.3084 - accuracy: 0.1016\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 2.3084 - accuracy: 0.1016\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 2.3084 - accuracy: 0.1015\n",
            "277/938 [=======>......................] - ETA: 12s - loss: 2.3085 - accuracy: 0.1020\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1018\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3087 - accuracy: 0.1021\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1017\n",
            "301/938 [========>.....................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1014\n",
            "307/938 [========>.....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1013\n",
            "310/938 [========>.....................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1014\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1012\n",
            "316/938 [=========>....................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1014\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1012\n",
            "322/938 [=========>....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1009\n",
            "328/938 [=========>....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1011\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1007\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1005\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1006\n",
            "352/938 [==========>...................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1007\n",
            "358/938 [==========>...................] - ETA: 10s - loss: 2.3086 - accuracy: 0.1012\n",
            "364/938 [==========>...................] - ETA: 10s - loss: 2.3085 - accuracy: 0.1017\n",
            "370/938 [==========>...................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1016\n",
            "376/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1019\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1017\n",
            "385/938 [===========>..................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1017\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1017\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1016\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3087 - accuracy: 0.1020\n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1022 \n",
            "415/938 [============>.................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1021\n",
            "421/938 [============>.................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1022\n",
            "427/938 [============>.................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1026\n",
            "433/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1026\n",
            "439/938 [=============>................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1027\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1029\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3086 - accuracy: 0.1029\n",
            "451/938 [=============>................] - ETA: 9s - loss: 2.3086 - accuracy: 0.1030\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3085 - accuracy: 0.1029\n",
            "457/938 [=============>................] - ETA: 9s - loss: 2.3085 - accuracy: 0.1027\n",
            "460/938 [=============>................] - ETA: 8s - loss: 2.3085 - accuracy: 0.1028\n",
            "463/938 [=============>................] - ETA: 8s - loss: 2.3086 - accuracy: 0.1027\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3085 - accuracy: 0.1027\n",
            "469/938 [==============>...............] - ETA: 8s - loss: 2.3085 - accuracy: 0.1024\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 2.3086 - accuracy: 0.1023\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 2.3086 - accuracy: 0.1021\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 2.3086 - accuracy: 0.1019\n",
            "490/938 [==============>...............] - ETA: 8s - loss: 2.3086 - accuracy: 0.1020\n",
            "496/938 [==============>...............] - ETA: 8s - loss: 2.3085 - accuracy: 0.1018\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 2.3085 - accuracy: 0.1018\n",
            "508/938 [===============>..............] - ETA: 8s - loss: 2.3085 - accuracy: 0.1017\n",
            "514/938 [===============>..............] - ETA: 7s - loss: 2.3084 - accuracy: 0.1019\n",
            "520/938 [===============>..............] - ETA: 7s - loss: 2.3084 - accuracy: 0.1018\n",
            "526/938 [===============>..............] - ETA: 7s - loss: 2.3085 - accuracy: 0.1017\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3083 - accuracy: 0.1020\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3085 - accuracy: 0.1017\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3086 - accuracy: 0.1017\n",
            "547/938 [================>.............] - ETA: 7s - loss: 2.3085 - accuracy: 0.1017\n",
            "553/938 [================>.............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1015\n",
            "559/938 [================>.............] - ETA: 7s - loss: 2.3086 - accuracy: 0.1017\n",
            "565/938 [=================>............] - ETA: 7s - loss: 2.3086 - accuracy: 0.1017\n",
            "571/938 [=================>............] - ETA: 6s - loss: 2.3086 - accuracy: 0.1017\n",
            "577/938 [=================>............] - ETA: 6s - loss: 2.3085 - accuracy: 0.1018\n",
            "583/938 [=================>............] - ETA: 6s - loss: 2.3086 - accuracy: 0.1019\n",
            "589/938 [=================>............] - ETA: 6s - loss: 2.3086 - accuracy: 0.1018\n",
            "592/938 [=================>............] - ETA: 6s - loss: 2.3085 - accuracy: 0.1019\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 2.3084 - accuracy: 0.1021\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 2.3083 - accuracy: 0.1024\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 2.3084 - accuracy: 0.1025\n",
            "616/938 [==================>...........] - ETA: 6s - loss: 2.3084 - accuracy: 0.1026\n",
            "622/938 [==================>...........] - ETA: 5s - loss: 2.3085 - accuracy: 0.1024\n",
            "628/938 [===================>..........] - ETA: 5s - loss: 2.3085 - accuracy: 0.1023\n",
            "634/938 [===================>..........] - ETA: 5s - loss: 2.3084 - accuracy: 0.1024\n",
            "640/938 [===================>..........] - ETA: 5s - loss: 2.3085 - accuracy: 0.1024\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 2.3084 - accuracy: 0.1026\n",
            "646/938 [===================>..........] - ETA: 5s - loss: 2.3085 - accuracy: 0.1025\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 2.3085 - accuracy: 0.1025\n",
            "652/938 [===================>..........] - ETA: 5s - loss: 2.3084 - accuracy: 0.1025\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 2.3084 - accuracy: 0.1027\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 2.3083 - accuracy: 0.1027\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 2.3084 - accuracy: 0.1026\n",
            "673/938 [====================>.........] - ETA: 4s - loss: 2.3085 - accuracy: 0.1026\n",
            "679/938 [====================>.........] - ETA: 4s - loss: 2.3085 - accuracy: 0.1025\n",
            "685/938 [====================>.........] - ETA: 4s - loss: 2.3086 - accuracy: 0.1026\n",
            "691/938 [=====================>........] - ETA: 4s - loss: 2.3087 - accuracy: 0.1024\n",
            "697/938 [=====================>........] - ETA: 4s - loss: 2.3086 - accuracy: 0.1025\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3086 - accuracy: 0.1025\n",
            "703/938 [=====================>........] - ETA: 4s - loss: 2.3087 - accuracy: 0.1025\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3088 - accuracy: 0.1024\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3087 - accuracy: 0.1023\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3087 - accuracy: 0.1024\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3087 - accuracy: 0.1025\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 2.3088 - accuracy: 0.1024\n",
            "736/938 [======================>.......] - ETA: 3s - loss: 2.3087 - accuracy: 0.1025\n",
            "742/938 [======================>.......] - ETA: 3s - loss: 2.3087 - accuracy: 0.1025\n",
            "748/938 [======================>.......] - ETA: 3s - loss: 2.3087 - accuracy: 0.1026\n",
            "751/938 [=======================>......] - ETA: 3s - loss: 2.3087 - accuracy: 0.1028\n",
            "757/938 [=======================>......] - ETA: 3s - loss: 2.3088 - accuracy: 0.1028\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3088 - accuracy: 0.1028\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1026\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3088 - accuracy: 0.1027\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 2.3087 - accuracy: 0.1027\n",
            "787/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1026\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1026\n",
            "793/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1026\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1026\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1026\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1026\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1027\n",
            "820/938 [=========================>....] - ETA: 2s - loss: 2.3087 - accuracy: 0.1028\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 2.3087 - accuracy: 0.1030\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3088 - accuracy: 0.1029\n",
            "835/938 [=========================>....] - ETA: 1s - loss: 2.3088 - accuracy: 0.1029\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3089 - accuracy: 0.1028\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3089 - accuracy: 0.1027\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3089 - accuracy: 0.1026\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3088 - accuracy: 0.1027\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3089 - accuracy: 0.1027\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3089 - accuracy: 0.1026\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3089 - accuracy: 0.1026\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3089 - accuracy: 0.1026\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 2.3089 - accuracy: 0.1026\n",
            "892/938 [===========================>..] - ETA: 0s - loss: 2.3089 - accuracy: 0.1026\n",
            "898/938 [===========================>..] - ETA: 0s - loss: 2.3089 - accuracy: 0.1025\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3089 - accuracy: 0.1028\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3088 - accuracy: 0.1029\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3088 - accuracy: 0.1030\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3088 - accuracy: 0.1032\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3087 - accuracy: 0.1033\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3087 - accuracy: 0.1032\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3087 - accuracy: 0.1031\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3087 - accuracy: 0.1032 - val_loss: 2.3074 - val_accuracy: 0.0980\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 6/12\n",
            "  4/938 [..............................] - ETA: 17s - loss: 2.3100 - accuracy: 0.0703\n",
            " 10/938 [..............................] - ETA: 17s - loss: 2.3068 - accuracy: 0.0984\n",
            " 16/938 [..............................] - ETA: 17s - loss: 2.3063 - accuracy: 0.1045\n",
            " 22/938 [..............................] - ETA: 16s - loss: 2.3063 - accuracy: 0.1094\n",
            " 28/938 [..............................] - ETA: 16s - loss: 2.3068 - accuracy: 0.1077\n",
            " 34/938 [>.............................] - ETA: 16s - loss: 2.3069 - accuracy: 0.1089\n",
            " 40/938 [>.............................] - ETA: 16s - loss: 2.3085 - accuracy: 0.1055\n",
            " 43/938 [>.............................] - ETA: 16s - loss: 2.3086 - accuracy: 0.1068\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 2.3087 - accuracy: 0.1039\n",
            " 49/938 [>.............................] - ETA: 16s - loss: 2.3087 - accuracy: 0.1030\n",
            " 55/938 [>.............................] - ETA: 16s - loss: 2.3085 - accuracy: 0.1060\n",
            " 61/938 [>.............................] - ETA: 16s - loss: 2.3071 - accuracy: 0.1071\n",
            " 67/938 [=>............................] - ETA: 16s - loss: 2.3077 - accuracy: 0.1073\n",
            " 73/938 [=>............................] - ETA: 16s - loss: 2.3081 - accuracy: 0.1089\n",
            " 79/938 [=>............................] - ETA: 16s - loss: 2.3082 - accuracy: 0.1084\n",
            " 85/938 [=>............................] - ETA: 16s - loss: 2.3081 - accuracy: 0.1081\n",
            " 91/938 [=>............................] - ETA: 16s - loss: 2.3086 - accuracy: 0.1058\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 2.3084 - accuracy: 0.1056\n",
            " 97/938 [==>...........................] - ETA: 15s - loss: 2.3086 - accuracy: 0.1052\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 2.3080 - accuracy: 0.1059\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 2.3091 - accuracy: 0.1048\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 2.3094 - accuracy: 0.1032\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 2.3093 - accuracy: 0.1029\n",
            "118/938 [==>...........................] - ETA: 15s - loss: 2.3092 - accuracy: 0.1017\n",
            "124/938 [==>...........................] - ETA: 15s - loss: 2.3099 - accuracy: 0.1002\n",
            "130/938 [===>..........................] - ETA: 15s - loss: 2.3100 - accuracy: 0.1005\n",
            "136/938 [===>..........................] - ETA: 15s - loss: 2.3098 - accuracy: 0.1009\n",
            "142/938 [===>..........................] - ETA: 15s - loss: 2.3100 - accuracy: 0.0999\n",
            "148/938 [===>..........................] - ETA: 14s - loss: 2.3099 - accuracy: 0.0992\n",
            "154/938 [===>..........................] - ETA: 14s - loss: 2.3097 - accuracy: 0.0996\n",
            "160/938 [====>.........................] - ETA: 14s - loss: 2.3095 - accuracy: 0.0997\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3096 - accuracy: 0.0988\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3094 - accuracy: 0.0985\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3091 - accuracy: 0.0991\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 2.3091 - accuracy: 0.0991\n",
            "184/938 [====>.........................] - ETA: 14s - loss: 2.3091 - accuracy: 0.0992\n",
            "187/938 [====>.........................] - ETA: 14s - loss: 2.3089 - accuracy: 0.0997\n",
            "193/938 [=====>........................] - ETA: 14s - loss: 2.3089 - accuracy: 0.1001\n",
            "199/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.1009\n",
            "205/938 [=====>........................] - ETA: 13s - loss: 2.3086 - accuracy: 0.1016\n",
            "211/938 [=====>........................] - ETA: 13s - loss: 2.3085 - accuracy: 0.1025\n",
            "217/938 [=====>........................] - ETA: 13s - loss: 2.3088 - accuracy: 0.1023\n",
            "223/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1020\n",
            "229/938 [======>.......................] - ETA: 13s - loss: 2.3088 - accuracy: 0.1017\n",
            "235/938 [======>.......................] - ETA: 13s - loss: 2.3087 - accuracy: 0.1021\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 2.3086 - accuracy: 0.1021\n",
            "244/938 [======>.......................] - ETA: 13s - loss: 2.3086 - accuracy: 0.1021\n",
            "250/938 [======>.......................] - ETA: 12s - loss: 2.3086 - accuracy: 0.1028\n",
            "256/938 [=======>......................] - ETA: 12s - loss: 2.3084 - accuracy: 0.1030\n",
            "262/938 [=======>......................] - ETA: 12s - loss: 2.3082 - accuracy: 0.1027\n",
            "268/938 [=======>......................] - ETA: 12s - loss: 2.3082 - accuracy: 0.1029\n",
            "274/938 [=======>......................] - ETA: 12s - loss: 2.3081 - accuracy: 0.1034\n",
            "280/938 [=======>......................] - ETA: 12s - loss: 2.3080 - accuracy: 0.1041\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1047\n",
            "286/938 [========>.....................] - ETA: 12s - loss: 2.3077 - accuracy: 0.1049\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3077 - accuracy: 0.1050\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 2.3076 - accuracy: 0.1054\n",
            "301/938 [========>.....................] - ETA: 12s - loss: 2.3075 - accuracy: 0.1061\n",
            "307/938 [========>.....................] - ETA: 11s - loss: 2.3073 - accuracy: 0.1068\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 2.3074 - accuracy: 0.1071\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 2.3073 - accuracy: 0.1074\n",
            "325/938 [=========>....................] - ETA: 11s - loss: 2.3073 - accuracy: 0.1070\n",
            "331/938 [=========>....................] - ETA: 11s - loss: 2.3073 - accuracy: 0.1074\n",
            "337/938 [=========>....................] - ETA: 11s - loss: 2.3075 - accuracy: 0.1071\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3075 - accuracy: 0.1069\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3075 - accuracy: 0.1069\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 2.3075 - accuracy: 0.1069\n",
            "352/938 [==========>...................] - ETA: 11s - loss: 2.3074 - accuracy: 0.1070\n",
            "358/938 [==========>...................] - ETA: 10s - loss: 2.3075 - accuracy: 0.1070\n",
            "364/938 [==========>...................] - ETA: 10s - loss: 2.3074 - accuracy: 0.1071\n",
            "370/938 [==========>...................] - ETA: 10s - loss: 2.3075 - accuracy: 0.1071\n",
            "376/938 [===========>..................] - ETA: 10s - loss: 2.3077 - accuracy: 0.1067\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 2.3076 - accuracy: 0.1068\n",
            "388/938 [===========>..................] - ETA: 10s - loss: 2.3077 - accuracy: 0.1064\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3077 - accuracy: 0.1064\n",
            "394/938 [===========>..................] - ETA: 10s - loss: 2.3077 - accuracy: 0.1065\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3077 - accuracy: 0.1065\n",
            "400/938 [===========>..................] - ETA: 10s - loss: 2.3077 - accuracy: 0.1066\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3077 - accuracy: 0.1067\n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3078 - accuracy: 0.1069 \n",
            "415/938 [============>.................] - ETA: 9s - loss: 2.3078 - accuracy: 0.1066\n",
            "421/938 [============>.................] - ETA: 9s - loss: 2.3079 - accuracy: 0.1065\n",
            "427/938 [============>.................] - ETA: 9s - loss: 2.3079 - accuracy: 0.1066\n",
            "433/938 [============>.................] - ETA: 9s - loss: 2.3080 - accuracy: 0.1062\n",
            "439/938 [=============>................] - ETA: 9s - loss: 2.3079 - accuracy: 0.1063\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3079 - accuracy: 0.1063\n",
            "451/938 [=============>................] - ETA: 9s - loss: 2.3077 - accuracy: 0.1063\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3077 - accuracy: 0.1064\n",
            "457/938 [=============>................] - ETA: 9s - loss: 2.3077 - accuracy: 0.1065\n",
            "463/938 [=============>................] - ETA: 8s - loss: 2.3077 - accuracy: 0.1065\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3077 - accuracy: 0.1067\n",
            "469/938 [==============>...............] - ETA: 8s - loss: 2.3076 - accuracy: 0.1070\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 2.3075 - accuracy: 0.1073\n",
            "475/938 [==============>...............] - ETA: 8s - loss: 2.3076 - accuracy: 0.1072\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 2.3077 - accuracy: 0.1070\n",
            "481/938 [==============>...............] - ETA: 8s - loss: 2.3077 - accuracy: 0.1068\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1066\n",
            "490/938 [==============>...............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1064\n",
            "496/938 [==============>...............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1066\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 2.3078 - accuracy: 0.1066\n",
            "508/938 [===============>..............] - ETA: 8s - loss: 2.3079 - accuracy: 0.1065\n",
            "514/938 [===============>..............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1066\n",
            "520/938 [===============>..............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1066\n",
            "526/938 [===============>..............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1065\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1063\n",
            "532/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1066\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1066\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3078 - accuracy: 0.1066\n",
            "547/938 [================>.............] - ETA: 7s - loss: 2.3078 - accuracy: 0.1067\n",
            "553/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1065\n",
            "559/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1065\n",
            "565/938 [=================>............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1064\n",
            "571/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1062\n",
            "577/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1065\n",
            "583/938 [=================>............] - ETA: 6s - loss: 2.3081 - accuracy: 0.1062\n",
            "589/938 [=================>............] - ETA: 6s - loss: 2.3081 - accuracy: 0.1061\n",
            "595/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1062\n",
            "601/938 [==================>...........] - ETA: 6s - loss: 2.3082 - accuracy: 0.1063\n",
            "607/938 [==================>...........] - ETA: 6s - loss: 2.3081 - accuracy: 0.1063\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 2.3081 - accuracy: 0.1064\n",
            "616/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1067\n",
            "622/938 [==================>...........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1067\n",
            "628/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1069\n",
            "634/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1067\n",
            "640/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1066\n",
            "646/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1067\n",
            "652/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1064\n",
            "658/938 [====================>.........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1064\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1065\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1066\n",
            "673/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1064\n",
            "679/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1066\n",
            "685/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1070\n",
            "691/938 [=====================>........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1070\n",
            "697/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1069\n",
            "703/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1066\n",
            "709/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1066\n",
            "715/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1065\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1065\n",
            "721/938 [======================>.......] - ETA: 4s - loss: 2.3081 - accuracy: 0.1065\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3080 - accuracy: 0.1067\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1065\n",
            "733/938 [======================>.......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1063\n",
            "736/938 [======================>.......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1063\n",
            "739/938 [======================>.......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1063\n",
            "742/938 [======================>.......] - ETA: 3s - loss: 2.3082 - accuracy: 0.1062\n",
            "745/938 [======================>.......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1062\n",
            "748/938 [======================>.......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1060\n",
            "754/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1058\n",
            "760/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1057\n",
            "766/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1054\n",
            "772/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1053\n",
            "778/938 [=======================>......] - ETA: 3s - loss: 2.3083 - accuracy: 0.1053\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1053\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1053\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1054\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1052\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1052\n",
            "811/938 [========================>.....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1052\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1051\n",
            "817/938 [=========================>....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1050\n",
            "823/938 [=========================>....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1049\n",
            "829/938 [=========================>....] - ETA: 2s - loss: 2.3083 - accuracy: 0.1048\n",
            "835/938 [=========================>....] - ETA: 1s - loss: 2.3083 - accuracy: 0.1048\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3083 - accuracy: 0.1046\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3084 - accuracy: 0.1046\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3083 - accuracy: 0.1046\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3083 - accuracy: 0.1046\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3083 - accuracy: 0.1047\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3083 - accuracy: 0.1048\n",
            "874/938 [==========================>...] - ETA: 1s - loss: 2.3083 - accuracy: 0.1047\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3083 - accuracy: 0.1047\n",
            "880/938 [===========================>..] - ETA: 1s - loss: 2.3083 - accuracy: 0.1047\n",
            "886/938 [===========================>..] - ETA: 0s - loss: 2.3083 - accuracy: 0.1046\n",
            "892/938 [===========================>..] - ETA: 0s - loss: 2.3084 - accuracy: 0.1048\n",
            "898/938 [===========================>..] - ETA: 0s - loss: 2.3084 - accuracy: 0.1048\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3084 - accuracy: 0.1048\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1048\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3082 - accuracy: 0.1050\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1049\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1047\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1048\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3083 - accuracy: 0.1049\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3083 - accuracy: 0.1049 - val_loss: 2.3152 - val_accuracy: 0.1028\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 7/12\n",
            "  4/938 [..............................] - ETA: 17s - loss: 2.3290 - accuracy: 0.0781\n",
            " 10/938 [..............................] - ETA: 17s - loss: 2.3205 - accuracy: 0.0844\n",
            " 16/938 [..............................] - ETA: 17s - loss: 2.3121 - accuracy: 0.0986\n",
            " 22/938 [..............................] - ETA: 16s - loss: 2.3145 - accuracy: 0.0980\n",
            " 28/938 [..............................] - ETA: 16s - loss: 2.3158 - accuracy: 0.1010\n",
            " 34/938 [>.............................] - ETA: 16s - loss: 2.3149 - accuracy: 0.1016\n",
            " 40/938 [>.............................] - ETA: 16s - loss: 2.3133 - accuracy: 0.0992\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 2.3130 - accuracy: 0.0982\n",
            " 49/938 [>.............................] - ETA: 16s - loss: 2.3133 - accuracy: 0.0973\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 2.3139 - accuracy: 0.0959\n",
            " 55/938 [>.............................] - ETA: 16s - loss: 2.3136 - accuracy: 0.0952\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3140 - accuracy: 0.0951\n",
            " 61/938 [>.............................] - ETA: 16s - loss: 2.3139 - accuracy: 0.0927\n",
            " 67/938 [=>............................] - ETA: 16s - loss: 2.3135 - accuracy: 0.0917\n",
            " 73/938 [=>............................] - ETA: 16s - loss: 2.3132 - accuracy: 0.0923\n",
            " 79/938 [=>............................] - ETA: 16s - loss: 2.3126 - accuracy: 0.0928\n",
            " 85/938 [=>............................] - ETA: 15s - loss: 2.3130 - accuracy: 0.0925\n",
            " 91/938 [=>............................] - ETA: 15s - loss: 2.3124 - accuracy: 0.0948\n",
            " 97/938 [==>...........................] - ETA: 15s - loss: 2.3114 - accuracy: 0.0970\n",
            "103/938 [==>...........................] - ETA: 15s - loss: 2.3116 - accuracy: 0.0978\n",
            "109/938 [==>...........................] - ETA: 15s - loss: 2.3115 - accuracy: 0.0983\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 2.3116 - accuracy: 0.0974\n",
            "118/938 [==>...........................] - ETA: 15s - loss: 2.3111 - accuracy: 0.0969\n",
            "124/938 [==>...........................] - ETA: 15s - loss: 2.3112 - accuracy: 0.0974\n",
            "130/938 [===>..........................] - ETA: 15s - loss: 2.3105 - accuracy: 0.0972\n",
            "136/938 [===>..........................] - ETA: 15s - loss: 2.3105 - accuracy: 0.0974\n",
            "142/938 [===>..........................] - ETA: 14s - loss: 2.3099 - accuracy: 0.0980\n",
            "148/938 [===>..........................] - ETA: 14s - loss: 2.3098 - accuracy: 0.0996\n",
            "154/938 [===>..........................] - ETA: 14s - loss: 2.3098 - accuracy: 0.0993\n",
            "160/938 [====>.........................] - ETA: 14s - loss: 2.3100 - accuracy: 0.0991\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3099 - accuracy: 0.0986\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3098 - accuracy: 0.0980\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3099 - accuracy: 0.0979\n",
            "184/938 [====>.........................] - ETA: 14s - loss: 2.3098 - accuracy: 0.0979\n",
            "187/938 [====>.........................] - ETA: 14s - loss: 2.3095 - accuracy: 0.0978\n",
            "190/938 [=====>........................] - ETA: 13s - loss: 2.3094 - accuracy: 0.0979\n",
            "193/938 [=====>........................] - ETA: 13s - loss: 2.3093 - accuracy: 0.0980\n",
            "199/938 [=====>........................] - ETA: 13s - loss: 2.3091 - accuracy: 0.0994\n",
            "205/938 [=====>........................] - ETA: 13s - loss: 2.3090 - accuracy: 0.0993\n",
            "211/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.0995\n",
            "217/938 [=====>........................] - ETA: 13s - loss: 2.3095 - accuracy: 0.0986\n",
            "223/938 [======>.......................] - ETA: 13s - loss: 2.3096 - accuracy: 0.0988\n",
            "229/938 [======>.......................] - ETA: 13s - loss: 2.3095 - accuracy: 0.0983\n",
            "235/938 [======>.......................] - ETA: 13s - loss: 2.3093 - accuracy: 0.0986\n",
            "241/938 [======>.......................] - ETA: 13s - loss: 2.3094 - accuracy: 0.0984\n",
            "244/938 [======>.......................] - ETA: 12s - loss: 2.3093 - accuracy: 0.0984\n",
            "247/938 [======>.......................] - ETA: 12s - loss: 2.3093 - accuracy: 0.0986\n",
            "250/938 [======>.......................] - ETA: 12s - loss: 2.3093 - accuracy: 0.0986\n",
            "256/938 [=======>......................] - ETA: 12s - loss: 2.3095 - accuracy: 0.0994\n",
            "262/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1000\n",
            "268/938 [=======>......................] - ETA: 12s - loss: 2.3096 - accuracy: 0.0996\n",
            "274/938 [=======>......................] - ETA: 12s - loss: 2.3096 - accuracy: 0.0995\n",
            "280/938 [=======>......................] - ETA: 12s - loss: 2.3099 - accuracy: 0.0992\n",
            "286/938 [========>.....................] - ETA: 12s - loss: 2.3100 - accuracy: 0.0988\n",
            "292/938 [========>.....................] - ETA: 12s - loss: 2.3102 - accuracy: 0.0988\n",
            "298/938 [========>.....................] - ETA: 11s - loss: 2.3103 - accuracy: 0.0986\n",
            "304/938 [========>.....................] - ETA: 11s - loss: 2.3104 - accuracy: 0.0985\n",
            "310/938 [========>.....................] - ETA: 11s - loss: 2.3104 - accuracy: 0.0986\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 2.3104 - accuracy: 0.0985\n",
            "316/938 [=========>....................] - ETA: 11s - loss: 2.3104 - accuracy: 0.0989\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 2.3103 - accuracy: 0.0991\n",
            "322/938 [=========>....................] - ETA: 11s - loss: 2.3103 - accuracy: 0.0993\n",
            "325/938 [=========>....................] - ETA: 11s - loss: 2.3103 - accuracy: 0.0992\n",
            "331/938 [=========>....................] - ETA: 11s - loss: 2.3103 - accuracy: 0.0988\n",
            "337/938 [=========>....................] - ETA: 11s - loss: 2.3103 - accuracy: 0.0986\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3103 - accuracy: 0.0983\n",
            "349/938 [==========>...................] - ETA: 11s - loss: 2.3102 - accuracy: 0.0982\n",
            "355/938 [==========>...................] - ETA: 10s - loss: 2.3103 - accuracy: 0.0979\n",
            "361/938 [==========>...................] - ETA: 10s - loss: 2.3104 - accuracy: 0.0979\n",
            "367/938 [==========>...................] - ETA: 10s - loss: 2.3104 - accuracy: 0.0979\n",
            "373/938 [==========>...................] - ETA: 10s - loss: 2.3106 - accuracy: 0.0978\n",
            "379/938 [===========>..................] - ETA: 10s - loss: 2.3105 - accuracy: 0.0977\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 2.3105 - accuracy: 0.0975\n",
            "385/938 [===========>..................] - ETA: 10s - loss: 2.3105 - accuracy: 0.0974\n",
            "388/938 [===========>..................] - ETA: 10s - loss: 2.3106 - accuracy: 0.0973\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3106 - accuracy: 0.0973\n",
            "394/938 [===========>..................] - ETA: 10s - loss: 2.3106 - accuracy: 0.0975\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3105 - accuracy: 0.0974\n",
            "400/938 [===========>..................] - ETA: 10s - loss: 2.3105 - accuracy: 0.0973\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3105 - accuracy: 0.0972\n",
            "406/938 [===========>..................] - ETA: 9s - loss: 2.3104 - accuracy: 0.0973 \n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3104 - accuracy: 0.0974\n",
            "412/938 [============>.................] - ETA: 9s - loss: 2.3103 - accuracy: 0.0979\n",
            "418/938 [============>.................] - ETA: 9s - loss: 2.3103 - accuracy: 0.0980\n",
            "424/938 [============>.................] - ETA: 9s - loss: 2.3102 - accuracy: 0.0982\n",
            "430/938 [============>.................] - ETA: 9s - loss: 2.3102 - accuracy: 0.0981\n",
            "436/938 [============>.................] - ETA: 9s - loss: 2.3101 - accuracy: 0.0978\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3100 - accuracy: 0.0977\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3100 - accuracy: 0.0979\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3100 - accuracy: 0.0985\n",
            "457/938 [=============>................] - ETA: 9s - loss: 2.3100 - accuracy: 0.0984\n",
            "460/938 [=============>................] - ETA: 8s - loss: 2.3100 - accuracy: 0.0984\n",
            "463/938 [=============>................] - ETA: 8s - loss: 2.3100 - accuracy: 0.0985\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3100 - accuracy: 0.0985\n",
            "469/938 [==============>...............] - ETA: 8s - loss: 2.3101 - accuracy: 0.0985\n",
            "475/938 [==============>...............] - ETA: 8s - loss: 2.3101 - accuracy: 0.0984\n",
            "481/938 [==============>...............] - ETA: 8s - loss: 2.3100 - accuracy: 0.0985\n",
            "487/938 [==============>...............] - ETA: 8s - loss: 2.3101 - accuracy: 0.0984\n",
            "493/938 [==============>...............] - ETA: 8s - loss: 2.3100 - accuracy: 0.0983\n",
            "499/938 [==============>...............] - ETA: 8s - loss: 2.3100 - accuracy: 0.0984\n",
            "505/938 [===============>..............] - ETA: 8s - loss: 2.3100 - accuracy: 0.0987\n",
            "511/938 [===============>..............] - ETA: 7s - loss: 2.3099 - accuracy: 0.0988\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3100 - accuracy: 0.0985\n",
            "520/938 [===============>..............] - ETA: 7s - loss: 2.3100 - accuracy: 0.0986\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3100 - accuracy: 0.0986\n",
            "526/938 [===============>..............] - ETA: 7s - loss: 2.3100 - accuracy: 0.0987\n",
            "532/938 [================>.............] - ETA: 7s - loss: 2.3099 - accuracy: 0.0992\n",
            "538/938 [================>.............] - ETA: 7s - loss: 2.3098 - accuracy: 0.0991\n",
            "544/938 [================>.............] - ETA: 7s - loss: 2.3099 - accuracy: 0.0992\n",
            "550/938 [================>.............] - ETA: 7s - loss: 2.3099 - accuracy: 0.0992\n",
            "556/938 [================>.............] - ETA: 7s - loss: 2.3099 - accuracy: 0.0993\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3098 - accuracy: 0.0989\n",
            "565/938 [=================>............] - ETA: 6s - loss: 2.3098 - accuracy: 0.0990\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3098 - accuracy: 0.0990\n",
            "571/938 [=================>............] - ETA: 6s - loss: 2.3097 - accuracy: 0.0992\n",
            "577/938 [=================>............] - ETA: 6s - loss: 2.3097 - accuracy: 0.0992\n",
            "583/938 [=================>............] - ETA: 6s - loss: 2.3098 - accuracy: 0.0993\n",
            "589/938 [=================>............] - ETA: 6s - loss: 2.3097 - accuracy: 0.0993\n",
            "595/938 [==================>...........] - ETA: 6s - loss: 2.3097 - accuracy: 0.0994\n",
            "601/938 [==================>...........] - ETA: 6s - loss: 2.3097 - accuracy: 0.0995\n",
            "607/938 [==================>...........] - ETA: 6s - loss: 2.3098 - accuracy: 0.0997\n",
            "613/938 [==================>...........] - ETA: 6s - loss: 2.3098 - accuracy: 0.0998\n",
            "616/938 [==================>...........] - ETA: 6s - loss: 2.3097 - accuracy: 0.1000\n",
            "619/938 [==================>...........] - ETA: 5s - loss: 2.3097 - accuracy: 0.1001\n",
            "622/938 [==================>...........] - ETA: 5s - loss: 2.3097 - accuracy: 0.1001\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3097 - accuracy: 0.1001\n",
            "628/938 [===================>..........] - ETA: 5s - loss: 2.3096 - accuracy: 0.1003\n",
            "634/938 [===================>..........] - ETA: 5s - loss: 2.3095 - accuracy: 0.1002\n",
            "640/938 [===================>..........] - ETA: 5s - loss: 2.3098 - accuracy: 0.1003\n",
            "646/938 [===================>..........] - ETA: 5s - loss: 2.3098 - accuracy: 0.1005\n",
            "652/938 [===================>..........] - ETA: 5s - loss: 2.3097 - accuracy: 0.1007\n",
            "658/938 [====================>.........] - ETA: 5s - loss: 2.3097 - accuracy: 0.1005\n",
            "664/938 [====================>.........] - ETA: 5s - loss: 2.3096 - accuracy: 0.1008\n",
            "670/938 [====================>.........] - ETA: 5s - loss: 2.3096 - accuracy: 0.1011\n",
            "676/938 [====================>.........] - ETA: 4s - loss: 2.3095 - accuracy: 0.1011\n",
            "682/938 [====================>.........] - ETA: 4s - loss: 2.3095 - accuracy: 0.1012\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 2.3095 - accuracy: 0.1014\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 2.3095 - accuracy: 0.1014\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3094 - accuracy: 0.1015\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3094 - accuracy: 0.1015\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3094 - accuracy: 0.1012\n",
            "715/938 [=====================>........] - ETA: 4s - loss: 2.3094 - accuracy: 0.1013\n",
            "721/938 [======================>.......] - ETA: 4s - loss: 2.3094 - accuracy: 0.1012\n",
            "727/938 [======================>.......] - ETA: 3s - loss: 2.3094 - accuracy: 0.1013\n",
            "733/938 [======================>.......] - ETA: 3s - loss: 2.3095 - accuracy: 0.1011\n",
            "739/938 [======================>.......] - ETA: 3s - loss: 2.3095 - accuracy: 0.1010\n",
            "745/938 [======================>.......] - ETA: 3s - loss: 2.3096 - accuracy: 0.1009\n",
            "751/938 [=======================>......] - ETA: 3s - loss: 2.3095 - accuracy: 0.1012\n",
            "757/938 [=======================>......] - ETA: 3s - loss: 2.3094 - accuracy: 0.1013\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3095 - accuracy: 0.1012\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3094 - accuracy: 0.1012\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3095 - accuracy: 0.1010\n",
            "778/938 [=======================>......] - ETA: 2s - loss: 2.3095 - accuracy: 0.1009\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 2.3095 - accuracy: 0.1007\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3095 - accuracy: 0.1009\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3095 - accuracy: 0.1008\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3096 - accuracy: 0.1009\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3096 - accuracy: 0.1008\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3097 - accuracy: 0.1008\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3097 - accuracy: 0.1008\n",
            "820/938 [=========================>....] - ETA: 2s - loss: 2.3097 - accuracy: 0.1008\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 2.3098 - accuracy: 0.1009\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3098 - accuracy: 0.1009\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3098 - accuracy: 0.1009\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3098 - accuracy: 0.1008\n",
            "844/938 [=========================>....] - ETA: 1s - loss: 2.3099 - accuracy: 0.1007\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3098 - accuracy: 0.1007\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3098 - accuracy: 0.1005\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3098 - accuracy: 0.1005\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3098 - accuracy: 0.1005\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3099 - accuracy: 0.1004\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3099 - accuracy: 0.1005\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3098 - accuracy: 0.1008\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 2.3099 - accuracy: 0.1008\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 2.3099 - accuracy: 0.1008\n",
            "901/938 [===========================>..] - ETA: 0s - loss: 2.3098 - accuracy: 0.1009\n",
            "907/938 [============================>.] - ETA: 0s - loss: 2.3099 - accuracy: 0.1010\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3099 - accuracy: 0.1011\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3099 - accuracy: 0.1011\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3099 - accuracy: 0.1011\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3099 - accuracy: 0.1012\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3098 - accuracy: 0.1013\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3098 - accuracy: 0.1013\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3098 - accuracy: 0.1013 - val_loss: 2.3087 - val_accuracy: 0.1010\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 8/12\n",
            "  1/938 [..............................] - ETA: 17s - loss: 2.3045 - accuracy: 0.1562\n",
            "  7/938 [..............................] - ETA: 17s - loss: 2.3157 - accuracy: 0.0960\n",
            " 13/938 [..............................] - ETA: 17s - loss: 2.3052 - accuracy: 0.1070\n",
            " 19/938 [..............................] - ETA: 17s - loss: 2.3068 - accuracy: 0.1110\n",
            " 22/938 [..............................] - ETA: 17s - loss: 2.3066 - accuracy: 0.1122\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3074 - accuracy: 0.1088\n",
            " 28/938 [..............................] - ETA: 17s - loss: 2.3060 - accuracy: 0.1122\n",
            " 34/938 [>.............................] - ETA: 17s - loss: 2.3059 - accuracy: 0.1117\n",
            " 40/938 [>.............................] - ETA: 16s - loss: 2.3059 - accuracy: 0.1109\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 2.3059 - accuracy: 0.1080\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 2.3073 - accuracy: 0.1055\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3075 - accuracy: 0.1053\n",
            " 64/938 [=>............................] - ETA: 16s - loss: 2.3074 - accuracy: 0.1050\n",
            " 70/938 [=>............................] - ETA: 16s - loss: 2.3073 - accuracy: 0.1045\n",
            " 76/938 [=>............................] - ETA: 16s - loss: 2.3057 - accuracy: 0.1057\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 2.3065 - accuracy: 0.1063\n",
            " 88/938 [=>............................] - ETA: 15s - loss: 2.3063 - accuracy: 0.1085\n",
            " 91/938 [=>............................] - ETA: 15s - loss: 2.3066 - accuracy: 0.1090\n",
            " 97/938 [==>...........................] - ETA: 15s - loss: 2.3065 - accuracy: 0.1087\n",
            "103/938 [==>...........................] - ETA: 15s - loss: 2.3069 - accuracy: 0.1083\n",
            "109/938 [==>...........................] - ETA: 15s - loss: 2.3069 - accuracy: 0.1077\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 2.3074 - accuracy: 0.1072\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 2.3072 - accuracy: 0.1085\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 2.3066 - accuracy: 0.1096\n",
            "133/938 [===>..........................] - ETA: 15s - loss: 2.3069 - accuracy: 0.1101\n",
            "139/938 [===>..........................] - ETA: 14s - loss: 2.3068 - accuracy: 0.1104\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 2.3068 - accuracy: 0.1099\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 2.3066 - accuracy: 0.1103\n",
            "154/938 [===>..........................] - ETA: 14s - loss: 2.3066 - accuracy: 0.1100\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 2.3067 - accuracy: 0.1096\n",
            "160/938 [====>.........................] - ETA: 14s - loss: 2.3068 - accuracy: 0.1099\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3069 - accuracy: 0.1098\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3072 - accuracy: 0.1092\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3073 - accuracy: 0.1088\n",
            "184/938 [====>.........................] - ETA: 14s - loss: 2.3071 - accuracy: 0.1090\n",
            "190/938 [=====>........................] - ETA: 14s - loss: 2.3074 - accuracy: 0.1089\n",
            "196/938 [=====>........................] - ETA: 13s - loss: 2.3079 - accuracy: 0.1083\n",
            "202/938 [=====>........................] - ETA: 13s - loss: 2.3079 - accuracy: 0.1084\n",
            "208/938 [=====>........................] - ETA: 13s - loss: 2.3079 - accuracy: 0.1093\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 2.3081 - accuracy: 0.1094\n",
            "217/938 [=====>........................] - ETA: 13s - loss: 2.3081 - accuracy: 0.1092\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 2.3080 - accuracy: 0.1092\n",
            "223/938 [======>.......................] - ETA: 13s - loss: 2.3081 - accuracy: 0.1090\n",
            "229/938 [======>.......................] - ETA: 13s - loss: 2.3082 - accuracy: 0.1086\n",
            "235/938 [======>.......................] - ETA: 13s - loss: 2.3081 - accuracy: 0.1087\n",
            "241/938 [======>.......................] - ETA: 13s - loss: 2.3082 - accuracy: 0.1085\n",
            "247/938 [======>.......................] - ETA: 12s - loss: 2.3086 - accuracy: 0.1079\n",
            "253/938 [=======>......................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1074\n",
            "259/938 [=======>......................] - ETA: 12s - loss: 2.3091 - accuracy: 0.1071\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1074\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 2.3091 - accuracy: 0.1071\n",
            "274/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1071\n",
            "280/938 [=======>......................] - ETA: 12s - loss: 2.3092 - accuracy: 0.1070\n",
            "286/938 [========>.....................] - ETA: 12s - loss: 2.3092 - accuracy: 0.1064\n",
            "292/938 [========>.....................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1067\n",
            "298/938 [========>.....................] - ETA: 12s - loss: 2.3092 - accuracy: 0.1063\n",
            "304/938 [========>.....................] - ETA: 11s - loss: 2.3091 - accuracy: 0.1064\n",
            "310/938 [========>.....................] - ETA: 11s - loss: 2.3092 - accuracy: 0.1060\n",
            "316/938 [=========>....................] - ETA: 11s - loss: 2.3091 - accuracy: 0.1057\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 2.3091 - accuracy: 0.1056\n",
            "325/938 [=========>....................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1057\n",
            "331/938 [=========>....................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1056\n",
            "337/938 [=========>....................] - ETA: 11s - loss: 2.3091 - accuracy: 0.1052\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1049\n",
            "349/938 [==========>...................] - ETA: 11s - loss: 2.3086 - accuracy: 0.1054\n",
            "355/938 [==========>...................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1055\n",
            "361/938 [==========>...................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1057\n",
            "367/938 [==========>...................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1055\n",
            "373/938 [==========>...................] - ETA: 10s - loss: 2.3091 - accuracy: 0.1052\n",
            "379/938 [===========>..................] - ETA: 10s - loss: 2.3091 - accuracy: 0.1051\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1050\n",
            "388/938 [===========>..................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1043\n",
            "394/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1046\n",
            "400/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1047\n",
            "406/938 [===========>..................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1048 \n",
            "412/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1053\n",
            "418/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1052\n",
            "424/938 [============>.................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1048\n",
            "430/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1049\n",
            "436/938 [============>.................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1050\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3086 - accuracy: 0.1050\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1049\n",
            "451/938 [=============>................] - ETA: 9s - loss: 2.3089 - accuracy: 0.1044\n",
            "457/938 [=============>................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1040\n",
            "463/938 [=============>................] - ETA: 8s - loss: 2.3087 - accuracy: 0.1042\n",
            "469/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1042\n",
            "475/938 [==============>...............] - ETA: 8s - loss: 2.3087 - accuracy: 0.1045\n",
            "481/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1043\n",
            "487/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1041\n",
            "493/938 [==============>...............] - ETA: 8s - loss: 2.3087 - accuracy: 0.1042\n",
            "499/938 [==============>...............] - ETA: 8s - loss: 2.3087 - accuracy: 0.1039\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1038\n",
            "505/938 [===============>..............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1039\n",
            "508/938 [===============>..............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1037\n",
            "514/938 [===============>..............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1037\n",
            "520/938 [===============>..............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1037\n",
            "526/938 [===============>..............] - ETA: 7s - loss: 2.3087 - accuracy: 0.1036\n",
            "532/938 [================>.............] - ETA: 7s - loss: 2.3086 - accuracy: 0.1038\n",
            "538/938 [================>.............] - ETA: 7s - loss: 2.3085 - accuracy: 0.1037\n",
            "544/938 [================>.............] - ETA: 7s - loss: 2.3083 - accuracy: 0.1044\n",
            "550/938 [================>.............] - ETA: 7s - loss: 2.3083 - accuracy: 0.1045\n",
            "556/938 [================>.............] - ETA: 7s - loss: 2.3083 - accuracy: 0.1047\n",
            "559/938 [================>.............] - ETA: 7s - loss: 2.3082 - accuracy: 0.1048\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3082 - accuracy: 0.1048\n",
            "565/938 [=================>............] - ETA: 6s - loss: 2.3082 - accuracy: 0.1048\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3082 - accuracy: 0.1048\n",
            "571/938 [=================>............] - ETA: 6s - loss: 2.3081 - accuracy: 0.1048\n",
            "577/938 [=================>............] - ETA: 6s - loss: 2.3082 - accuracy: 0.1047\n",
            "583/938 [=================>............] - ETA: 6s - loss: 2.3082 - accuracy: 0.1048\n",
            "589/938 [=================>............] - ETA: 6s - loss: 2.3082 - accuracy: 0.1048\n",
            "595/938 [==================>...........] - ETA: 6s - loss: 2.3081 - accuracy: 0.1048\n",
            "601/938 [==================>...........] - ETA: 6s - loss: 2.3081 - accuracy: 0.1048\n",
            "607/938 [==================>...........] - ETA: 6s - loss: 2.3082 - accuracy: 0.1046\n",
            "613/938 [==================>...........] - ETA: 6s - loss: 2.3081 - accuracy: 0.1049\n",
            "619/938 [==================>...........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1050\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1051\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1052\n",
            "634/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1053\n",
            "640/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1054\n",
            "646/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1055\n",
            "652/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1053\n",
            "658/938 [====================>.........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1051\n",
            "664/938 [====================>.........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1053\n",
            "670/938 [====================>.........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1053\n",
            "676/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1051\n",
            "679/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1051\n",
            "685/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1049\n",
            "691/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1051\n",
            "697/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1050\n",
            "703/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1051\n",
            "709/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1050\n",
            "715/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1050\n",
            "721/938 [======================>.......] - ETA: 4s - loss: 2.3080 - accuracy: 0.1049\n",
            "727/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1049\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1049\n",
            "736/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1051\n",
            "742/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1050\n",
            "748/938 [======================>.......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1048\n",
            "754/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1049\n",
            "760/938 [=======================>......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1049\n",
            "766/938 [=======================>......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1047\n",
            "772/938 [=======================>......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1046\n",
            "778/938 [=======================>......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1047\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 2.3081 - accuracy: 0.1048\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1049\n",
            "787/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1047\n",
            "793/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1049\n",
            "799/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1049\n",
            "805/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1050\n",
            "811/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1050\n",
            "817/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1051\n",
            "823/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1052\n",
            "829/938 [=========================>....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1054\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1053\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1051\n",
            "844/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1052\n",
            "850/938 [==========================>...] - ETA: 1s - loss: 2.3079 - accuracy: 0.1053\n",
            "856/938 [==========================>...] - ETA: 1s - loss: 2.3079 - accuracy: 0.1053\n",
            "862/938 [==========================>...] - ETA: 1s - loss: 2.3079 - accuracy: 0.1054\n",
            "868/938 [==========================>...] - ETA: 1s - loss: 2.3079 - accuracy: 0.1054\n",
            "874/938 [==========================>...] - ETA: 1s - loss: 2.3079 - accuracy: 0.1053\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3079 - accuracy: 0.1053\n",
            "880/938 [===========================>..] - ETA: 1s - loss: 2.3079 - accuracy: 0.1053\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3079 - accuracy: 0.1053\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1052\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1051\n",
            "901/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1050\n",
            "907/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1048\n",
            "913/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1049\n",
            "919/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1047\n",
            "925/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1048\n",
            "931/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1048\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1048\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1047\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3080 - accuracy: 0.1047 - val_loss: 2.3082 - val_accuracy: 0.1028\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 9/12\n",
            "  1/938 [..............................] - ETA: 17s - loss: 2.2677 - accuracy: 0.1562\n",
            "  7/938 [..............................] - ETA: 17s - loss: 2.3028 - accuracy: 0.1205\n",
            " 13/938 [..............................] - ETA: 17s - loss: 2.3038 - accuracy: 0.1274\n",
            " 19/938 [..............................] - ETA: 17s - loss: 2.3079 - accuracy: 0.1127\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3069 - accuracy: 0.1138\n",
            " 31/938 [..............................] - ETA: 17s - loss: 2.3067 - accuracy: 0.1144\n",
            " 37/938 [>.............................] - ETA: 17s - loss: 2.3070 - accuracy: 0.1098\n",
            " 43/938 [>.............................] - ETA: 17s - loss: 2.3074 - accuracy: 0.1086\n",
            " 49/938 [>.............................] - ETA: 17s - loss: 2.3069 - accuracy: 0.1081\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 2.3070 - accuracy: 0.1079\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3075 - accuracy: 0.1064\n",
            " 64/938 [=>............................] - ETA: 16s - loss: 2.3072 - accuracy: 0.1077\n",
            " 70/938 [=>............................] - ETA: 16s - loss: 2.3069 - accuracy: 0.1096\n",
            " 76/938 [=>............................] - ETA: 16s - loss: 2.3068 - accuracy: 0.1112\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 2.3066 - accuracy: 0.1111\n",
            " 88/938 [=>............................] - ETA: 16s - loss: 2.3062 - accuracy: 0.1117\n",
            " 91/938 [=>............................] - ETA: 16s - loss: 2.3066 - accuracy: 0.1104\n",
            " 97/938 [==>...........................] - ETA: 16s - loss: 2.3066 - accuracy: 0.1111\n",
            "103/938 [==>...........................] - ETA: 15s - loss: 2.3069 - accuracy: 0.1101\n",
            "109/938 [==>...........................] - ETA: 15s - loss: 2.3072 - accuracy: 0.1097\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 2.3081 - accuracy: 0.1083\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 2.3084 - accuracy: 0.1069\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 2.3085 - accuracy: 0.1072\n",
            "133/938 [===>..........................] - ETA: 15s - loss: 2.3086 - accuracy: 0.1066\n",
            "139/938 [===>..........................] - ETA: 15s - loss: 2.3085 - accuracy: 0.1062\n",
            "142/938 [===>..........................] - ETA: 15s - loss: 2.3084 - accuracy: 0.1054\n",
            "148/938 [===>..........................] - ETA: 15s - loss: 2.3088 - accuracy: 0.1049\n",
            "154/938 [===>..........................] - ETA: 14s - loss: 2.3087 - accuracy: 0.1045\n",
            "160/938 [====>.........................] - ETA: 14s - loss: 2.3084 - accuracy: 0.1055\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3082 - accuracy: 0.1061\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3085 - accuracy: 0.1059\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3085 - accuracy: 0.1065\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.1063\n",
            "187/938 [====>.........................] - ETA: 14s - loss: 2.3087 - accuracy: 0.1057\n",
            "193/938 [=====>........................] - ETA: 14s - loss: 2.3085 - accuracy: 0.1048\n",
            "199/938 [=====>........................] - ETA: 14s - loss: 2.3083 - accuracy: 0.1051\n",
            "205/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.1047\n",
            "211/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.1044\n",
            "217/938 [=====>........................] - ETA: 13s - loss: 2.3084 - accuracy: 0.1053\n",
            "223/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1050\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1050\n",
            "229/938 [======>.......................] - ETA: 13s - loss: 2.3089 - accuracy: 0.1051\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1051\n",
            "235/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1046\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1048\n",
            "244/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1051\n",
            "250/938 [======>.......................] - ETA: 13s - loss: 2.3091 - accuracy: 0.1048\n",
            "256/938 [=======>......................] - ETA: 12s - loss: 2.3091 - accuracy: 0.1042\n",
            "262/938 [=======>......................] - ETA: 12s - loss: 2.3093 - accuracy: 0.1041\n",
            "268/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1037\n",
            "274/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1035\n",
            "280/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1041\n",
            "286/938 [========>.....................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1039\n",
            "292/938 [========>.....................] - ETA: 12s - loss: 2.3092 - accuracy: 0.1047\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 2.3091 - accuracy: 0.1051\n",
            "298/938 [========>.....................] - ETA: 12s - loss: 2.3092 - accuracy: 0.1050\n",
            "301/938 [========>.....................] - ETA: 12s - loss: 2.3091 - accuracy: 0.1054\n",
            "307/938 [========>.....................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1058\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1060\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 2.3091 - accuracy: 0.1059\n",
            "325/938 [=========>....................] - ETA: 11s - loss: 2.3091 - accuracy: 0.1058\n",
            "331/938 [=========>....................] - ETA: 11s - loss: 2.3091 - accuracy: 0.1060\n",
            "337/938 [=========>....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1065\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3092 - accuracy: 0.1062\n",
            "349/938 [==========>...................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1060\n",
            "355/938 [==========>...................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1059\n",
            "361/938 [==========>...................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1060\n",
            "364/938 [==========>...................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1062\n",
            "367/938 [==========>...................] - ETA: 10s - loss: 2.3087 - accuracy: 0.1062\n",
            "370/938 [==========>...................] - ETA: 10s - loss: 2.3086 - accuracy: 0.1063\n",
            "373/938 [==========>...................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1062\n",
            "376/938 [===========>..................] - ETA: 10s - loss: 2.3087 - accuracy: 0.1062\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1060\n",
            "388/938 [===========>..................] - ETA: 10s - loss: 2.3091 - accuracy: 0.1055\n",
            "394/938 [===========>..................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1056\n",
            "400/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1061\n",
            "406/938 [===========>..................] - ETA: 10s - loss: 2.3091 - accuracy: 0.1061\n",
            "412/938 [============>.................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1059\n",
            "418/938 [============>.................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1057\n",
            "424/938 [============>.................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1058\n",
            "430/938 [============>.................] - ETA: 9s - loss: 2.3093 - accuracy: 0.1055\n",
            "436/938 [============>.................] - ETA: 9s - loss: 2.3093 - accuracy: 0.1054\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1052\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1052\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1050\n",
            "451/938 [=============>................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1048\n",
            "457/938 [=============>................] - ETA: 9s - loss: 2.3092 - accuracy: 0.1050\n",
            "463/938 [=============>................] - ETA: 8s - loss: 2.3091 - accuracy: 0.1052\n",
            "469/938 [==============>...............] - ETA: 8s - loss: 2.3092 - accuracy: 0.1052\n",
            "475/938 [==============>...............] - ETA: 8s - loss: 2.3091 - accuracy: 0.1050\n",
            "481/938 [==============>...............] - ETA: 8s - loss: 2.3090 - accuracy: 0.1053\n",
            "487/938 [==============>...............] - ETA: 8s - loss: 2.3090 - accuracy: 0.1052\n",
            "493/938 [==============>...............] - ETA: 8s - loss: 2.3090 - accuracy: 0.1049\n",
            "499/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1051\n",
            "505/938 [===============>..............] - ETA: 8s - loss: 2.3089 - accuracy: 0.1051\n",
            "511/938 [===============>..............] - ETA: 8s - loss: 2.3089 - accuracy: 0.1051\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1053\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1053\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3089 - accuracy: 0.1051\n",
            "532/938 [================>.............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1051\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1053\n",
            "538/938 [================>.............] - ETA: 7s - loss: 2.3089 - accuracy: 0.1054\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3089 - accuracy: 0.1054\n",
            "544/938 [================>.............] - ETA: 7s - loss: 2.3090 - accuracy: 0.1053\n",
            "550/938 [================>.............] - ETA: 7s - loss: 2.3089 - accuracy: 0.1054\n",
            "556/938 [================>.............] - ETA: 7s - loss: 2.3089 - accuracy: 0.1053\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3088 - accuracy: 0.1053\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3088 - accuracy: 0.1055\n",
            "574/938 [=================>............] - ETA: 6s - loss: 2.3088 - accuracy: 0.1053\n",
            "580/938 [=================>............] - ETA: 6s - loss: 2.3087 - accuracy: 0.1058\n",
            "586/938 [=================>............] - ETA: 6s - loss: 2.3087 - accuracy: 0.1056\n",
            "592/938 [=================>............] - ETA: 6s - loss: 2.3087 - accuracy: 0.1056\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 2.3087 - accuracy: 0.1055\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 2.3086 - accuracy: 0.1055\n",
            "607/938 [==================>...........] - ETA: 6s - loss: 2.3086 - accuracy: 0.1056\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 2.3085 - accuracy: 0.1056\n",
            "613/938 [==================>...........] - ETA: 6s - loss: 2.3086 - accuracy: 0.1055\n",
            "619/938 [==================>...........] - ETA: 6s - loss: 2.3085 - accuracy: 0.1057\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3083 - accuracy: 0.1059\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 2.3083 - accuracy: 0.1059\n",
            "637/938 [===================>..........] - ETA: 5s - loss: 2.3086 - accuracy: 0.1055\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 2.3087 - accuracy: 0.1053\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 2.3086 - accuracy: 0.1056\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 2.3087 - accuracy: 0.1054\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 2.3089 - accuracy: 0.1051\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 2.3088 - accuracy: 0.1051\n",
            "670/938 [====================>.........] - ETA: 5s - loss: 2.3088 - accuracy: 0.1052\n",
            "676/938 [====================>.........] - ETA: 4s - loss: 2.3089 - accuracy: 0.1051\n",
            "682/938 [====================>.........] - ETA: 4s - loss: 2.3089 - accuracy: 0.1053\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 2.3088 - accuracy: 0.1055\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 2.3088 - accuracy: 0.1054\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3088 - accuracy: 0.1054\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3088 - accuracy: 0.1053\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3087 - accuracy: 0.1053\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3087 - accuracy: 0.1055\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3089 - accuracy: 0.1054\n",
            "727/938 [======================>.......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1054\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1054\n",
            "733/938 [======================>.......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1053\n",
            "739/938 [======================>.......] - ETA: 3s - loss: 2.3088 - accuracy: 0.1052\n",
            "745/938 [======================>.......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1048\n",
            "751/938 [=======================>......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1051\n",
            "757/938 [=======================>......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1051\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1052\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1052\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1051\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 2.3090 - accuracy: 0.1051\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3090 - accuracy: 0.1050\n",
            "787/938 [========================>.....] - ETA: 2s - loss: 2.3090 - accuracy: 0.1051\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3089 - accuracy: 0.1051\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3089 - accuracy: 0.1050\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3089 - accuracy: 0.1049\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1049\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1049\n",
            "820/938 [=========================>....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1049\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1048\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3087 - accuracy: 0.1049\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3088 - accuracy: 0.1048\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3087 - accuracy: 0.1048\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3087 - accuracy: 0.1048\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3087 - accuracy: 0.1046\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3087 - accuracy: 0.1047\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3087 - accuracy: 0.1048\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3087 - accuracy: 0.1048\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3086 - accuracy: 0.1050\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3086 - accuracy: 0.1051\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 2.3086 - accuracy: 0.1050\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 2.3086 - accuracy: 0.1050\n",
            "898/938 [===========================>..] - ETA: 0s - loss: 2.3086 - accuracy: 0.1050\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3087 - accuracy: 0.1050\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3086 - accuracy: 0.1052\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3086 - accuracy: 0.1050\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3086 - accuracy: 0.1050\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3087 - accuracy: 0.1049\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3088 - accuracy: 0.1049\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3088 - accuracy: 0.1048\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3088 - accuracy: 0.1048 - val_loss: 2.3086 - val_accuracy: 0.0892\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 10/12\n",
            "  1/938 [..............................] - ETA: 17s - loss: 2.3112 - accuracy: 0.0781\n",
            "  7/938 [..............................] - ETA: 17s - loss: 2.3007 - accuracy: 0.1071\n",
            " 13/938 [..............................] - ETA: 17s - loss: 2.3058 - accuracy: 0.0877\n",
            " 19/938 [..............................] - ETA: 17s - loss: 2.3099 - accuracy: 0.0831\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3103 - accuracy: 0.0875\n",
            " 28/938 [..............................] - ETA: 17s - loss: 2.3111 - accuracy: 0.0904\n",
            " 31/938 [..............................] - ETA: 17s - loss: 2.3089 - accuracy: 0.0968\n",
            " 34/938 [>.............................] - ETA: 16s - loss: 2.3108 - accuracy: 0.0965\n",
            " 37/938 [>.............................] - ETA: 16s - loss: 2.3113 - accuracy: 0.0959\n",
            " 40/938 [>.............................] - ETA: 16s - loss: 2.3109 - accuracy: 0.0973\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 2.3094 - accuracy: 0.0999\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 2.3093 - accuracy: 0.0992\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3085 - accuracy: 0.1018\n",
            " 64/938 [=>............................] - ETA: 16s - loss: 2.3087 - accuracy: 0.1018\n",
            " 70/938 [=>............................] - ETA: 16s - loss: 2.3083 - accuracy: 0.1047\n",
            " 76/938 [=>............................] - ETA: 16s - loss: 2.3079 - accuracy: 0.1065\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 2.3077 - accuracy: 0.1071\n",
            " 88/938 [=>............................] - ETA: 15s - loss: 2.3080 - accuracy: 0.1062\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 2.3074 - accuracy: 0.1061\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 2.3077 - accuracy: 0.1055\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 2.3079 - accuracy: 0.1048\n",
            "109/938 [==>...........................] - ETA: 15s - loss: 2.3076 - accuracy: 0.1058\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 2.3074 - accuracy: 0.1063\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 2.3073 - accuracy: 0.1061\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 2.3066 - accuracy: 0.1080\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 2.3071 - accuracy: 0.1079\n",
            "133/938 [===>..........................] - ETA: 15s - loss: 2.3068 - accuracy: 0.1078\n",
            "139/938 [===>..........................] - ETA: 14s - loss: 2.3069 - accuracy: 0.1072\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 2.3065 - accuracy: 0.1082\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 2.3069 - accuracy: 0.1079\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 2.3071 - accuracy: 0.1084\n",
            "163/938 [====>.........................] - ETA: 14s - loss: 2.3069 - accuracy: 0.1097\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3066 - accuracy: 0.1099\n",
            "169/938 [====>.........................] - ETA: 14s - loss: 2.3067 - accuracy: 0.1096\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3069 - accuracy: 0.1091\n",
            "175/938 [====>.........................] - ETA: 14s - loss: 2.3068 - accuracy: 0.1088\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 2.3067 - accuracy: 0.1091\n",
            "184/938 [====>.........................] - ETA: 14s - loss: 2.3069 - accuracy: 0.1086\n",
            "190/938 [=====>........................] - ETA: 13s - loss: 2.3068 - accuracy: 0.1079\n",
            "196/938 [=====>........................] - ETA: 13s - loss: 2.3069 - accuracy: 0.1075\n",
            "202/938 [=====>........................] - ETA: 13s - loss: 2.3072 - accuracy: 0.1066\n",
            "208/938 [=====>........................] - ETA: 13s - loss: 2.3074 - accuracy: 0.1064\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 2.3073 - accuracy: 0.1065\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 2.3072 - accuracy: 0.1067\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 2.3076 - accuracy: 0.1067\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 2.3076 - accuracy: 0.1068\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 2.3075 - accuracy: 0.1072\n",
            "241/938 [======>.......................] - ETA: 13s - loss: 2.3074 - accuracy: 0.1072\n",
            "244/938 [======>.......................] - ETA: 12s - loss: 2.3076 - accuracy: 0.1070\n",
            "247/938 [======>.......................] - ETA: 12s - loss: 2.3077 - accuracy: 0.1063\n",
            "253/938 [=======>......................] - ETA: 12s - loss: 2.3076 - accuracy: 0.1062\n",
            "259/938 [=======>......................] - ETA: 12s - loss: 2.3076 - accuracy: 0.1064\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1062\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 2.3077 - accuracy: 0.1066\n",
            "277/938 [=======>......................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1064\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1064\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1060\n",
            "292/938 [========>.....................] - ETA: 12s - loss: 2.3078 - accuracy: 0.1060\n",
            "298/938 [========>.....................] - ETA: 11s - loss: 2.3078 - accuracy: 0.1063\n",
            "304/938 [========>.....................] - ETA: 11s - loss: 2.3078 - accuracy: 0.1062\n",
            "310/938 [========>.....................] - ETA: 11s - loss: 2.3078 - accuracy: 0.1059\n",
            "316/938 [=========>....................] - ETA: 11s - loss: 2.3079 - accuracy: 0.1060\n",
            "322/938 [=========>....................] - ETA: 11s - loss: 2.3080 - accuracy: 0.1055\n",
            "328/938 [=========>....................] - ETA: 11s - loss: 2.3081 - accuracy: 0.1050\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 2.3081 - accuracy: 0.1048\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3081 - accuracy: 0.1045\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 2.3081 - accuracy: 0.1045\n",
            "349/938 [==========>...................] - ETA: 11s - loss: 2.3081 - accuracy: 0.1043\n",
            "352/938 [==========>...................] - ETA: 10s - loss: 2.3081 - accuracy: 0.1042\n",
            "355/938 [==========>...................] - ETA: 10s - loss: 2.3081 - accuracy: 0.1040\n",
            "361/938 [==========>...................] - ETA: 10s - loss: 2.3080 - accuracy: 0.1038\n",
            "367/938 [==========>...................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1031\n",
            "373/938 [==========>...................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1030\n",
            "379/938 [===========>..................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1035\n",
            "385/938 [===========>..................] - ETA: 10s - loss: 2.3081 - accuracy: 0.1035\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1034\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3081 - accuracy: 0.1035\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1035\n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1035\n",
            "412/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1033\n",
            "415/938 [============>.................] - ETA: 9s - loss: 2.3080 - accuracy: 0.1037\n",
            "418/938 [============>.................] - ETA: 9s - loss: 2.3080 - accuracy: 0.1037\n",
            "424/938 [============>.................] - ETA: 9s - loss: 2.3080 - accuracy: 0.1038\n",
            "430/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1035\n",
            "436/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1036\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1030\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1028\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1028\n",
            "460/938 [=============>................] - ETA: 8s - loss: 2.3083 - accuracy: 0.1025\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3083 - accuracy: 0.1025\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 2.3082 - accuracy: 0.1027\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1027\n",
            "481/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1028\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1028\n",
            "487/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1029\n",
            "493/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1029\n",
            "499/938 [==============>...............] - ETA: 8s - loss: 2.3084 - accuracy: 0.1026\n",
            "505/938 [===============>..............] - ETA: 8s - loss: 2.3084 - accuracy: 0.1024\n",
            "511/938 [===============>..............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1024\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3082 - accuracy: 0.1024\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3082 - accuracy: 0.1026\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3082 - accuracy: 0.1026\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1029\n",
            "538/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1029\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1030\n",
            "544/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1028\n",
            "547/938 [================>.............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1028\n",
            "550/938 [================>.............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1030\n",
            "556/938 [================>.............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1029\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1030\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1032\n",
            "574/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1031\n",
            "580/938 [=================>............] - ETA: 6s - loss: 2.3080 - accuracy: 0.1030\n",
            "586/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1031\n",
            "592/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1028\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1030\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1032\n",
            "607/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1028\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1028\n",
            "613/938 [==================>...........] - ETA: 6s - loss: 2.3080 - accuracy: 0.1029\n",
            "619/938 [==================>...........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1028\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1028\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1029\n",
            "637/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1028\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1026\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1024\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1024\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1022\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1024\n",
            "670/938 [====================>.........] - ETA: 5s - loss: 2.3081 - accuracy: 0.1024\n",
            "673/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1025\n",
            "676/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1026\n",
            "679/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1028\n",
            "682/938 [====================>.........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1029\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1030\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1031\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1033\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3079 - accuracy: 0.1032\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1032\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1032\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3079 - accuracy: 0.1031\n",
            "727/938 [======================>.......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1030\n",
            "730/938 [======================>.......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1029\n",
            "733/938 [======================>.......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1029\n",
            "739/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1029\n",
            "745/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1029\n",
            "751/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1028\n",
            "757/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1029\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1028\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3081 - accuracy: 0.1028\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1028\n",
            "781/938 [=======================>......] - ETA: 2s - loss: 2.3081 - accuracy: 0.1027\n",
            "787/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1027\n",
            "793/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1026\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1025\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1026\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1026\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1026\n",
            "820/938 [=========================>....] - ETA: 2s - loss: 2.3082 - accuracy: 0.1026\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1024\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3081 - accuracy: 0.1025\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1024\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1025\n",
            "844/938 [=========================>....] - ETA: 1s - loss: 2.3081 - accuracy: 0.1025\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1024\n",
            "850/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1024\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1024\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1025\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1026\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1025\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3081 - accuracy: 0.1024\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3080 - accuracy: 0.1027\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 2.3081 - accuracy: 0.1025\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 2.3081 - accuracy: 0.1025\n",
            "901/938 [===========================>..] - ETA: 0s - loss: 2.3081 - accuracy: 0.1026\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3081 - accuracy: 0.1025\n",
            "907/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1025\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1026\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1025\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1024\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1024\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1024\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3081 - accuracy: 0.1025\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3081 - accuracy: 0.1025 - val_loss: 2.3047 - val_accuracy: 0.1135\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 11/12\n",
            "  1/938 [..............................] - ETA: 17s - loss: 2.2757 - accuracy: 0.1875\n",
            "  7/938 [..............................] - ETA: 17s - loss: 2.3077 - accuracy: 0.1094\n",
            " 13/938 [..............................] - ETA: 17s - loss: 2.3130 - accuracy: 0.0925\n",
            " 19/938 [..............................] - ETA: 17s - loss: 2.3104 - accuracy: 0.1020\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3084 - accuracy: 0.1088\n",
            " 31/938 [..............................] - ETA: 16s - loss: 2.3054 - accuracy: 0.1094\n",
            " 37/938 [>.............................] - ETA: 16s - loss: 2.3078 - accuracy: 0.1043\n",
            " 40/938 [>.............................] - ETA: 16s - loss: 2.3085 - accuracy: 0.1023\n",
            " 43/938 [>.............................] - ETA: 16s - loss: 2.3091 - accuracy: 0.1028\n",
            " 46/938 [>.............................] - ETA: 16s - loss: 2.3089 - accuracy: 0.1039\n",
            " 52/938 [>.............................] - ETA: 16s - loss: 2.3071 - accuracy: 0.1064\n",
            " 58/938 [>.............................] - ETA: 16s - loss: 2.3077 - accuracy: 0.1070\n",
            " 64/938 [=>............................] - ETA: 16s - loss: 2.3088 - accuracy: 0.1030\n",
            " 70/938 [=>............................] - ETA: 16s - loss: 2.3085 - accuracy: 0.1027\n",
            " 76/938 [=>............................] - ETA: 16s - loss: 2.3083 - accuracy: 0.1024\n",
            " 82/938 [=>............................] - ETA: 15s - loss: 2.3084 - accuracy: 0.1006\n",
            " 88/938 [=>............................] - ETA: 15s - loss: 2.3082 - accuracy: 0.1030\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 2.3075 - accuracy: 0.1057\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 2.3074 - accuracy: 0.1061\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 2.3079 - accuracy: 0.1052\n",
            "109/938 [==>...........................] - ETA: 15s - loss: 2.3083 - accuracy: 0.1046\n",
            "115/938 [==>...........................] - ETA: 15s - loss: 2.3088 - accuracy: 0.1042\n",
            "121/938 [==>...........................] - ETA: 15s - loss: 2.3090 - accuracy: 0.1037\n",
            "127/938 [===>..........................] - ETA: 15s - loss: 2.3090 - accuracy: 0.1027\n",
            "133/938 [===>..........................] - ETA: 15s - loss: 2.3096 - accuracy: 0.1016\n",
            "139/938 [===>..........................] - ETA: 14s - loss: 2.3092 - accuracy: 0.1015\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 2.3092 - accuracy: 0.1031\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 2.3092 - accuracy: 0.1033\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 2.3089 - accuracy: 0.1038\n",
            "160/938 [====>.........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.1037\n",
            "163/938 [====>.........................] - ETA: 14s - loss: 2.3090 - accuracy: 0.1034\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3089 - accuracy: 0.1032\n",
            "172/938 [====>.........................] - ETA: 14s - loss: 2.3090 - accuracy: 0.1024\n",
            "178/938 [====>.........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.1025\n",
            "184/938 [====>.........................] - ETA: 14s - loss: 2.3085 - accuracy: 0.1028\n",
            "190/938 [=====>........................] - ETA: 14s - loss: 2.3086 - accuracy: 0.1027\n",
            "196/938 [=====>........................] - ETA: 13s - loss: 2.3086 - accuracy: 0.1022\n",
            "202/938 [=====>........................] - ETA: 13s - loss: 2.3086 - accuracy: 0.1013\n",
            "208/938 [=====>........................] - ETA: 13s - loss: 2.3086 - accuracy: 0.1011\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 2.3088 - accuracy: 0.1011\n",
            "217/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.1009\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 2.3090 - accuracy: 0.1005\n",
            "223/938 [======>.......................] - ETA: 13s - loss: 2.3089 - accuracy: 0.1003\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 2.3088 - accuracy: 0.1007\n",
            "229/938 [======>.......................] - ETA: 13s - loss: 2.3087 - accuracy: 0.1008\n",
            "235/938 [======>.......................] - ETA: 13s - loss: 2.3086 - accuracy: 0.1015\n",
            "241/938 [======>.......................] - ETA: 13s - loss: 2.3087 - accuracy: 0.1008\n",
            "247/938 [======>.......................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1010\n",
            "253/938 [=======>......................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1015\n",
            "259/938 [=======>......................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1014\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1021\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1024\n",
            "277/938 [=======>......................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1025\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 2.3086 - accuracy: 0.1025\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1027\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 2.3089 - accuracy: 0.1023\n",
            "298/938 [========>.....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1021\n",
            "304/938 [========>.....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1025\n",
            "310/938 [========>.....................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1030\n",
            "316/938 [=========>....................] - ETA: 11s - loss: 2.3086 - accuracy: 0.1031\n",
            "322/938 [=========>....................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1032\n",
            "328/938 [=========>....................] - ETA: 11s - loss: 2.3085 - accuracy: 0.1034\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 2.3085 - accuracy: 0.1032\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3086 - accuracy: 0.1033\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3086 - accuracy: 0.1032\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 2.3086 - accuracy: 0.1034\n",
            "349/938 [==========>...................] - ETA: 11s - loss: 2.3084 - accuracy: 0.1034\n",
            "352/938 [==========>...................] - ETA: 10s - loss: 2.3084 - accuracy: 0.1034\n",
            "355/938 [==========>...................] - ETA: 10s - loss: 2.3084 - accuracy: 0.1036\n",
            "361/938 [==========>...................] - ETA: 10s - loss: 2.3082 - accuracy: 0.1040\n",
            "367/938 [==========>...................] - ETA: 10s - loss: 2.3084 - accuracy: 0.1040\n",
            "373/938 [==========>...................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1044\n",
            "379/938 [===========>..................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1044\n",
            "385/938 [===========>..................] - ETA: 10s - loss: 2.3084 - accuracy: 0.1045\n",
            "391/938 [===========>..................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1043\n",
            "397/938 [===========>..................] - ETA: 10s - loss: 2.3084 - accuracy: 0.1043\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3083 - accuracy: 0.1043\n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3082 - accuracy: 0.1045\n",
            "415/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1045\n",
            "418/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1046\n",
            "421/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1045\n",
            "424/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1046\n",
            "427/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1047\n",
            "430/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1048\n",
            "433/938 [============>.................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1046\n",
            "439/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1045\n",
            "442/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1045\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1046\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1045\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3081 - accuracy: 0.1046\n",
            "460/938 [=============>................] - ETA: 8s - loss: 2.3082 - accuracy: 0.1047\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3082 - accuracy: 0.1046\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1044\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1043\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1044\n",
            "490/938 [==============>...............] - ETA: 8s - loss: 2.3082 - accuracy: 0.1040\n",
            "496/938 [==============>...............] - ETA: 8s - loss: 2.3083 - accuracy: 0.1041\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 2.3082 - accuracy: 0.1042\n",
            "505/938 [===============>..............] - ETA: 8s - loss: 2.3082 - accuracy: 0.1044\n",
            "511/938 [===============>..............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1045\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1046\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1047\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1047\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1046\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3081 - accuracy: 0.1047\n",
            "547/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1051\n",
            "550/938 [================>.............] - ETA: 7s - loss: 2.3080 - accuracy: 0.1052\n",
            "553/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1051\n",
            "556/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1050\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3079 - accuracy: 0.1051\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3078 - accuracy: 0.1051\n",
            "574/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1050\n",
            "580/938 [=================>............] - ETA: 6s - loss: 2.3078 - accuracy: 0.1053\n",
            "586/938 [=================>............] - ETA: 6s - loss: 2.3079 - accuracy: 0.1055\n",
            "592/938 [=================>............] - ETA: 6s - loss: 2.3078 - accuracy: 0.1056\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 2.3079 - accuracy: 0.1058\n",
            "601/938 [==================>...........] - ETA: 6s - loss: 2.3078 - accuracy: 0.1058\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 2.3077 - accuracy: 0.1058\n",
            "607/938 [==================>...........] - ETA: 6s - loss: 2.3077 - accuracy: 0.1057\n",
            "613/938 [==================>...........] - ETA: 6s - loss: 2.3077 - accuracy: 0.1057\n",
            "619/938 [==================>...........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1055\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3079 - accuracy: 0.1053\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1053\n",
            "637/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1053\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1055\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1054\n",
            "652/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1053\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1054\n",
            "658/938 [====================>.........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1054\n",
            "664/938 [====================>.........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1054\n",
            "670/938 [====================>.........] - ETA: 5s - loss: 2.3080 - accuracy: 0.1054\n",
            "676/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1054\n",
            "682/938 [====================>.........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1055\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 2.3081 - accuracy: 0.1055\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1057\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1055\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1056\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1056\n",
            "715/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1056\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3080 - accuracy: 0.1056\n",
            "721/938 [======================>.......] - ETA: 4s - loss: 2.3080 - accuracy: 0.1056\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3080 - accuracy: 0.1057\n",
            "727/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1056\n",
            "733/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1057\n",
            "739/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1057\n",
            "745/938 [======================>.......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1056\n",
            "751/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1057\n",
            "757/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1057\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1056\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1055\n",
            "772/938 [=======================>......] - ETA: 3s - loss: 2.3079 - accuracy: 0.1056\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1055\n",
            "778/938 [=======================>......] - ETA: 3s - loss: 2.3080 - accuracy: 0.1054\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3079 - accuracy: 0.1055\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1054\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1054\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3080 - accuracy: 0.1055\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3079 - accuracy: 0.1057\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1057\n",
            "820/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1056\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1054\n",
            "829/938 [=========================>....] - ETA: 2s - loss: 2.3081 - accuracy: 0.1054\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3081 - accuracy: 0.1055\n",
            "835/938 [=========================>....] - ETA: 1s - loss: 2.3081 - accuracy: 0.1053\n",
            "838/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1053\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3080 - accuracy: 0.1052\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1054\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1054\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1054\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3081 - accuracy: 0.1053\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3080 - accuracy: 0.1053\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3079 - accuracy: 0.1052\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3079 - accuracy: 0.1052\n",
            "886/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1053\n",
            "892/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1051\n",
            "898/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1050\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3080 - accuracy: 0.1049\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1048\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1048\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1049\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1049\n",
            "931/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1050\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3080 - accuracy: 0.1050\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3080 - accuracy: 0.1051 - val_loss: 2.3112 - val_accuracy: 0.1010\n",
            "\u001b[36m(train_mnist pid=3301447)\u001b[0m Epoch 12/12\n",
            "  4/938 [..............................] - ETA: 17s - loss: 2.3167 - accuracy: 0.1133\n",
            " 10/938 [..............................] - ETA: 17s - loss: 2.3133 - accuracy: 0.1234\n",
            " 16/938 [..............................] - ETA: 17s - loss: 2.3101 - accuracy: 0.1045\n",
            " 22/938 [..............................] - ETA: 17s - loss: 2.3113 - accuracy: 0.1037\n",
            " 25/938 [..............................] - ETA: 17s - loss: 2.3110 - accuracy: 0.1037\n",
            " 28/938 [..............................] - ETA: 16s - loss: 2.3099 - accuracy: 0.1055\n",
            " 31/938 [..............................] - ETA: 16s - loss: 2.3096 - accuracy: 0.1038\n",
            " 37/938 [>.............................] - ETA: 16s - loss: 2.3099 - accuracy: 0.1035\n",
            " 43/938 [>.............................] - ETA: 16s - loss: 2.3091 - accuracy: 0.1068\n",
            " 49/938 [>.............................] - ETA: 16s - loss: 2.3089 - accuracy: 0.1084\n",
            " 55/938 [>.............................] - ETA: 16s - loss: 2.3091 - accuracy: 0.1065\n",
            " 61/938 [>.............................] - ETA: 16s - loss: 2.3085 - accuracy: 0.1058\n",
            " 67/938 [=>............................] - ETA: 16s - loss: 2.3081 - accuracy: 0.1049\n",
            " 73/938 [=>............................] - ETA: 16s - loss: 2.3095 - accuracy: 0.1032\n",
            " 79/938 [=>............................] - ETA: 16s - loss: 2.3091 - accuracy: 0.1023\n",
            " 82/938 [=>............................] - ETA: 16s - loss: 2.3088 - accuracy: 0.1021\n",
            " 85/938 [=>............................] - ETA: 16s - loss: 2.3088 - accuracy: 0.1024\n",
            " 88/938 [=>............................] - ETA: 16s - loss: 2.3085 - accuracy: 0.1023\n",
            " 94/938 [==>...........................] - ETA: 15s - loss: 2.3086 - accuracy: 0.1011\n",
            "100/938 [==>...........................] - ETA: 15s - loss: 2.3087 - accuracy: 0.1006\n",
            "106/938 [==>...........................] - ETA: 15s - loss: 2.3085 - accuracy: 0.1001\n",
            "112/938 [==>...........................] - ETA: 15s - loss: 2.3084 - accuracy: 0.1000\n",
            "118/938 [==>...........................] - ETA: 15s - loss: 2.3086 - accuracy: 0.0990\n",
            "124/938 [==>...........................] - ETA: 15s - loss: 2.3087 - accuracy: 0.0985\n",
            "130/938 [===>..........................] - ETA: 15s - loss: 2.3087 - accuracy: 0.0992\n",
            "136/938 [===>..........................] - ETA: 15s - loss: 2.3086 - accuracy: 0.0990\n",
            "139/938 [===>..........................] - ETA: 15s - loss: 2.3090 - accuracy: 0.0984\n",
            "142/938 [===>..........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.0991\n",
            "145/938 [===>..........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.0992\n",
            "148/938 [===>..........................] - ETA: 14s - loss: 2.3087 - accuracy: 0.0988\n",
            "151/938 [===>..........................] - ETA: 14s - loss: 2.3089 - accuracy: 0.0980\n",
            "154/938 [===>..........................] - ETA: 14s - loss: 2.3087 - accuracy: 0.0984\n",
            "157/938 [====>.........................] - ETA: 14s - loss: 2.3086 - accuracy: 0.0986\n",
            "160/938 [====>.........................] - ETA: 14s - loss: 2.3089 - accuracy: 0.0985\n",
            "163/938 [====>.........................] - ETA: 14s - loss: 2.3090 - accuracy: 0.0986\n",
            "166/938 [====>.........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.0990\n",
            "169/938 [====>.........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.0993\n",
            "175/938 [====>.........................] - ETA: 14s - loss: 2.3087 - accuracy: 0.0990\n",
            "181/938 [====>.........................] - ETA: 14s - loss: 2.3088 - accuracy: 0.0991\n",
            "187/938 [====>.........................] - ETA: 14s - loss: 2.3086 - accuracy: 0.0989\n",
            "193/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.0989\n",
            "199/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.0992\n",
            "205/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.0989\n",
            "211/938 [=====>........................] - ETA: 13s - loss: 2.3087 - accuracy: 0.0988\n",
            "214/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.0991\n",
            "217/938 [=====>........................] - ETA: 13s - loss: 2.3089 - accuracy: 0.0995\n",
            "220/938 [======>.......................] - ETA: 13s - loss: 2.3092 - accuracy: 0.0995\n",
            "226/938 [======>.......................] - ETA: 13s - loss: 2.3096 - accuracy: 0.0996\n",
            "232/938 [======>.......................] - ETA: 13s - loss: 2.3095 - accuracy: 0.1001\n",
            "238/938 [======>.......................] - ETA: 13s - loss: 2.3098 - accuracy: 0.1002\n",
            "244/938 [======>.......................] - ETA: 13s - loss: 2.3097 - accuracy: 0.1002\n",
            "250/938 [======>.......................] - ETA: 12s - loss: 2.3098 - accuracy: 0.1001\n",
            "256/938 [=======>......................] - ETA: 12s - loss: 2.3095 - accuracy: 0.1008\n",
            "262/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1014\n",
            "265/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1016\n",
            "268/938 [=======>......................] - ETA: 12s - loss: 2.3094 - accuracy: 0.1019\n",
            "271/938 [=======>......................] - ETA: 12s - loss: 2.3093 - accuracy: 0.1019\n",
            "277/938 [=======>......................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1027\n",
            "283/938 [========>.....................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1026\n",
            "289/938 [========>.....................] - ETA: 12s - loss: 2.3090 - accuracy: 0.1028\n",
            "295/938 [========>.....................] - ETA: 12s - loss: 2.3088 - accuracy: 0.1030\n",
            "301/938 [========>.....................] - ETA: 11s - loss: 2.3085 - accuracy: 0.1030\n",
            "307/938 [========>.....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1026\n",
            "313/938 [=========>....................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1029\n",
            "319/938 [=========>....................] - ETA: 11s - loss: 2.3087 - accuracy: 0.1022\n",
            "325/938 [=========>....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1024\n",
            "331/938 [=========>....................] - ETA: 11s - loss: 2.3088 - accuracy: 0.1021\n",
            "334/938 [=========>....................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1019\n",
            "337/938 [=========>....................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1017\n",
            "340/938 [=========>....................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1017\n",
            "343/938 [=========>....................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1016\n",
            "346/938 [==========>...................] - ETA: 11s - loss: 2.3090 - accuracy: 0.1015\n",
            "352/938 [==========>...................] - ETA: 11s - loss: 2.3089 - accuracy: 0.1014\n",
            "358/938 [==========>...................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1013\n",
            "364/938 [==========>...................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1013\n",
            "370/938 [==========>...................] - ETA: 10s - loss: 2.3090 - accuracy: 0.1014\n",
            "376/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1014\n",
            "382/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1014\n",
            "388/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1013\n",
            "394/938 [===========>..................] - ETA: 10s - loss: 2.3089 - accuracy: 0.1016\n",
            "400/938 [===========>..................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1022\n",
            "403/938 [===========>..................] - ETA: 10s - loss: 2.3088 - accuracy: 0.1021\n",
            "409/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1020\n",
            "415/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1017\n",
            "421/938 [============>.................] - ETA: 9s - loss: 2.3087 - accuracy: 0.1017\n",
            "427/938 [============>.................] - ETA: 9s - loss: 2.3086 - accuracy: 0.1018\n",
            "433/938 [============>.................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1017\n",
            "439/938 [=============>................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1014\n",
            "445/938 [=============>................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1013\n",
            "448/938 [=============>................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1013\n",
            "451/938 [=============>................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1013\n",
            "454/938 [=============>................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1015\n",
            "457/938 [=============>................] - ETA: 9s - loss: 2.3088 - accuracy: 0.1016\n",
            "460/938 [=============>................] - ETA: 8s - loss: 2.3089 - accuracy: 0.1013\n",
            "463/938 [=============>................] - ETA: 8s - loss: 2.3089 - accuracy: 0.1014\n",
            "466/938 [=============>................] - ETA: 8s - loss: 2.3089 - accuracy: 0.1014\n",
            "472/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1014\n",
            "478/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1016\n",
            "484/938 [==============>...............] - ETA: 8s - loss: 2.3088 - accuracy: 0.1013\n",
            "490/938 [==============>...............] - ETA: 8s - loss: 2.3089 - accuracy: 0.1010\n",
            "496/938 [==============>...............] - ETA: 8s - loss: 2.3090 - accuracy: 0.1009\n",
            "502/938 [===============>..............] - ETA: 8s - loss: 2.3091 - accuracy: 0.1008\n",
            "508/938 [===============>..............] - ETA: 8s - loss: 2.3091 - accuracy: 0.1011\n",
            "511/938 [===============>..............] - ETA: 8s - loss: 2.3090 - accuracy: 0.1013\n",
            "517/938 [===============>..............] - ETA: 7s - loss: 2.3090 - accuracy: 0.1015\n",
            "523/938 [===============>..............] - ETA: 7s - loss: 2.3090 - accuracy: 0.1014\n",
            "529/938 [===============>..............] - ETA: 7s - loss: 2.3091 - accuracy: 0.1013\n",
            "535/938 [================>.............] - ETA: 7s - loss: 2.3092 - accuracy: 0.1011\n",
            "541/938 [================>.............] - ETA: 7s - loss: 2.3091 - accuracy: 0.1012\n",
            "547/938 [================>.............] - ETA: 7s - loss: 2.3091 - accuracy: 0.1012\n",
            "553/938 [================>.............] - ETA: 7s - loss: 2.3092 - accuracy: 0.1011\n",
            "559/938 [================>.............] - ETA: 7s - loss: 2.3091 - accuracy: 0.1015\n",
            "562/938 [================>.............] - ETA: 7s - loss: 2.3090 - accuracy: 0.1016\n",
            "565/938 [=================>............] - ETA: 7s - loss: 2.3091 - accuracy: 0.1017\n",
            "568/938 [=================>............] - ETA: 6s - loss: 2.3091 - accuracy: 0.1016\n",
            "574/938 [=================>............] - ETA: 6s - loss: 2.3091 - accuracy: 0.1017\n",
            "580/938 [=================>............] - ETA: 6s - loss: 2.3090 - accuracy: 0.1017\n",
            "586/938 [=================>............] - ETA: 6s - loss: 2.3088 - accuracy: 0.1019\n",
            "592/938 [=================>............] - ETA: 6s - loss: 2.3089 - accuracy: 0.1020\n",
            "598/938 [==================>...........] - ETA: 6s - loss: 2.3090 - accuracy: 0.1018\n",
            "604/938 [==================>...........] - ETA: 6s - loss: 2.3090 - accuracy: 0.1016\n",
            "610/938 [==================>...........] - ETA: 6s - loss: 2.3090 - accuracy: 0.1017\n",
            "613/938 [==================>...........] - ETA: 6s - loss: 2.3091 - accuracy: 0.1017\n",
            "616/938 [==================>...........] - ETA: 6s - loss: 2.3091 - accuracy: 0.1017\n",
            "619/938 [==================>...........] - ETA: 6s - loss: 2.3091 - accuracy: 0.1018\n",
            "625/938 [==================>...........] - ETA: 5s - loss: 2.3091 - accuracy: 0.1017\n",
            "631/938 [===================>..........] - ETA: 5s - loss: 2.3092 - accuracy: 0.1015\n",
            "637/938 [===================>..........] - ETA: 5s - loss: 2.3090 - accuracy: 0.1016\n",
            "643/938 [===================>..........] - ETA: 5s - loss: 2.3092 - accuracy: 0.1014\n",
            "649/938 [===================>..........] - ETA: 5s - loss: 2.3091 - accuracy: 0.1018\n",
            "655/938 [===================>..........] - ETA: 5s - loss: 2.3092 - accuracy: 0.1016\n",
            "661/938 [====================>.........] - ETA: 5s - loss: 2.3092 - accuracy: 0.1018\n",
            "664/938 [====================>.........] - ETA: 5s - loss: 2.3092 - accuracy: 0.1018\n",
            "667/938 [====================>.........] - ETA: 5s - loss: 2.3092 - accuracy: 0.1020\n",
            "670/938 [====================>.........] - ETA: 5s - loss: 2.3091 - accuracy: 0.1021\n",
            "676/938 [====================>.........] - ETA: 4s - loss: 2.3091 - accuracy: 0.1019\n",
            "682/938 [====================>.........] - ETA: 4s - loss: 2.3091 - accuracy: 0.1017\n",
            "688/938 [=====================>........] - ETA: 4s - loss: 2.3091 - accuracy: 0.1019\n",
            "694/938 [=====================>........] - ETA: 4s - loss: 2.3089 - accuracy: 0.1023\n",
            "700/938 [=====================>........] - ETA: 4s - loss: 2.3090 - accuracy: 0.1020\n",
            "706/938 [=====================>........] - ETA: 4s - loss: 2.3090 - accuracy: 0.1020\n",
            "712/938 [=====================>........] - ETA: 4s - loss: 2.3090 - accuracy: 0.1022\n",
            "718/938 [=====================>........] - ETA: 4s - loss: 2.3089 - accuracy: 0.1022\n",
            "724/938 [======================>.......] - ETA: 4s - loss: 2.3089 - accuracy: 0.1023\n",
            "727/938 [======================>.......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1022\n",
            "733/938 [======================>.......] - ETA: 3s - loss: 2.3090 - accuracy: 0.1019\n",
            "739/938 [======================>.......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1020\n",
            "745/938 [======================>.......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1020\n",
            "751/938 [=======================>......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1020\n",
            "757/938 [=======================>......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1021\n",
            "763/938 [=======================>......] - ETA: 3s - loss: 2.3088 - accuracy: 0.1024\n",
            "769/938 [=======================>......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1024\n",
            "772/938 [=======================>......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1023\n",
            "775/938 [=======================>......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1023\n",
            "778/938 [=======================>......] - ETA: 3s - loss: 2.3089 - accuracy: 0.1022\n",
            "784/938 [========================>.....] - ETA: 2s - loss: 2.3089 - accuracy: 0.1022\n",
            "790/938 [========================>.....] - ETA: 2s - loss: 2.3089 - accuracy: 0.1022\n",
            "796/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1021\n",
            "802/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1019\n",
            "808/938 [========================>.....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1020\n",
            "814/938 [=========================>....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1017\n",
            "820/938 [=========================>....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1018\n",
            "826/938 [=========================>....] - ETA: 2s - loss: 2.3087 - accuracy: 0.1019\n",
            "829/938 [=========================>....] - ETA: 2s - loss: 2.3088 - accuracy: 0.1019\n",
            "832/938 [=========================>....] - ETA: 1s - loss: 2.3087 - accuracy: 0.1020\n",
            "835/938 [=========================>....] - ETA: 1s - loss: 2.3087 - accuracy: 0.1021\n",
            "841/938 [=========================>....] - ETA: 1s - loss: 2.3086 - accuracy: 0.1022\n",
            "847/938 [==========================>...] - ETA: 1s - loss: 2.3086 - accuracy: 0.1021\n",
            "853/938 [==========================>...] - ETA: 1s - loss: 2.3086 - accuracy: 0.1022\n",
            "859/938 [==========================>...] - ETA: 1s - loss: 2.3086 - accuracy: 0.1022\n",
            "865/938 [==========================>...] - ETA: 1s - loss: 2.3086 - accuracy: 0.1022\n",
            "871/938 [==========================>...] - ETA: 1s - loss: 2.3085 - accuracy: 0.1023\n",
            "877/938 [===========================>..] - ETA: 1s - loss: 2.3086 - accuracy: 0.1022\n",
            "883/938 [===========================>..] - ETA: 1s - loss: 2.3085 - accuracy: 0.1024\n",
            "889/938 [===========================>..] - ETA: 0s - loss: 2.3085 - accuracy: 0.1024\n",
            "895/938 [===========================>..] - ETA: 0s - loss: 2.3085 - accuracy: 0.1024\n",
            "901/938 [===========================>..] - ETA: 0s - loss: 2.3085 - accuracy: 0.1025\n",
            "904/938 [===========================>..] - ETA: 0s - loss: 2.3085 - accuracy: 0.1024\n",
            "910/938 [============================>.] - ETA: 0s - loss: 2.3085 - accuracy: 0.1021\n",
            "916/938 [============================>.] - ETA: 0s - loss: 2.3085 - accuracy: 0.1021\n",
            "922/938 [============================>.] - ETA: 0s - loss: 2.3085 - accuracy: 0.1021\n",
            "928/938 [============================>.] - ETA: 0s - loss: 2.3086 - accuracy: 0.1022\n",
            "934/938 [============================>.] - ETA: 0s - loss: 2.3085 - accuracy: 0.1024\n",
            "937/938 [============================>.] - ETA: 0s - loss: 2.3084 - accuracy: 0.1025\n",
            "938/938 [==============================] - 19s 20ms/step - loss: 2.3084 - accuracy: 0.1025 - val_loss: 2.3197 - val_accuracy: 0.1135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3302450)\u001b[0m 2023-12-05 01:59:15.119165: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3302450)\u001b[0m 2023-12-05 01:59:15.167325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3302450)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3302450)\u001b[0m 2023-12-05 01:59:15.166816: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3302450)\u001b[0m 2023-12-05 01:59:16.068071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3302450)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3302450)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3302450)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3302450)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3302450)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3302450)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3302450)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3302450)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m 2023-12-05 01:59:17.681002: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 1/12\n",
            "  6/469 [..............................] - ETA: 4s - loss: 9.5227 - accuracy: 0.1589  \n",
            " 17/469 [>.............................] - ETA: 4s - loss: 4.8520 - accuracy: 0.1324\n",
            " 28/469 [>.............................] - ETA: 4s - loss: 3.7983 - accuracy: 0.1660\n",
            " 39/469 [=>............................] - ETA: 4s - loss: 3.1849 - accuracy: 0.2514\n",
            " 45/469 [=>............................] - ETA: 4s - loss: 2.9294 - accuracy: 0.2983\n",
            " 57/469 [==>...........................] - ETA: 4s - loss: 2.5455 - accuracy: 0.3746\n",
            " 68/469 [===>..........................] - ETA: 3s - loss: 2.3034 - accuracy: 0.4243\n",
            " 80/469 [====>.........................] - ETA: 3s - loss: 2.1026 - accuracy: 0.4640\n",
            " 92/469 [====>.........................] - ETA: 3s - loss: 1.9499 - accuracy: 0.4956\n",
            "104/469 [=====>........................] - ETA: 3s - loss: 1.8296 - accuracy: 0.5230\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 1.7278 - accuracy: 0.5450\n",
            "127/469 [=======>......................] - ETA: 3s - loss: 1.6593 - accuracy: 0.5600\n",
            "133/469 [=======>......................] - ETA: 3s - loss: 1.6256 - accuracy: 0.5668\n",
            "145/469 [========>.....................] - ETA: 3s - loss: 1.5577 - accuracy: 0.5824\n",
            "157/469 [=========>....................] - ETA: 3s - loss: 1.4982 - accuracy: 0.5959\n",
            "169/469 [=========>....................] - ETA: 2s - loss: 1.4441 - accuracy: 0.6075\n",
            "180/469 [==========>...................] - ETA: 2s - loss: 1.4027 - accuracy: 0.6168\n",
            "192/469 [===========>..................] - ETA: 2s - loss: 1.3652 - accuracy: 0.6260\n",
            "198/469 [===========>..................] - ETA: 2s - loss: 1.3500 - accuracy: 0.6299\n",
            "204/469 [============>.................] - ETA: 2s - loss: 1.3307 - accuracy: 0.6344\n",
            "210/469 [============>.................] - ETA: 2s - loss: 1.3130 - accuracy: 0.6379\n",
            "222/469 [=============>................] - ETA: 2s - loss: 1.2869 - accuracy: 0.6434\n",
            "234/469 [=============>................] - ETA: 2s - loss: 1.2592 - accuracy: 0.6499\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 1.2360 - accuracy: 0.6550\n",
            "258/469 [===============>..............] - ETA: 2s - loss: 1.2137 - accuracy: 0.6601\n",
            "264/469 [===============>..............] - ETA: 2s - loss: 1.2008 - accuracy: 0.6631\n",
            "270/469 [================>.............] - ETA: 1s - loss: 1.1894 - accuracy: 0.6659\n",
            "276/469 [================>.............] - ETA: 1s - loss: 1.1808 - accuracy: 0.6678\n",
            "288/469 [=================>............] - ETA: 1s - loss: 1.1639 - accuracy: 0.6718\n",
            "300/469 [==================>...........] - ETA: 1s - loss: 1.1466 - accuracy: 0.6757\n",
            "312/469 [==================>...........] - ETA: 1s - loss: 1.1302 - accuracy: 0.6794\n",
            "324/469 [===================>..........] - ETA: 1s - loss: 1.1152 - accuracy: 0.6824\n",
            "330/469 [====================>.........] - ETA: 1s - loss: 1.1068 - accuracy: 0.6845\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 1.0979 - accuracy: 0.6865\n",
            "342/469 [====================>.........] - ETA: 1s - loss: 1.0897 - accuracy: 0.6889\n",
            "353/469 [=====================>........] - ETA: 1s - loss: 1.0798 - accuracy: 0.6909\n",
            "365/469 [======================>.......] - ETA: 1s - loss: 1.0685 - accuracy: 0.6941\n",
            "377/469 [=======================>......] - ETA: 0s - loss: 1.0561 - accuracy: 0.6970\n",
            "389/469 [=======================>......] - ETA: 0s - loss: 1.0480 - accuracy: 0.6990\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 1.0403 - accuracy: 0.7009\n",
            "407/469 [=========================>....] - ETA: 0s - loss: 1.0356 - accuracy: 0.7019\n",
            "419/469 [=========================>....] - ETA: 0s - loss: 1.0282 - accuracy: 0.7039\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 1.0215 - accuracy: 0.7055\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 1.0137 - accuracy: 0.7074\n",
            "454/469 [============================>.] - ETA: 0s - loss: 1.0064 - accuracy: 0.7088\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.9990 - accuracy: 0.7106\n",
            "469/469 [==============================] - 6s 11ms/step - loss: 0.9973 - accuracy: 0.7110 - val_loss: 0.3906 - val_accuracy: 0.8828\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 2/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.7129 - accuracy: 0.8047\n",
            " 13/469 [..............................] - ETA: 4s - loss: 0.7078 - accuracy: 0.7837\n",
            " 25/469 [>.............................] - ETA: 4s - loss: 0.7025 - accuracy: 0.7825\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.6905 - accuracy: 0.7855\n",
            " 42/469 [=>............................] - ETA: 4s - loss: 0.6806 - accuracy: 0.7842\n",
            " 54/469 [==>...........................] - ETA: 4s - loss: 0.6817 - accuracy: 0.7824\n",
            " 66/469 [===>..........................] - ETA: 3s - loss: 0.6732 - accuracy: 0.7843\n",
            " 77/469 [===>..........................] - ETA: 3s - loss: 0.6704 - accuracy: 0.7860\n",
            " 89/469 [====>.........................] - ETA: 3s - loss: 0.6706 - accuracy: 0.7861\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.6767 - accuracy: 0.7855\n",
            "107/469 [=====>........................] - ETA: 3s - loss: 0.6795 - accuracy: 0.7851\n",
            "119/469 [======>.......................] - ETA: 3s - loss: 0.6820 - accuracy: 0.7848\n",
            "131/469 [=======>......................] - ETA: 3s - loss: 0.6810 - accuracy: 0.7850\n",
            "143/469 [========>.....................] - ETA: 3s - loss: 0.6854 - accuracy: 0.7840\n",
            "155/469 [========>.....................] - ETA: 3s - loss: 0.6863 - accuracy: 0.7836\n",
            "161/469 [=========>....................] - ETA: 3s - loss: 0.6838 - accuracy: 0.7835\n",
            "167/469 [=========>....................] - ETA: 2s - loss: 0.6834 - accuracy: 0.7833\n",
            "173/469 [==========>...................] - ETA: 2s - loss: 0.6815 - accuracy: 0.7838\n",
            "185/469 [==========>...................] - ETA: 2s - loss: 0.6791 - accuracy: 0.7842\n",
            "197/469 [===========>..................] - ETA: 2s - loss: 0.6782 - accuracy: 0.7837\n",
            "209/469 [============>.................] - ETA: 2s - loss: 0.6804 - accuracy: 0.7838\n",
            "221/469 [=============>................] - ETA: 2s - loss: 0.6817 - accuracy: 0.7844\n",
            "233/469 [=============>................] - ETA: 2s - loss: 0.6829 - accuracy: 0.7833\n",
            "239/469 [==============>...............] - ETA: 2s - loss: 0.6835 - accuracy: 0.7832\n",
            "251/469 [===============>..............] - ETA: 2s - loss: 0.6820 - accuracy: 0.7844\n",
            "262/469 [===============>..............] - ETA: 2s - loss: 0.6780 - accuracy: 0.7857\n",
            "274/469 [================>.............] - ETA: 1s - loss: 0.6780 - accuracy: 0.7858\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.6751 - accuracy: 0.7865\n",
            "298/469 [==================>...........] - ETA: 1s - loss: 0.6758 - accuracy: 0.7866\n",
            "310/469 [==================>...........] - ETA: 1s - loss: 0.6761 - accuracy: 0.7867\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.6760 - accuracy: 0.7867\n",
            "322/469 [===================>..........] - ETA: 1s - loss: 0.6757 - accuracy: 0.7865\n",
            "328/469 [===================>..........] - ETA: 1s - loss: 0.6747 - accuracy: 0.7870\n",
            "340/469 [====================>.........] - ETA: 1s - loss: 0.6750 - accuracy: 0.7871\n",
            "352/469 [=====================>........] - ETA: 1s - loss: 0.6730 - accuracy: 0.7883\n",
            "364/469 [======================>.......] - ETA: 1s - loss: 0.6734 - accuracy: 0.7880\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.6736 - accuracy: 0.7884\n",
            "382/469 [=======================>......] - ETA: 0s - loss: 0.6743 - accuracy: 0.7881\n",
            "388/469 [=======================>......] - ETA: 0s - loss: 0.6755 - accuracy: 0.7880\n",
            "394/469 [========================>.....] - ETA: 0s - loss: 0.6748 - accuracy: 0.7883\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.6756 - accuracy: 0.7884\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.6763 - accuracy: 0.7879\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.6753 - accuracy: 0.7882\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.6751 - accuracy: 0.7884\n",
            "454/469 [============================>.] - ETA: 0s - loss: 0.6750 - accuracy: 0.7886\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.6750 - accuracy: 0.7886\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.6753 - accuracy: 0.7886\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6770 - accuracy: 0.7885 - val_loss: 0.3305 - val_accuracy: 0.9015\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 3/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.6100 - accuracy: 0.8047\n",
            "  7/469 [..............................] - ETA: 4s - loss: 0.7949 - accuracy: 0.7533\n",
            " 19/469 [>.............................] - ETA: 4s - loss: 0.6821 - accuracy: 0.7850\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.6745 - accuracy: 0.7956\n",
            " 43/469 [=>............................] - ETA: 4s - loss: 0.6839 - accuracy: 0.7927\n",
            " 55/469 [==>...........................] - ETA: 4s - loss: 0.6907 - accuracy: 0.7889\n",
            " 61/469 [==>...........................] - ETA: 3s - loss: 0.6898 - accuracy: 0.7880\n",
            " 67/469 [===>..........................] - ETA: 3s - loss: 0.6958 - accuracy: 0.7870\n",
            " 73/469 [===>..........................] - ETA: 3s - loss: 0.6919 - accuracy: 0.7867\n",
            " 85/469 [====>.........................] - ETA: 3s - loss: 0.6932 - accuracy: 0.7875\n",
            " 97/469 [=====>........................] - ETA: 3s - loss: 0.6886 - accuracy: 0.7891\n",
            "109/469 [=====>........................] - ETA: 3s - loss: 0.6854 - accuracy: 0.7906\n",
            "121/469 [======>.......................] - ETA: 3s - loss: 0.6865 - accuracy: 0.7896\n",
            "127/469 [=======>......................] - ETA: 3s - loss: 0.6827 - accuracy: 0.7902\n",
            "133/469 [=======>......................] - ETA: 3s - loss: 0.6855 - accuracy: 0.7901\n",
            "139/469 [=======>......................] - ETA: 3s - loss: 0.6836 - accuracy: 0.7901\n",
            "151/469 [========>.....................] - ETA: 3s - loss: 0.6825 - accuracy: 0.7898\n",
            "163/469 [=========>....................] - ETA: 2s - loss: 0.6783 - accuracy: 0.7903\n",
            "175/469 [==========>...................] - ETA: 2s - loss: 0.6792 - accuracy: 0.7904\n",
            "187/469 [==========>...................] - ETA: 2s - loss: 0.6798 - accuracy: 0.7908\n",
            "199/469 [===========>..................] - ETA: 2s - loss: 0.6774 - accuracy: 0.7917\n",
            "205/469 [============>.................] - ETA: 2s - loss: 0.6752 - accuracy: 0.7921\n",
            "217/469 [============>.................] - ETA: 2s - loss: 0.6739 - accuracy: 0.7924\n",
            "229/469 [=============>................] - ETA: 2s - loss: 0.6744 - accuracy: 0.7923\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.6761 - accuracy: 0.7918\n",
            "252/469 [===============>..............] - ETA: 2s - loss: 0.6783 - accuracy: 0.7911\n",
            "263/469 [===============>..............] - ETA: 2s - loss: 0.6777 - accuracy: 0.7910\n",
            "274/469 [================>.............] - ETA: 1s - loss: 0.6771 - accuracy: 0.7909\n",
            "284/469 [=================>............] - ETA: 1s - loss: 0.6740 - accuracy: 0.7913\n",
            "295/469 [=================>............] - ETA: 1s - loss: 0.6753 - accuracy: 0.7910\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.6735 - accuracy: 0.7913\n",
            "317/469 [===================>..........] - ETA: 1s - loss: 0.6771 - accuracy: 0.7898\n",
            "327/469 [===================>..........] - ETA: 1s - loss: 0.6752 - accuracy: 0.7903\n",
            "333/469 [====================>.........] - ETA: 1s - loss: 0.6739 - accuracy: 0.7906\n",
            "339/469 [====================>.........] - ETA: 1s - loss: 0.6726 - accuracy: 0.7909\n",
            "345/469 [=====================>........] - ETA: 1s - loss: 0.6725 - accuracy: 0.7911\n",
            "357/469 [=====================>........] - ETA: 1s - loss: 0.6722 - accuracy: 0.7909\n",
            "369/469 [======================>.......] - ETA: 0s - loss: 0.6708 - accuracy: 0.7912\n",
            "381/469 [=======================>......] - ETA: 0s - loss: 0.6705 - accuracy: 0.7912\n",
            "393/469 [========================>.....] - ETA: 0s - loss: 0.6698 - accuracy: 0.7918\n",
            "399/469 [========================>.....] - ETA: 0s - loss: 0.6694 - accuracy: 0.7915\n",
            "405/469 [========================>.....] - ETA: 0s - loss: 0.6708 - accuracy: 0.7911\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.6696 - accuracy: 0.7916\n",
            "423/469 [==========================>...] - ETA: 0s - loss: 0.6699 - accuracy: 0.7915\n",
            "435/469 [==========================>...] - ETA: 0s - loss: 0.6680 - accuracy: 0.7920\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.6688 - accuracy: 0.7921\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.6671 - accuracy: 0.7924\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.6656 - accuracy: 0.7929\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6658 - accuracy: 0.7930 - val_loss: 0.3305 - val_accuracy: 0.9054\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 4/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.7066 - accuracy: 0.8047\n",
            " 13/469 [..............................] - ETA: 4s - loss: 0.7013 - accuracy: 0.7945\n",
            " 25/469 [>.............................] - ETA: 4s - loss: 0.6806 - accuracy: 0.7972\n",
            " 37/469 [=>............................] - ETA: 4s - loss: 0.6713 - accuracy: 0.7994\n",
            " 43/469 [=>............................] - ETA: 4s - loss: 0.6630 - accuracy: 0.8009\n",
            " 49/469 [==>...........................] - ETA: 4s - loss: 0.6674 - accuracy: 0.8005\n",
            " 55/469 [==>...........................] - ETA: 4s - loss: 0.6630 - accuracy: 0.8000\n",
            " 66/469 [===>..........................] - ETA: 3s - loss: 0.6560 - accuracy: 0.8000\n",
            " 78/469 [===>..........................] - ETA: 3s - loss: 0.6546 - accuracy: 0.8006\n",
            " 90/469 [====>.........................] - ETA: 3s - loss: 0.6616 - accuracy: 0.7990\n",
            "102/469 [=====>........................] - ETA: 3s - loss: 0.6607 - accuracy: 0.7978\n",
            "114/469 [======>.......................] - ETA: 3s - loss: 0.6613 - accuracy: 0.7989\n",
            "120/469 [======>.......................] - ETA: 3s - loss: 0.6641 - accuracy: 0.7988\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.6650 - accuracy: 0.7980\n",
            "132/469 [=======>......................] - ETA: 3s - loss: 0.6625 - accuracy: 0.7982\n",
            "144/469 [========>.....................] - ETA: 3s - loss: 0.6604 - accuracy: 0.7993\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.6578 - accuracy: 0.7989\n",
            "168/469 [=========>....................] - ETA: 2s - loss: 0.6572 - accuracy: 0.7993\n",
            "180/469 [==========>...................] - ETA: 2s - loss: 0.6536 - accuracy: 0.8000\n",
            "192/469 [===========>..................] - ETA: 2s - loss: 0.6489 - accuracy: 0.8009\n",
            "198/469 [===========>..................] - ETA: 2s - loss: 0.6479 - accuracy: 0.8010\n",
            "204/469 [============>.................] - ETA: 2s - loss: 0.6478 - accuracy: 0.8008\n",
            "210/469 [============>.................] - ETA: 2s - loss: 0.6478 - accuracy: 0.8005\n",
            "222/469 [=============>................] - ETA: 2s - loss: 0.6469 - accuracy: 0.8010\n",
            "234/469 [=============>................] - ETA: 2s - loss: 0.6475 - accuracy: 0.8004\n",
            "246/469 [==============>...............] - ETA: 2s - loss: 0.6493 - accuracy: 0.8001\n",
            "258/469 [===============>..............] - ETA: 2s - loss: 0.6475 - accuracy: 0.8006\n",
            "264/469 [===============>..............] - ETA: 1s - loss: 0.6460 - accuracy: 0.8011\n",
            "270/469 [================>.............] - ETA: 1s - loss: 0.6450 - accuracy: 0.8014\n",
            "276/469 [================>.............] - ETA: 1s - loss: 0.6425 - accuracy: 0.8016\n",
            "288/469 [=================>............] - ETA: 1s - loss: 0.6413 - accuracy: 0.8013\n",
            "300/469 [==================>...........] - ETA: 1s - loss: 0.6409 - accuracy: 0.8008\n",
            "312/469 [==================>...........] - ETA: 1s - loss: 0.6423 - accuracy: 0.8004\n",
            "324/469 [===================>..........] - ETA: 1s - loss: 0.6440 - accuracy: 0.8001\n",
            "330/469 [====================>.........] - ETA: 1s - loss: 0.6438 - accuracy: 0.8003\n",
            "336/469 [====================>.........] - ETA: 1s - loss: 0.6434 - accuracy: 0.8004\n",
            "342/469 [====================>.........] - ETA: 1s - loss: 0.6442 - accuracy: 0.8003\n",
            "347/469 [=====================>........] - ETA: 1s - loss: 0.6448 - accuracy: 0.8000\n",
            "353/469 [=====================>........] - ETA: 1s - loss: 0.6459 - accuracy: 0.7995\n",
            "358/469 [=====================>........] - ETA: 1s - loss: 0.6474 - accuracy: 0.7994\n",
            "364/469 [======================>.......] - ETA: 1s - loss: 0.6483 - accuracy: 0.7992\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.6464 - accuracy: 0.8001\n",
            "388/469 [=======================>......] - ETA: 0s - loss: 0.6457 - accuracy: 0.8001\n",
            "400/469 [========================>.....] - ETA: 0s - loss: 0.6445 - accuracy: 0.8003\n",
            "412/469 [=========================>....] - ETA: 0s - loss: 0.6443 - accuracy: 0.8001\n",
            "418/469 [=========================>....] - ETA: 0s - loss: 0.6439 - accuracy: 0.8000\n",
            "424/469 [==========================>...] - ETA: 0s - loss: 0.6432 - accuracy: 0.8002\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.6433 - accuracy: 0.8000\n",
            "442/469 [===========================>..] - ETA: 0s - loss: 0.6421 - accuracy: 0.8000\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.6439 - accuracy: 0.7995\n",
            "465/469 [============================>.] - ETA: 0s - loss: 0.6437 - accuracy: 0.7997\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6436 - accuracy: 0.7996 - val_loss: 0.3285 - val_accuracy: 0.9069\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 5/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.7376 - accuracy: 0.7734\n",
            " 12/469 [..............................] - ETA: 4s - loss: 0.6632 - accuracy: 0.7839\n",
            " 24/469 [>.............................] - ETA: 4s - loss: 0.6536 - accuracy: 0.7884\n",
            " 35/469 [=>............................] - ETA: 4s - loss: 0.6463 - accuracy: 0.7897\n",
            " 46/469 [=>............................] - ETA: 4s - loss: 0.6464 - accuracy: 0.7940\n",
            " 52/469 [==>...........................] - ETA: 4s - loss: 0.6533 - accuracy: 0.7913\n",
            " 64/469 [===>..........................] - ETA: 3s - loss: 0.6486 - accuracy: 0.7952\n",
            " 76/469 [===>..........................] - ETA: 3s - loss: 0.6562 - accuracy: 0.7930\n",
            " 88/469 [====>.........................] - ETA: 3s - loss: 0.6593 - accuracy: 0.7917\n",
            " 99/469 [=====>........................] - ETA: 3s - loss: 0.6558 - accuracy: 0.7933\n",
            "111/469 [======>.......................] - ETA: 3s - loss: 0.6526 - accuracy: 0.7942\n",
            "117/469 [======>.......................] - ETA: 3s - loss: 0.6540 - accuracy: 0.7935\n",
            "129/469 [=======>......................] - ETA: 3s - loss: 0.6518 - accuracy: 0.7940\n",
            "141/469 [========>.....................] - ETA: 3s - loss: 0.6537 - accuracy: 0.7921\n",
            "153/469 [========>.....................] - ETA: 3s - loss: 0.6512 - accuracy: 0.7927\n",
            "165/469 [=========>....................] - ETA: 2s - loss: 0.6516 - accuracy: 0.7926\n",
            "177/469 [==========>...................] - ETA: 2s - loss: 0.6517 - accuracy: 0.7929\n",
            "183/469 [==========>...................] - ETA: 2s - loss: 0.6512 - accuracy: 0.7930\n",
            "189/469 [===========>..................] - ETA: 2s - loss: 0.6493 - accuracy: 0.7932\n",
            "195/469 [===========>..................] - ETA: 2s - loss: 0.6497 - accuracy: 0.7929\n",
            "207/469 [============>.................] - ETA: 2s - loss: 0.6487 - accuracy: 0.7937\n",
            "219/469 [=============>................] - ETA: 2s - loss: 0.6464 - accuracy: 0.7939\n",
            "231/469 [=============>................] - ETA: 2s - loss: 0.6471 - accuracy: 0.7939\n",
            "243/469 [==============>...............] - ETA: 2s - loss: 0.6452 - accuracy: 0.7949\n",
            "249/469 [==============>...............] - ETA: 2s - loss: 0.6442 - accuracy: 0.7951\n",
            "255/469 [===============>..............] - ETA: 2s - loss: 0.6421 - accuracy: 0.7956\n",
            "261/469 [===============>..............] - ETA: 2s - loss: 0.6424 - accuracy: 0.7955\n",
            "266/469 [================>.............] - ETA: 1s - loss: 0.6406 - accuracy: 0.7961\n",
            "272/469 [================>.............] - ETA: 1s - loss: 0.6404 - accuracy: 0.7964\n",
            "284/469 [=================>............] - ETA: 1s - loss: 0.6395 - accuracy: 0.7976\n",
            "296/469 [=================>............] - ETA: 1s - loss: 0.6415 - accuracy: 0.7971\n",
            "308/469 [==================>...........] - ETA: 1s - loss: 0.6451 - accuracy: 0.7958\n",
            "320/469 [===================>..........] - ETA: 1s - loss: 0.6447 - accuracy: 0.7962\n",
            "332/469 [====================>.........] - ETA: 1s - loss: 0.6460 - accuracy: 0.7964\n",
            "344/469 [=====================>........] - ETA: 1s - loss: 0.6479 - accuracy: 0.7957\n",
            "350/469 [=====================>........] - ETA: 1s - loss: 0.6467 - accuracy: 0.7961\n",
            "362/469 [======================>.......] - ETA: 1s - loss: 0.6464 - accuracy: 0.7962\n",
            "374/469 [======================>.......] - ETA: 0s - loss: 0.6440 - accuracy: 0.7969\n",
            "386/469 [=======================>......] - ETA: 0s - loss: 0.6423 - accuracy: 0.7971\n",
            "398/469 [========================>.....] - ETA: 0s - loss: 0.6420 - accuracy: 0.7976\n",
            "410/469 [=========================>....] - ETA: 0s - loss: 0.6426 - accuracy: 0.7979\n",
            "416/469 [=========================>....] - ETA: 0s - loss: 0.6423 - accuracy: 0.7980\n",
            "422/469 [=========================>....] - ETA: 0s - loss: 0.6427 - accuracy: 0.7982\n",
            "428/469 [==========================>...] - ETA: 0s - loss: 0.6424 - accuracy: 0.7983\n",
            "440/469 [===========================>..] - ETA: 0s - loss: 0.6426 - accuracy: 0.7981\n",
            "452/469 [===========================>..] - ETA: 0s - loss: 0.6425 - accuracy: 0.7981\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.6430 - accuracy: 0.7978\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6433 - accuracy: 0.7976 - val_loss: 0.3262 - val_accuracy: 0.9071\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 6/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.7919 - accuracy: 0.8047\n",
            " 13/469 [..............................] - ETA: 4s - loss: 0.7352 - accuracy: 0.7903\n",
            " 19/469 [>.............................] - ETA: 4s - loss: 0.7085 - accuracy: 0.7981\n",
            " 24/469 [>.............................] - ETA: 4s - loss: 0.6964 - accuracy: 0.8011\n",
            " 34/469 [=>............................] - ETA: 4s - loss: 0.7020 - accuracy: 0.7953\n",
            " 45/469 [=>............................] - ETA: 4s - loss: 0.6955 - accuracy: 0.7932\n",
            " 56/469 [==>...........................] - ETA: 4s - loss: 0.6761 - accuracy: 0.7974\n",
            " 66/469 [===>..........................] - ETA: 4s - loss: 0.6663 - accuracy: 0.7991\n",
            " 76/469 [===>..........................] - ETA: 3s - loss: 0.6573 - accuracy: 0.8017\n",
            " 86/469 [====>.........................] - ETA: 3s - loss: 0.6607 - accuracy: 0.8009\n",
            " 96/469 [=====>........................] - ETA: 3s - loss: 0.6628 - accuracy: 0.7996\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.6618 - accuracy: 0.8006\n",
            "116/469 [======>.......................] - ETA: 3s - loss: 0.6610 - accuracy: 0.7998\n",
            "127/469 [=======>......................] - ETA: 3s - loss: 0.6548 - accuracy: 0.8011\n",
            "138/469 [=======>......................] - ETA: 3s - loss: 0.6547 - accuracy: 0.8008\n",
            "149/469 [========>.....................] - ETA: 3s - loss: 0.6547 - accuracy: 0.8020\n",
            "161/469 [=========>....................] - ETA: 3s - loss: 0.6471 - accuracy: 0.8026\n",
            "172/469 [==========>...................] - ETA: 2s - loss: 0.6444 - accuracy: 0.8039\n",
            "182/469 [==========>...................] - ETA: 2s - loss: 0.6421 - accuracy: 0.8049\n",
            "193/469 [===========>..................] - ETA: 2s - loss: 0.6425 - accuracy: 0.8046\n",
            "204/469 [============>.................] - ETA: 2s - loss: 0.6418 - accuracy: 0.8047\n",
            "210/469 [============>.................] - ETA: 2s - loss: 0.6434 - accuracy: 0.8043\n",
            "215/469 [============>.................] - ETA: 2s - loss: 0.6442 - accuracy: 0.8037\n",
            "221/469 [=============>................] - ETA: 2s - loss: 0.6463 - accuracy: 0.8031\n",
            "233/469 [=============>................] - ETA: 2s - loss: 0.6482 - accuracy: 0.8024\n",
            "245/469 [==============>...............] - ETA: 2s - loss: 0.6484 - accuracy: 0.8024\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.6488 - accuracy: 0.8020\n",
            "266/469 [================>.............] - ETA: 2s - loss: 0.6514 - accuracy: 0.8005\n",
            "277/469 [================>.............] - ETA: 1s - loss: 0.6481 - accuracy: 0.8014\n",
            "289/469 [=================>............] - ETA: 1s - loss: 0.6458 - accuracy: 0.8018\n",
            "300/469 [==================>...........] - ETA: 1s - loss: 0.6470 - accuracy: 0.8014\n",
            "311/469 [==================>...........] - ETA: 1s - loss: 0.6475 - accuracy: 0.8010\n",
            "322/469 [===================>..........] - ETA: 1s - loss: 0.6476 - accuracy: 0.8010\n",
            "333/469 [====================>.........] - ETA: 1s - loss: 0.6462 - accuracy: 0.8009\n",
            "344/469 [=====================>........] - ETA: 1s - loss: 0.6470 - accuracy: 0.8009\n",
            "350/469 [=====================>........] - ETA: 1s - loss: 0.6484 - accuracy: 0.8007\n",
            "355/469 [=====================>........] - ETA: 1s - loss: 0.6481 - accuracy: 0.8009\n",
            "361/469 [======================>.......] - ETA: 1s - loss: 0.6491 - accuracy: 0.8006\n",
            "373/469 [======================>.......] - ETA: 0s - loss: 0.6489 - accuracy: 0.8003\n",
            "385/469 [=======================>......] - ETA: 0s - loss: 0.6485 - accuracy: 0.8005\n",
            "397/469 [========================>.....] - ETA: 0s - loss: 0.6466 - accuracy: 0.8008\n",
            "409/469 [=========================>....] - ETA: 0s - loss: 0.6457 - accuracy: 0.8010\n",
            "419/469 [=========================>....] - ETA: 0s - loss: 0.6458 - accuracy: 0.8007\n",
            "429/469 [==========================>...] - ETA: 0s - loss: 0.6449 - accuracy: 0.8005\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.6459 - accuracy: 0.8005\n",
            "451/469 [===========================>..] - ETA: 0s - loss: 0.6473 - accuracy: 0.8003\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.6474 - accuracy: 0.8001\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.6476 - accuracy: 0.8002\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.6476 - accuracy: 0.8002 - val_loss: 0.3365 - val_accuracy: 0.9040\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 7/12\n",
            "  7/469 [..............................] - ETA: 4s - loss: 0.7415 - accuracy: 0.7879\n",
            " 19/469 [>.............................] - ETA: 4s - loss: 0.7875 - accuracy: 0.7763\n",
            " 30/469 [>.............................] - ETA: 4s - loss: 0.7773 - accuracy: 0.7776\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.7705 - accuracy: 0.7752\n",
            " 41/469 [=>............................] - ETA: 4s - loss: 0.7562 - accuracy: 0.7765\n",
            " 47/469 [==>...........................] - ETA: 4s - loss: 0.7443 - accuracy: 0.7789\n",
            " 59/469 [==>...........................] - ETA: 4s - loss: 0.7187 - accuracy: 0.7831\n",
            " 71/469 [===>..........................] - ETA: 3s - loss: 0.7110 - accuracy: 0.7815\n",
            " 83/469 [====>.........................] - ETA: 3s - loss: 0.7029 - accuracy: 0.7829\n",
            " 94/469 [=====>........................] - ETA: 3s - loss: 0.6974 - accuracy: 0.7838\n",
            "106/469 [=====>........................] - ETA: 3s - loss: 0.6889 - accuracy: 0.7850\n",
            "112/469 [======>.......................] - ETA: 3s - loss: 0.6883 - accuracy: 0.7859\n",
            "118/469 [======>.......................] - ETA: 3s - loss: 0.6900 - accuracy: 0.7858\n",
            "124/469 [======>.......................] - ETA: 3s - loss: 0.6882 - accuracy: 0.7855\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.6826 - accuracy: 0.7878\n",
            "148/469 [========>.....................] - ETA: 3s - loss: 0.6806 - accuracy: 0.7887\n",
            "160/469 [=========>....................] - ETA: 3s - loss: 0.6743 - accuracy: 0.7898\n",
            "172/469 [==========>...................] - ETA: 2s - loss: 0.6695 - accuracy: 0.7919\n",
            "182/469 [==========>...................] - ETA: 2s - loss: 0.6692 - accuracy: 0.7915\n",
            "192/469 [===========>..................] - ETA: 2s - loss: 0.6680 - accuracy: 0.7922\n",
            "203/469 [===========>..................] - ETA: 2s - loss: 0.6705 - accuracy: 0.7927\n",
            "214/469 [============>.................] - ETA: 2s - loss: 0.6660 - accuracy: 0.7936\n",
            "226/469 [=============>................] - ETA: 2s - loss: 0.6641 - accuracy: 0.7942\n",
            "232/469 [=============>................] - ETA: 2s - loss: 0.6604 - accuracy: 0.7954\n",
            "237/469 [==============>...............] - ETA: 2s - loss: 0.6591 - accuracy: 0.7957\n",
            "247/469 [==============>...............] - ETA: 2s - loss: 0.6599 - accuracy: 0.7948\n",
            "253/469 [===============>..............] - ETA: 2s - loss: 0.6587 - accuracy: 0.7952\n",
            "265/469 [===============>..............] - ETA: 2s - loss: 0.6609 - accuracy: 0.7948\n",
            "276/469 [================>.............] - ETA: 1s - loss: 0.6625 - accuracy: 0.7946\n",
            "288/469 [=================>............] - ETA: 1s - loss: 0.6622 - accuracy: 0.7948\n",
            "298/469 [==================>...........] - ETA: 1s - loss: 0.6630 - accuracy: 0.7942\n",
            "308/469 [==================>...........] - ETA: 1s - loss: 0.6628 - accuracy: 0.7942\n",
            "318/469 [===================>..........] - ETA: 1s - loss: 0.6627 - accuracy: 0.7942\n",
            "330/469 [====================>.........] - ETA: 1s - loss: 0.6580 - accuracy: 0.7949\n",
            "342/469 [====================>.........] - ETA: 1s - loss: 0.6546 - accuracy: 0.7963\n",
            "354/469 [=====================>........] - ETA: 1s - loss: 0.6548 - accuracy: 0.7969\n",
            "365/469 [======================>.......] - ETA: 1s - loss: 0.6601 - accuracy: 0.7955\n",
            "371/469 [======================>.......] - ETA: 0s - loss: 0.6612 - accuracy: 0.7953\n",
            "377/469 [=======================>......] - ETA: 0s - loss: 0.6602 - accuracy: 0.7954\n",
            "383/469 [=======================>......] - ETA: 0s - loss: 0.6599 - accuracy: 0.7956\n",
            "395/469 [========================>.....] - ETA: 0s - loss: 0.6609 - accuracy: 0.7952\n",
            "407/469 [=========================>....] - ETA: 0s - loss: 0.6624 - accuracy: 0.7951\n",
            "419/469 [=========================>....] - ETA: 0s - loss: 0.6624 - accuracy: 0.7954\n",
            "430/469 [==========================>...] - ETA: 0s - loss: 0.6622 - accuracy: 0.7953\n",
            "441/469 [===========================>..] - ETA: 0s - loss: 0.6612 - accuracy: 0.7957\n",
            "447/469 [===========================>..] - ETA: 0s - loss: 0.6614 - accuracy: 0.7957\n",
            "453/469 [===========================>..] - ETA: 0s - loss: 0.6610 - accuracy: 0.7956\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.6610 - accuracy: 0.7956\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.6610 - accuracy: 0.7958\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6612 - accuracy: 0.7956 - val_loss: 0.3016 - val_accuracy: 0.9097\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 8/12\n",
            "  7/469 [..............................] - ETA: 4s - loss: 0.6071 - accuracy: 0.8002\n",
            " 19/469 [>.............................] - ETA: 4s - loss: 0.7014 - accuracy: 0.7833\n",
            " 25/469 [>.............................] - ETA: 4s - loss: 0.6800 - accuracy: 0.7881\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.6796 - accuracy: 0.7860\n",
            " 41/469 [=>............................] - ETA: 4s - loss: 0.6664 - accuracy: 0.7913\n",
            " 47/469 [==>...........................] - ETA: 4s - loss: 0.6674 - accuracy: 0.7919\n",
            " 53/469 [==>...........................] - ETA: 4s - loss: 0.6598 - accuracy: 0.7932\n",
            " 59/469 [==>...........................] - ETA: 4s - loss: 0.6595 - accuracy: 0.7936\n",
            " 71/469 [===>..........................] - ETA: 3s - loss: 0.6565 - accuracy: 0.7950\n",
            " 83/469 [====>.........................] - ETA: 3s - loss: 0.6490 - accuracy: 0.7950\n",
            " 95/469 [=====>........................] - ETA: 3s - loss: 0.6436 - accuracy: 0.7939\n",
            "107/469 [=====>........................] - ETA: 3s - loss: 0.6405 - accuracy: 0.7959\n",
            "113/469 [======>.......................] - ETA: 3s - loss: 0.6478 - accuracy: 0.7956\n",
            "119/469 [======>.......................] - ETA: 3s - loss: 0.6472 - accuracy: 0.7962\n",
            "125/469 [======>.......................] - ETA: 3s - loss: 0.6456 - accuracy: 0.7971\n",
            "137/469 [=======>......................] - ETA: 3s - loss: 0.6445 - accuracy: 0.7964\n",
            "148/469 [========>.....................] - ETA: 3s - loss: 0.6498 - accuracy: 0.7956\n",
            "159/469 [=========>....................] - ETA: 3s - loss: 0.6499 - accuracy: 0.7946\n",
            "171/469 [=========>....................] - ETA: 2s - loss: 0.6509 - accuracy: 0.7942\n",
            "183/469 [==========>...................] - ETA: 2s - loss: 0.6542 - accuracy: 0.7940\n",
            "195/469 [===========>..................] - ETA: 2s - loss: 0.6504 - accuracy: 0.7945\n",
            "201/469 [===========>..................] - ETA: 2s - loss: 0.6478 - accuracy: 0.7954\n",
            "213/469 [============>.................] - ETA: 2s - loss: 0.6495 - accuracy: 0.7950\n",
            "225/469 [=============>................] - ETA: 2s - loss: 0.6493 - accuracy: 0.7949\n",
            "237/469 [==============>...............] - ETA: 2s - loss: 0.6510 - accuracy: 0.7949\n",
            "249/469 [==============>...............] - ETA: 2s - loss: 0.6513 - accuracy: 0.7947\n",
            "260/469 [===============>..............] - ETA: 2s - loss: 0.6495 - accuracy: 0.7956\n",
            "272/469 [================>.............] - ETA: 1s - loss: 0.6496 - accuracy: 0.7962\n",
            "278/469 [================>.............] - ETA: 1s - loss: 0.6510 - accuracy: 0.7960\n",
            "290/469 [=================>............] - ETA: 1s - loss: 0.6519 - accuracy: 0.7965\n",
            "302/469 [==================>...........] - ETA: 1s - loss: 0.6526 - accuracy: 0.7961\n",
            "313/469 [===================>..........] - ETA: 1s - loss: 0.6499 - accuracy: 0.7972\n",
            "325/469 [===================>..........] - ETA: 1s - loss: 0.6507 - accuracy: 0.7967\n",
            "337/469 [====================>.........] - ETA: 1s - loss: 0.6516 - accuracy: 0.7963\n",
            "343/469 [====================>.........] - ETA: 1s - loss: 0.6513 - accuracy: 0.7965\n",
            "349/469 [=====================>........] - ETA: 1s - loss: 0.6514 - accuracy: 0.7966\n",
            "354/469 [=====================>........] - ETA: 1s - loss: 0.6512 - accuracy: 0.7968\n",
            "360/469 [======================>.......] - ETA: 1s - loss: 0.6506 - accuracy: 0.7970\n",
            "366/469 [======================>.......] - ETA: 1s - loss: 0.6494 - accuracy: 0.7972\n",
            "378/469 [=======================>......] - ETA: 0s - loss: 0.6463 - accuracy: 0.7980\n",
            "389/469 [=======================>......] - ETA: 0s - loss: 0.6454 - accuracy: 0.7984\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.6442 - accuracy: 0.7989\n",
            "413/469 [=========================>....] - ETA: 0s - loss: 0.6462 - accuracy: 0.7987\n",
            "425/469 [==========================>...] - ETA: 0s - loss: 0.6485 - accuracy: 0.7985\n",
            "431/469 [==========================>...] - ETA: 0s - loss: 0.6493 - accuracy: 0.7984\n",
            "443/469 [===========================>..] - ETA: 0s - loss: 0.6501 - accuracy: 0.7982\n",
            "455/469 [============================>.] - ETA: 0s - loss: 0.6521 - accuracy: 0.7976\n",
            "467/469 [============================>.] - ETA: 0s - loss: 0.6540 - accuracy: 0.7973\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6544 - accuracy: 0.7972 - val_loss: 0.3593 - val_accuracy: 0.9058\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 9/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.5212 - accuracy: 0.8438\n",
            " 13/469 [..............................] - ETA: 4s - loss: 0.6229 - accuracy: 0.8053\n",
            " 25/469 [>.............................] - ETA: 4s - loss: 0.6358 - accuracy: 0.8019\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.6415 - accuracy: 0.8009\n",
            " 43/469 [=>............................] - ETA: 4s - loss: 0.6280 - accuracy: 0.8052\n",
            " 55/469 [==>...........................] - ETA: 4s - loss: 0.6348 - accuracy: 0.8031\n",
            " 67/469 [===>..........................] - ETA: 3s - loss: 0.6403 - accuracy: 0.8054\n",
            " 79/469 [====>.........................] - ETA: 3s - loss: 0.6504 - accuracy: 0.8047\n",
            " 91/469 [====>.........................] - ETA: 3s - loss: 0.6445 - accuracy: 0.8049\n",
            " 97/469 [=====>........................] - ETA: 3s - loss: 0.6423 - accuracy: 0.8046\n",
            "109/469 [=====>........................] - ETA: 3s - loss: 0.6356 - accuracy: 0.8066\n",
            "121/469 [======>.......................] - ETA: 3s - loss: 0.6291 - accuracy: 0.8082\n",
            "133/469 [=======>......................] - ETA: 3s - loss: 0.6329 - accuracy: 0.8064\n",
            "145/469 [========>.....................] - ETA: 3s - loss: 0.6345 - accuracy: 0.8050\n",
            "151/469 [========>.....................] - ETA: 3s - loss: 0.6355 - accuracy: 0.8048\n",
            "157/469 [=========>....................] - ETA: 3s - loss: 0.6335 - accuracy: 0.8045\n",
            "163/469 [=========>....................] - ETA: 2s - loss: 0.6313 - accuracy: 0.8054\n",
            "175/469 [==========>...................] - ETA: 2s - loss: 0.6363 - accuracy: 0.8053\n",
            "187/469 [==========>...................] - ETA: 2s - loss: 0.6414 - accuracy: 0.8036\n",
            "199/469 [===========>..................] - ETA: 2s - loss: 0.6424 - accuracy: 0.8027\n",
            "211/469 [============>.................] - ETA: 2s - loss: 0.6434 - accuracy: 0.8030\n",
            "223/469 [=============>................] - ETA: 2s - loss: 0.6419 - accuracy: 0.8032\n",
            "229/469 [=============>................] - ETA: 2s - loss: 0.6462 - accuracy: 0.8023\n",
            "241/469 [==============>...............] - ETA: 2s - loss: 0.6478 - accuracy: 0.8009\n",
            "252/469 [===============>..............] - ETA: 2s - loss: 0.6458 - accuracy: 0.8009\n",
            "264/469 [===============>..............] - ETA: 1s - loss: 0.6459 - accuracy: 0.8008\n",
            "275/469 [================>.............] - ETA: 1s - loss: 0.6455 - accuracy: 0.8013\n",
            "287/469 [=================>............] - ETA: 1s - loss: 0.6420 - accuracy: 0.8025\n",
            "293/469 [=================>............] - ETA: 1s - loss: 0.6420 - accuracy: 0.8025\n",
            "299/469 [==================>...........] - ETA: 1s - loss: 0.6404 - accuracy: 0.8028\n",
            "305/469 [==================>...........] - ETA: 1s - loss: 0.6394 - accuracy: 0.8029\n",
            "317/469 [===================>..........] - ETA: 1s - loss: 0.6375 - accuracy: 0.8031\n",
            "327/469 [===================>..........] - ETA: 1s - loss: 0.6375 - accuracy: 0.8028\n",
            "338/469 [====================>.........] - ETA: 1s - loss: 0.6434 - accuracy: 0.8015\n",
            "349/469 [=====================>........] - ETA: 1s - loss: 0.6425 - accuracy: 0.8019\n",
            "360/469 [======================>.......] - ETA: 1s - loss: 0.6419 - accuracy: 0.8021\n",
            "371/469 [======================>.......] - ETA: 0s - loss: 0.6429 - accuracy: 0.8021\n",
            "382/469 [=======================>......] - ETA: 0s - loss: 0.6448 - accuracy: 0.8019\n",
            "394/469 [========================>.....] - ETA: 0s - loss: 0.6476 - accuracy: 0.8015\n",
            "406/469 [========================>.....] - ETA: 0s - loss: 0.6490 - accuracy: 0.8010\n",
            "417/469 [=========================>....] - ETA: 0s - loss: 0.6509 - accuracy: 0.8004\n",
            "428/469 [==========================>...] - ETA: 0s - loss: 0.6530 - accuracy: 0.7996\n",
            "439/469 [===========================>..] - ETA: 0s - loss: 0.6544 - accuracy: 0.7989\n",
            "445/469 [===========================>..] - ETA: 0s - loss: 0.6561 - accuracy: 0.7985\n",
            "457/469 [============================>.] - ETA: 0s - loss: 0.6572 - accuracy: 0.7982\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.7977\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6571 - accuracy: 0.7977 - val_loss: 0.2990 - val_accuracy: 0.9145\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 10/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.7606 - accuracy: 0.8047\n",
            " 12/469 [..............................] - ETA: 4s - loss: 0.6899 - accuracy: 0.8092\n",
            " 24/469 [>.............................] - ETA: 4s - loss: 0.7878 - accuracy: 0.7865\n",
            " 35/469 [=>............................] - ETA: 4s - loss: 0.7709 - accuracy: 0.7801\n",
            " 47/469 [==>...........................] - ETA: 4s - loss: 0.7685 - accuracy: 0.7822\n",
            " 59/469 [==>...........................] - ETA: 4s - loss: 0.7625 - accuracy: 0.7805\n",
            " 65/469 [===>..........................] - ETA: 3s - loss: 0.7535 - accuracy: 0.7815\n",
            " 71/469 [===>..........................] - ETA: 3s - loss: 0.7405 - accuracy: 0.7831\n",
            " 77/469 [===>..........................] - ETA: 3s - loss: 0.7307 - accuracy: 0.7851\n",
            " 89/469 [====>.........................] - ETA: 3s - loss: 0.7165 - accuracy: 0.7861\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.7200 - accuracy: 0.7859\n",
            "113/469 [======>.......................] - ETA: 3s - loss: 0.7126 - accuracy: 0.7871\n",
            "124/469 [======>.......................] - ETA: 3s - loss: 0.7081 - accuracy: 0.7883\n",
            "136/469 [=======>......................] - ETA: 3s - loss: 0.7027 - accuracy: 0.7895\n",
            "142/469 [========>.....................] - ETA: 3s - loss: 0.6983 - accuracy: 0.7903\n",
            "148/469 [========>.....................] - ETA: 3s - loss: 0.6948 - accuracy: 0.7906\n",
            "154/469 [========>.....................] - ETA: 3s - loss: 0.6919 - accuracy: 0.7914\n",
            "166/469 [=========>....................] - ETA: 2s - loss: 0.6849 - accuracy: 0.7924\n",
            "178/469 [==========>...................] - ETA: 2s - loss: 0.6772 - accuracy: 0.7945\n",
            "190/469 [===========>..................] - ETA: 2s - loss: 0.6796 - accuracy: 0.7938\n",
            "202/469 [===========>..................] - ETA: 2s - loss: 0.6803 - accuracy: 0.7929\n",
            "208/469 [============>.................] - ETA: 2s - loss: 0.6794 - accuracy: 0.7927\n",
            "214/469 [============>.................] - ETA: 2s - loss: 0.6789 - accuracy: 0.7926\n",
            "220/469 [=============>................] - ETA: 2s - loss: 0.6759 - accuracy: 0.7934\n",
            "232/469 [=============>................] - ETA: 2s - loss: 0.6724 - accuracy: 0.7939\n",
            "244/469 [==============>...............] - ETA: 2s - loss: 0.6699 - accuracy: 0.7950\n",
            "256/469 [===============>..............] - ETA: 2s - loss: 0.6683 - accuracy: 0.7951\n",
            "268/469 [================>.............] - ETA: 1s - loss: 0.6686 - accuracy: 0.7951\n",
            "280/469 [================>.............] - ETA: 1s - loss: 0.6665 - accuracy: 0.7959\n",
            "286/469 [=================>............] - ETA: 1s - loss: 0.6663 - accuracy: 0.7956\n",
            "298/469 [==================>...........] - ETA: 1s - loss: 0.6662 - accuracy: 0.7957\n",
            "310/469 [==================>...........] - ETA: 1s - loss: 0.6634 - accuracy: 0.7965\n",
            "322/469 [===================>..........] - ETA: 1s - loss: 0.6624 - accuracy: 0.7973\n",
            "334/469 [====================>.........] - ETA: 1s - loss: 0.6601 - accuracy: 0.7978\n",
            "340/469 [====================>.........] - ETA: 1s - loss: 0.6611 - accuracy: 0.7972\n",
            "346/469 [=====================>........] - ETA: 1s - loss: 0.6603 - accuracy: 0.7977\n",
            "352/469 [=====================>........] - ETA: 1s - loss: 0.6612 - accuracy: 0.7976\n",
            "364/469 [======================>.......] - ETA: 1s - loss: 0.6569 - accuracy: 0.7989\n",
            "376/469 [=======================>......] - ETA: 0s - loss: 0.6557 - accuracy: 0.7990\n",
            "387/469 [=======================>......] - ETA: 0s - loss: 0.6575 - accuracy: 0.7985\n",
            "399/469 [========================>.....] - ETA: 0s - loss: 0.6579 - accuracy: 0.7990\n",
            "411/469 [=========================>....] - ETA: 0s - loss: 0.6592 - accuracy: 0.7983\n",
            "417/469 [=========================>....] - ETA: 0s - loss: 0.6610 - accuracy: 0.7982\n",
            "428/469 [==========================>...] - ETA: 0s - loss: 0.6639 - accuracy: 0.7977\n",
            "440/469 [===========================>..] - ETA: 0s - loss: 0.6664 - accuracy: 0.7969\n",
            "452/469 [===========================>..] - ETA: 0s - loss: 0.6694 - accuracy: 0.7968\n",
            "463/469 [============================>.] - ETA: 0s - loss: 0.6701 - accuracy: 0.7966\n",
            "469/469 [==============================] - ETA: 0s - loss: 0.6702 - accuracy: 0.7964\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6702 - accuracy: 0.7964 - val_loss: 0.3253 - val_accuracy: 0.9027\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 11/12\n",
            "  1/469 [..............................] - ETA: 5s - loss: 0.6003 - accuracy: 0.8047\n",
            "  7/469 [..............................] - ETA: 4s - loss: 0.6687 - accuracy: 0.7835\n",
            " 19/469 [>.............................] - ETA: 4s - loss: 0.6214 - accuracy: 0.8084\n",
            " 31/469 [>.............................] - ETA: 4s - loss: 0.6299 - accuracy: 0.8052\n",
            " 43/469 [=>............................] - ETA: 4s - loss: 0.6248 - accuracy: 0.8054\n",
            " 55/469 [==>...........................] - ETA: 4s - loss: 0.6094 - accuracy: 0.8071\n",
            " 66/469 [===>..........................] - ETA: 3s - loss: 0.6277 - accuracy: 0.8045\n",
            " 72/469 [===>..........................] - ETA: 3s - loss: 0.6235 - accuracy: 0.8073\n",
            " 84/469 [====>.........................] - ETA: 3s - loss: 0.6130 - accuracy: 0.8096\n",
            " 96/469 [=====>........................] - ETA: 3s - loss: 0.6104 - accuracy: 0.8088\n",
            "108/469 [=====>........................] - ETA: 3s - loss: 0.6086 - accuracy: 0.8089\n",
            "120/469 [======>.......................] - ETA: 3s - loss: 0.6057 - accuracy: 0.8100\n",
            "126/469 [=======>......................] - ETA: 3s - loss: 0.6046 - accuracy: 0.8098\n",
            "132/469 [=======>......................] - ETA: 3s - loss: 0.6027 - accuracy: 0.8105\n",
            "138/469 [=======>......................] - ETA: 3s - loss: 0.6020 - accuracy: 0.8114\n",
            "150/469 [========>.....................] - ETA: 3s - loss: 0.6056 - accuracy: 0.8113\n",
            "162/469 [=========>....................] - ETA: 3s - loss: 0.6130 - accuracy: 0.8104\n",
            "174/469 [==========>...................] - ETA: 2s - loss: 0.6180 - accuracy: 0.8095\n",
            "186/469 [==========>...................] - ETA: 2s - loss: 0.6202 - accuracy: 0.8084\n",
            "192/469 [===========>..................] - ETA: 2s - loss: 0.6189 - accuracy: 0.8088\n",
            "204/469 [============>.................] - ETA: 2s - loss: 0.6202 - accuracy: 0.8084\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.6192 - accuracy: 0.8084\n",
            "228/469 [=============>................] - ETA: 2s - loss: 0.6177 - accuracy: 0.8093\n",
            "240/469 [==============>...............] - ETA: 2s - loss: 0.6223 - accuracy: 0.8081\n",
            "252/469 [===============>..............] - ETA: 2s - loss: 0.6234 - accuracy: 0.8082\n",
            "258/469 [===============>..............] - ETA: 2s - loss: 0.6247 - accuracy: 0.8071\n",
            "263/469 [===============>..............] - ETA: 2s - loss: 0.6262 - accuracy: 0.8070\n",
            "269/469 [================>.............] - ETA: 1s - loss: 0.6259 - accuracy: 0.8070\n",
            "281/469 [================>.............] - ETA: 1s - loss: 0.6242 - accuracy: 0.8073\n",
            "292/469 [=================>............] - ETA: 1s - loss: 0.6241 - accuracy: 0.8072\n",
            "304/469 [==================>...........] - ETA: 1s - loss: 0.6231 - accuracy: 0.8074\n",
            "316/469 [===================>..........] - ETA: 1s - loss: 0.6242 - accuracy: 0.8072\n",
            "328/469 [===================>..........] - ETA: 1s - loss: 0.6238 - accuracy: 0.8072\n",
            "339/469 [====================>.........] - ETA: 1s - loss: 0.6294 - accuracy: 0.8060\n",
            "345/469 [=====================>........] - ETA: 1s - loss: 0.6310 - accuracy: 0.8058\n",
            "357/469 [=====================>........] - ETA: 1s - loss: 0.6319 - accuracy: 0.8053\n",
            "369/469 [======================>.......] - ETA: 0s - loss: 0.6339 - accuracy: 0.8042\n",
            "380/469 [=======================>......] - ETA: 0s - loss: 0.6344 - accuracy: 0.8040\n",
            "392/469 [========================>.....] - ETA: 0s - loss: 0.6339 - accuracy: 0.8039\n",
            "404/469 [========================>.....] - ETA: 0s - loss: 0.6348 - accuracy: 0.8038\n",
            "410/469 [=========================>....] - ETA: 0s - loss: 0.6352 - accuracy: 0.8036\n",
            "422/469 [=========================>....] - ETA: 0s - loss: 0.6364 - accuracy: 0.8037\n",
            "434/469 [==========================>...] - ETA: 0s - loss: 0.6399 - accuracy: 0.8029\n",
            "446/469 [===========================>..] - ETA: 0s - loss: 0.6399 - accuracy: 0.8028\n",
            "458/469 [============================>.] - ETA: 0s - loss: 0.6412 - accuracy: 0.8024\n",
            "464/469 [============================>.] - ETA: 0s - loss: 0.6422 - accuracy: 0.8019\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6417 - accuracy: 0.8022 - val_loss: 0.3047 - val_accuracy: 0.9159\n",
            "\u001b[36m(train_mnist pid=3302450)\u001b[0m Epoch 12/12\n",
            "  1/469 [..............................] - ETA: 4s - loss: 0.4653 - accuracy: 0.8359\n",
            " 13/469 [..............................] - ETA: 4s - loss: 0.6302 - accuracy: 0.7927\n",
            " 25/469 [>.............................] - ETA: 4s - loss: 0.6179 - accuracy: 0.7978\n",
            " 36/469 [=>............................] - ETA: 4s - loss: 0.6214 - accuracy: 0.7980\n",
            " 42/469 [=>............................] - ETA: 4s - loss: 0.6096 - accuracy: 0.8025\n",
            " 54/469 [==>...........................] - ETA: 4s - loss: 0.5993 - accuracy: 0.8095\n",
            " 66/469 [===>..........................] - ETA: 3s - loss: 0.5905 - accuracy: 0.8111\n",
            " 78/469 [===>..........................] - ETA: 3s - loss: 0.6015 - accuracy: 0.8089\n",
            " 89/469 [====>.........................] - ETA: 3s - loss: 0.5962 - accuracy: 0.8100\n",
            "101/469 [=====>........................] - ETA: 3s - loss: 0.5996 - accuracy: 0.8101\n",
            "107/469 [=====>........................] - ETA: 3s - loss: 0.5977 - accuracy: 0.8107\n",
            "118/469 [======>.......................] - ETA: 3s - loss: 0.5989 - accuracy: 0.8121\n",
            "129/469 [=======>......................] - ETA: 3s - loss: 0.6088 - accuracy: 0.8096\n",
            "134/469 [=======>......................] - ETA: 3s - loss: 0.6129 - accuracy: 0.8094\n",
            "144/469 [========>.....................] - ETA: 3s - loss: 0.6160 - accuracy: 0.8092\n",
            "150/469 [========>.....................] - ETA: 3s - loss: 0.6191 - accuracy: 0.8082\n",
            "156/469 [========>.....................] - ETA: 3s - loss: 0.6214 - accuracy: 0.8076\n",
            "162/469 [=========>....................] - ETA: 3s - loss: 0.6208 - accuracy: 0.8082\n",
            "174/469 [==========>...................] - ETA: 2s - loss: 0.6238 - accuracy: 0.8081\n",
            "186/469 [==========>...................] - ETA: 2s - loss: 0.6274 - accuracy: 0.8074\n",
            "198/469 [===========>..................] - ETA: 2s - loss: 0.6316 - accuracy: 0.8058\n",
            "210/469 [============>.................] - ETA: 2s - loss: 0.6283 - accuracy: 0.8069\n",
            "216/469 [============>.................] - ETA: 2s - loss: 0.6293 - accuracy: 0.8072\n",
            "228/469 [=============>................] - ETA: 2s - loss: 0.6317 - accuracy: 0.8063\n",
            "240/469 [==============>...............] - ETA: 2s - loss: 0.6315 - accuracy: 0.8062\n",
            "252/469 [===============>..............] - ETA: 2s - loss: 0.6318 - accuracy: 0.8058\n",
            "264/469 [===============>..............] - ETA: 2s - loss: 0.6327 - accuracy: 0.8061\n",
            "270/469 [================>.............] - ETA: 1s - loss: 0.6343 - accuracy: 0.8056\n",
            "276/469 [================>.............] - ETA: 1s - loss: 0.6351 - accuracy: 0.8048\n",
            "282/469 [=================>............] - ETA: 1s - loss: 0.6339 - accuracy: 0.8052\n",
            "294/469 [=================>............] - ETA: 1s - loss: 0.6387 - accuracy: 0.8043\n",
            "306/469 [==================>...........] - ETA: 1s - loss: 0.6409 - accuracy: 0.8037\n",
            "317/469 [===================>..........] - ETA: 1s - loss: 0.6414 - accuracy: 0.8033\n",
            "329/469 [====================>.........] - ETA: 1s - loss: 0.6431 - accuracy: 0.8025\n",
            "341/469 [====================>.........] - ETA: 1s - loss: 0.6442 - accuracy: 0.8017\n",
            "347/469 [=====================>........] - ETA: 1s - loss: 0.6449 - accuracy: 0.8016\n",
            "359/469 [=====================>........] - ETA: 1s - loss: 0.6436 - accuracy: 0.8018\n",
            "371/469 [======================>.......] - ETA: 0s - loss: 0.6436 - accuracy: 0.8021\n",
            "383/469 [=======================>......] - ETA: 0s - loss: 0.6432 - accuracy: 0.8018\n",
            "395/469 [========================>.....] - ETA: 0s - loss: 0.6423 - accuracy: 0.8021\n",
            "401/469 [========================>.....] - ETA: 0s - loss: 0.6424 - accuracy: 0.8024\n",
            "407/469 [=========================>....] - ETA: 0s - loss: 0.6426 - accuracy: 0.8024\n",
            "413/469 [=========================>....] - ETA: 0s - loss: 0.6418 - accuracy: 0.8021\n",
            "425/469 [==========================>...] - ETA: 0s - loss: 0.6428 - accuracy: 0.8020\n",
            "437/469 [==========================>...] - ETA: 0s - loss: 0.6437 - accuracy: 0.8019\n",
            "448/469 [===========================>..] - ETA: 0s - loss: 0.6432 - accuracy: 0.8021\n",
            "460/469 [============================>.] - ETA: 0s - loss: 0.6441 - accuracy: 0.8019\n",
            "466/469 [============================>.] - ETA: 0s - loss: 0.6446 - accuracy: 0.8020\n",
            "469/469 [==============================] - 5s 10ms/step - loss: 0.6448 - accuracy: 0.8019 - val_loss: 0.3849 - val_accuracy: 0.8942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-05 02:00:18,216\tINFO tune.py:1047 -- Total run time: 1244.61 seconds (1244.58 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "# Define the modified search space\n",
        "search_space = {\n",
        "    \"conv_filters\": tune.uniform(64,256),\n",
        "    \"lr\": tune.loguniform(0.001, 0.1),\n",
        "    \"batch_size\": tune.uniform(0, 2),  # 0 for 64, 1 for 128, 2 for 256\n",
        "    \"dropout\": tune.uniform(0, 1)\n",
        "}\n",
        "\n",
        "# Initialize Bayesian optimization search algorithm\n",
        "bayesopt_search = BayesOptSearch()\n",
        "\n",
        "# Run the optimization using Bayesian search\n",
        "start_time = time.time()\n",
        "\n",
        "bayes_analysis = tune.run(\n",
        "    train_mnist,\n",
        "    name=\"exp_bayes\",\n",
        "    metric=\"mean_accuracy\",\n",
        "    mode=\"max\",\n",
        "    stop={\"mean_accuracy\": 0.99},\n",
        "    resources_per_trial={\"gpu\": 1},\n",
        "    config=search_space,\n",
        "    search_alg=bayesopt_search,\n",
        "    num_samples=10\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "bayes_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbVBDVsYjbB-"
      },
      "source": [
        "### Hyperband"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9_h4amUcjCx"
      },
      "outputs": [],
      "source": [
        "def train_mnist(config):\n",
        "    num_classes = 10\n",
        "    epochs = 12\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_test = x_train.reshape(-1, 28, 28, 1) / 255.0, x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(filters=config[\"conv_filters\"], kernel_size=(3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(config[\"dropout\"]),\n",
        "        tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=config[\"lr\"]),\n",
        "        metrics=[\"accuracy\"])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.fit(\n",
        "            x_train,\n",
        "            y_train,\n",
        "            batch_size=config[\"batch_size\"],\n",
        "            epochs=1,\n",
        "            verbose=0,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "        # Evaluate the model\n",
        "        i, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "        session.report({\"mean_accuracy\": accuracy})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A9qSVr0UUda",
        "outputId": "3b04fc5a-ad04-4ca9-cae1-b529403b5107"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-05 02:01:18,660\tINFO tune.py:586 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"tuneStatus\">\n",
              "  <div style=\"display: flex;flex-direction: row\">\n",
              "    <div style=\"display: flex;flex-direction: column;\">\n",
              "      <h3>Tune Status</h3>\n",
              "      <table>\n",
              "<tbody>\n",
              "<tr><td>Current time:</td><td>2023-12-05 02:22:43</td></tr>\n",
              "<tr><td>Running for: </td><td>00:21:24.77        </td></tr>\n",
              "<tr><td>Memory:      </td><td>68.9/377.3 GiB     </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "    </div>\n",
              "    <div class=\"vDivider\"></div>\n",
              "    <div class=\"systemInfo\">\n",
              "      <h3>System Info</h3>\n",
              "      Using HyperBand: num_stopped=0 total_brackets=2<br>Round #0:<br>  Bracket(Max Size (n)=5, Milestone (r)=100, completed=100.0%): {TERMINATED: 5} <br>  Bracket(Max Size (n)=3, Milestone (r)=99, completed=100.0%): {TERMINATED: 5} <br>Logical resource usage: 0/48 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:RTX)\n",
              "    </div>\n",
              "    \n",
              "  </div>\n",
              "  <div class=\"hDivider\"></div>\n",
              "  <div class=\"trialStatus\">\n",
              "    <h3>Trial Status</h3>\n",
              "    <table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  conv_filters</th><th style=\"text-align: right;\">  dropout</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">   acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_16ec0_00000</td><td>TERMINATED</td><td>10.32.35.160:3303162</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.540798 </td><td style=\"text-align: right;\">0.031441  </td><td style=\"text-align: right;\">0.9626</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        193.888 </td></tr>\n",
              "<tr><td>train_mnist_16ec0_00001</td><td>TERMINATED</td><td>10.32.35.160:3304567</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.0435369</td><td style=\"text-align: right;\">0.0269051 </td><td style=\"text-align: right;\">0.956 </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         71.8648</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00002</td><td>TERMINATED</td><td>10.32.35.160:3305567</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">0.0750938</td><td style=\"text-align: right;\">0.00175483</td><td style=\"text-align: right;\">0.9867</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        101.749 </td></tr>\n",
              "<tr><td>train_mnist_16ec0_00003</td><td>TERMINATED</td><td>10.32.35.160:3306609</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">0.77175  </td><td style=\"text-align: right;\">0.0833653 </td><td style=\"text-align: right;\">0.1009</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         85.0505</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00004</td><td>TERMINATED</td><td>10.32.35.160:3307587</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.116587 </td><td style=\"text-align: right;\">0.0162096 </td><td style=\"text-align: right;\">0.9836</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        192.796 </td></tr>\n",
              "<tr><td>train_mnist_16ec0_00005</td><td>TERMINATED</td><td>10.32.35.160:3308931</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.77417  </td><td style=\"text-align: right;\">0.00119084</td><td style=\"text-align: right;\">0.9869</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         58.6939</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00006</td><td>TERMINATED</td><td>10.32.35.160:3309817</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">0.660095 </td><td style=\"text-align: right;\">0.0092037 </td><td style=\"text-align: right;\">0.9851</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         50.0868</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00007</td><td>TERMINATED</td><td>10.32.35.160:3310699</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">0.519591 </td><td style=\"text-align: right;\">0.0622316 </td><td style=\"text-align: right;\">0.9577</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         85.1461</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00008</td><td>TERMINATED</td><td>10.32.35.160:3311719</td><td style=\"text-align: right;\">         256</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.670494 </td><td style=\"text-align: right;\">0.0898003 </td><td style=\"text-align: right;\">0.9169</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        161.788 </td></tr>\n",
              "<tr><td>train_mnist_16ec0_00009</td><td>TERMINATED</td><td>10.32.35.160:3312943</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">           256</td><td style=\"text-align: right;\">0.0643842</td><td style=\"text-align: right;\">0.00233183</td><td style=\"text-align: right;\">0.9845</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">        240.936 </td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "  </div>\n",
              "</div>\n",
              "<style>\n",
              ".tuneStatus {\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".tuneStatus .systemInfo {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              ".tuneStatus .trialStatus {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              ".tuneStatus h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".tuneStatus .hDivider {\n",
              "  border-bottom-width: var(--jp-border-width);\n",
              "  border-bottom-color: var(--jp-border-color0);\n",
              "  border-bottom-style: solid;\n",
              "}\n",
              ".tuneStatus .vDivider {\n",
              "  border-left-width: var(--jp-border-width);\n",
              "  border-left-color: var(--jp-border-color0);\n",
              "  border-left-style: solid;\n",
              "  margin: 0.5em 1em 0.5em 1em;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3303162)\u001b[0m 2023-12-05 02:01:20.480179: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3303162)\u001b[0m 2023-12-05 02:01:20.482465: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3303162)\u001b[0m 2023-12-05 02:01:20.529078: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3303162)\u001b[0m 2023-12-05 02:01:20.529611: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3303162)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3303162)\u001b[0m 2023-12-05 02:01:21.487489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3303162)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3303162)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3303162)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3303162)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3303162)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3303162)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3303162)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3303162)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3303162)\u001b[0m 2023-12-05 02:01:23.199869: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3303162)\u001b[0m Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div class=\"trialProgress\">\n",
              "  <h3>Trial Progress</h3>\n",
              "  <table>\n",
              "<thead>\n",
              "<tr><th>Trial name             </th><th style=\"text-align: right;\">  mean_accuracy</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>train_mnist_16ec0_00000</td><td style=\"text-align: right;\">         0.9626</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00001</td><td style=\"text-align: right;\">         0.956 </td></tr>\n",
              "<tr><td>train_mnist_16ec0_00002</td><td style=\"text-align: right;\">         0.9867</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00003</td><td style=\"text-align: right;\">         0.1009</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00004</td><td style=\"text-align: right;\">         0.9836</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00005</td><td style=\"text-align: right;\">         0.9869</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00006</td><td style=\"text-align: right;\">         0.9851</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00007</td><td style=\"text-align: right;\">         0.9577</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00008</td><td style=\"text-align: right;\">         0.9169</td></tr>\n",
              "<tr><td>train_mnist_16ec0_00009</td><td style=\"text-align: right;\">         0.9845</td></tr>\n",
              "</tbody>\n",
              "</table>\n",
              "</div>\n",
              "<style>\n",
              ".trialProgress {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  color: var(--jp-ui-font-color1);\n",
              "}\n",
              ".trialProgress h3 {\n",
              "  font-weight: bold;\n",
              "}\n",
              ".trialProgress td {\n",
              "  white-space: nowrap;\n",
              "}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(pid=3304567)\u001b[0m 2023-12-05 02:04:39.216653: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3304567)\u001b[0m 2023-12-05 02:04:39.264370: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3304567)\u001b[0m 2023-12-05 02:04:39.264876: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3304567)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3304567)\u001b[0m 2023-12-05 02:04:40.193589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3304567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3304567)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3304567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3304567)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3304567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3304567)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3304567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3304567)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3304567)\u001b[0m 2023-12-05 02:04:41.849226: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3304567)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3305567)\u001b[0m 2023-12-05 02:05:55.252042: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3305567)\u001b[0m 2023-12-05 02:05:55.300362: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3305567)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3305567)\u001b[0m 2023-12-05 02:05:55.299850: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3305567)\u001b[0m 2023-12-05 02:05:56.197994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3305567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3305567)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3305567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3305567)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3305567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3305567)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3305567)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3305567)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3305567)\u001b[0m 2023-12-05 02:05:57.831851: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3305567)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3306609)\u001b[0m 2023-12-05 02:07:41.228240: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3306609)\u001b[0m 2023-12-05 02:07:41.230489: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3306609)\u001b[0m 2023-12-05 02:07:41.276294: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3306609)\u001b[0m 2023-12-05 02:07:41.276803: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3306609)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3306609)\u001b[0m 2023-12-05 02:07:42.193552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3306609)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3306609)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3306609)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3306609)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3306609)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3306609)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3306609)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3306609)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3306609)\u001b[0m 2023-12-05 02:07:43.808451: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3306609)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3307587)\u001b[0m 2023-12-05 02:09:10.275855: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3307587)\u001b[0m 2023-12-05 02:09:10.278093: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3307587)\u001b[0m 2023-12-05 02:09:10.323724: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3307587)\u001b[0m 2023-12-05 02:09:10.324234: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3307587)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3307587)\u001b[0m 2023-12-05 02:09:11.204985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3307587)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3307587)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3307587)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3307587)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3307587)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3307587)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3307587)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3307587)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3307587)\u001b[0m 2023-12-05 02:09:12.800116: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3307587)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3308931)\u001b[0m 2023-12-05 02:12:27.456427: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3308931)\u001b[0m 2023-12-05 02:12:27.458679: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3308931)\u001b[0m 2023-12-05 02:12:27.504043: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3308931)\u001b[0m 2023-12-05 02:12:27.504528: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3308931)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3308931)\u001b[0m 2023-12-05 02:12:28.594486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3308931)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3308931)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3308931)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3308931)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3308931)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3308931)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3308931)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3308931)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3308931)\u001b[0m 2023-12-05 02:12:30.258564: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3308931)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3309817)\u001b[0m 2023-12-05 02:13:30.399272: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3309817)\u001b[0m 2023-12-05 02:13:30.401583: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3309817)\u001b[0m 2023-12-05 02:13:30.448195: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3309817)\u001b[0m 2023-12-05 02:13:30.448716: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3309817)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3309817)\u001b[0m 2023-12-05 02:13:31.390585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3309817)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3309817)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3309817)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3309817)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3309817)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3309817)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3309817)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3309817)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3309817)\u001b[0m 2023-12-05 02:13:33.035641: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3309817)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3310699)\u001b[0m 2023-12-05 02:14:25.377341: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3310699)\u001b[0m 2023-12-05 02:14:25.379579: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3310699)\u001b[0m 2023-12-05 02:14:25.424878: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3310699)\u001b[0m 2023-12-05 02:14:25.425370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3310699)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3310699)\u001b[0m 2023-12-05 02:14:26.301070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3310699)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3310699)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3310699)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3310699)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3310699)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3310699)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3310699)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3310699)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3310699)\u001b[0m 2023-12-05 02:14:27.872155: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3310699)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3311719)\u001b[0m 2023-12-05 02:15:54.353290: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3311719)\u001b[0m 2023-12-05 02:15:54.355533: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3311719)\u001b[0m 2023-12-05 02:15:54.401481: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "\u001b[36m(pid=3311719)\u001b[0m 2023-12-05 02:15:54.401987: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3311719)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3311719)\u001b[0m 2023-12-05 02:15:55.288556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3311719)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3311719)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3311719)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3311719)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3311719)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3311719)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3311719)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3311719)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3311719)\u001b[0m 2023-12-05 02:15:56.925340: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3311719)\u001b[0m Skipping registering GPU devices...\n",
            "\u001b[36m(pid=3312943)\u001b[0m 2023-12-05 02:18:40.455488: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "\u001b[36m(pid=3312943)\u001b[0m 2023-12-05 02:18:40.503178: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(pid=3312943)\u001b[0m 2023-12-05 02:18:40.503679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(pid=3312943)\u001b[0m To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(pid=3312943)\u001b[0m 2023-12-05 02:18:41.432307: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[36m(pid=3312943)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3312943)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3312943)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
            "\u001b[36m(pid=3312943)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(pid=3312943)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:518: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3312943)\u001b[0m   setattr(self, word, getattr(machar, word).flat[0])\n",
            "\u001b[36m(pid=3312943)\u001b[0m /home/pi2018/.local/lib/python3.8/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
            "\u001b[36m(pid=3312943)\u001b[0m   return self._float_to_str(self.smallest_subnormal)\n",
            "\u001b[36m(train_mnist pid=3312943)\u001b[0m 2023-12-05 02:18:43.007737: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "\u001b[36m(train_mnist pid=3312943)\u001b[0m Skipping registering GPU devices...\n",
            "2023-12-05 02:22:43,478\tINFO tune.py:1047 -- Total run time: 1284.82 seconds (1284.77 seconds for the tuning loop).\n"
          ]
        }
      ],
      "source": [
        "# Define the search space\n",
        "search_space = {\n",
        "    \"conv_filters\": tune.choice([64, 128, 256]),\n",
        "    \"lr\": tune.loguniform(0.001, 0.1),\n",
        "    \"batch_size\": tune.choice([64, 128, 256]),\n",
        "    \"dropout\": tune.uniform(0, 1)\n",
        "}\n",
        "\n",
        "# Define the Hyperband scheduler\n",
        "hyperband = HyperBandScheduler(\n",
        "    time_attr=\"training_iteration\",\n",
        "    max_t=100,  # Maximum training iterations\n",
        "    reduction_factor=3\n",
        ")\n",
        "\n",
        "# Run the optimization using Hyperband\n",
        "start_time = time.time()\n",
        "\n",
        "hyperband_analysis = tune.run(\n",
        "    train_mnist,\n",
        "    name=\"exp_hyperband\",\n",
        "    metric=\"mean_accuracy\",\n",
        "    mode=\"max\",\n",
        "    stop={\"training_iteration\": 12},  # Adjust based on your epochs\n",
        "    resources_per_trial={\"gpu\": 1},\n",
        "    config=search_space,\n",
        "    num_samples=10,  # Number of different hyperparameter configurations to try\n",
        "    scheduler=hyperband\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "hyperband_time = end_time - start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kineJbrfcg7"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Above we can see the execution for Grid Search, Bayesian Search, and Hyperband for the given hyperparameter configurations. For each we have defined differet train_mnist functions since each of the methods has slightly different approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG5ysXMqTAZu",
        "outputId": "2d501188-fafc-41ce-df79-371d21411765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for Grid Search:  5255.451504707336\n",
            "Best mean accuracy found is:  0.9904000163078308\n",
            "Best hyperparameters found were:  {'conv_filters': 256, 'lr': 0.001, 'batch_size': 64, 'dropout': 0.25}\n"
          ]
        }
      ],
      "source": [
        "print(\"Time taken for Grid Search: \", grid_time)\n",
        "\n",
        "best_result = grid_analysis.best_result\n",
        "print(\"Best mean accuracy found is: \", best_result[\"mean_accuracy\"])\n",
        "\n",
        "print(\"Best hyperparameters found were: \", grid_analysis.best_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEh4bVTxdyvw",
        "outputId": "5cde402f-b041-49ec-9982-043f12ad67de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for Bayesian Search:  617.1990833282471\n",
            "Best mean accuracy found is:  0.9837999939918518\n",
            "Best hyperparameters found were:  {'conv_filters': 105, 'lr': 0.01915704647548995, 'batch_size': 256, 'dropout': 0.18182496720710062}\n"
          ]
        }
      ],
      "source": [
        "# Define the mapping for the categorical values\n",
        "batch_size_map = {0: 64, 1: 128, 2: 256}\n",
        "\n",
        "# Retrieve the best configuration\n",
        "best_config = bayes_analysis.best_config\n",
        "\n",
        "# Apply the mapping to the continuous values\n",
        "best_config[\"conv_filters\"] = round(best_config[\"conv_filters\"])\n",
        "best_config[\"batch_size\"] = batch_size_map[int(round(best_config[\"batch_size\"]))]\n",
        "\n",
        "# Print the results\n",
        "print(\"Time taken for Bayesian Search: \", bayes_time)\n",
        "print(\"Best mean accuracy found is: \", bayes_analysis.best_result[\"mean_accuracy\"])\n",
        "print(\"Best hyperparameters found were: \", best_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_9i2aSDUURJ",
        "outputId": "6c2d524c-947e-4be3-963e-9753cb781473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time taken for Hyperband Search:  1284.8512477874756\n",
            "Best mean accuracy found is:  0.9868999719619751\n",
            "Best hyperparameters found were:  {'conv_filters': 64, 'lr': 0.0011908442812931575, 'batch_size': 128, 'dropout': 0.7741701209349096}\n"
          ]
        }
      ],
      "source": [
        "print(\"Time taken for Hyperband Search: \", hyperband_time)\n",
        "print(\"Best mean accuracy found is: \", hyperband_analysis.best_result[\"mean_accuracy\"])\n",
        "print(\"Best hyperparameters found were: \", hyperband_analysis.best_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMHZDOECjD3"
      },
      "source": [
        "## 3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRo9fm0tf-ga"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Above outputs show us results of our 3 hyperparameter optimization methods - Grid Search, Bayesian Search, and Hyperband - on our Lenet model using the MNIST data. Looking at the outputs we can notice that all the models performed very well (maybe even too well - note the overfitting) with accuracies between 0.984 and 0.990. However, they had very different perfomances in the term of time taken to run it and hyperparameters they found and selected as best.\n",
        "\n",
        "**Grid Search**\n",
        "- First method we used was Grid Search. This method \"tries out\" **all** possible hyperparameter combinations of specified configurations and as such runs for the longest period of time. We can see that Grid Search took 5255.45 seconds to run but it did yield the highest accuracy of 0.9904. While this is the best accuracy compared to other 2 methods we need to consider if the long run time (and with that high computational cost) is worth it especially if we have a large number of potential hyperparameters.\n",
        "\n",
        "**Bayesian Search**\n",
        "- Next method we used was Bayesian Search. This method uses probabilistic modeling for parameter selection and due to that it is more efficient than Grid Search as it focuses on regions with more promising hyperparameters based on prior evaluations. We can see that Bayesian Search was the fastest of 3 methods and took only 617.20 seconds to run but it also achieved the lowest accuracy of 0.9838. In general, Bayesian Search can end in not optimal hyperparameters compared to Grid Search as it does not try all combinations but it is a good option as it is very fast and usually provides good results. While it did the worst out of our 3 methods it still achieved 0.98 accuracy which is very high.\n",
        "\n",
        "**Hyperband Search**\n",
        "- The final method we used was Hyperband Search. This method is resource efficiant optimization as it can very quickly identify promising hyperparameters by adaptively allocating resources.  Looking at the above results we can see that this method is a good choice if we want to find a middle ground between Grid Search and Bayesian Search as it has higher accuracy than Bayesian Search at 0.9869 (but lower than Grid Search) with much lower run time than Grid Search as it ran for 1284.85 seconds(but 2x longer than Bayesian Search).\n",
        "\n",
        "\n",
        "Looking at all these we can see that while Grid Search provided the best accuracy it was the slowest method out of all. Further, while Bayesian Search ran fastest it had the lowest accuracy and Hyperband Search was somewhere in the middle on both accuracy and runtime front. In our case I would argue that we should use Bayesian Search becuase it ran fastest (and as such required the least computational resources) and it still achieved a very high accuracy of 0.98. However, in general we need to consider which is more important running quickly and using little computational resources or achiving the max accuracy (tradeoff between exploration and exploitation). If we are unsure, Hyperband Search is a good choice as it is somewhere between other two models."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
