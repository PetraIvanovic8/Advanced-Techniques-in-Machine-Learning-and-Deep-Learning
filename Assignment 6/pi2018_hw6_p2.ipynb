{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 2 - Automated Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVQIZUZeC5b"
      },
      "source": [
        "Sources:\n",
        "*   https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes\n",
        "*   https://github.com/cod3licious/autofeat\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbEBvSo3ClZT"
      },
      "source": [
        "**Answer:**\n",
        "When training ML models for real work problems output interpretability is crucial as it allowes users to understand how model is making decisions and to see if the logic is valid or if there is a mistake being made. More specifically, in a lot of fields like finance or medicine models output predictions cannot just be taken for what they are but stakeholders need to understand how they were made to be sure they were ethical and that the logic behind them is not incorrect / unacceptable. Further, having models with no iterpretability means that if asked about their decisions companies cannot explain why there were made which can also lead to lawsuits in cases where models turned out to be discriminatory towards certain population (which is often the case). In addition to this, models are often used as a tool to help humans make informed, data-driven decisions rather than just letting the machine make them itself. In this kind of situation, person making the decision wants to understand the decision making process not just see the result. Finally, interpretability is crucial to be able to understand when the model is making mistakes, what kind of a mistake or bias is causing it, and in the end how and what to fix to make the model better and help it generalize better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpPSDkYz9L8a"
      },
      "outputs": [],
      "source": [
        "!pip install autofeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RTjl1I0ailz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from autofeat import FeatureSelector, AutoFeatRegressor\n",
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAXg827kroEb"
      },
      "outputs": [],
      "source": [
        "# Load the diabetes dataset and get the featues and target\n",
        "X, y = datasets.load_diabetes(return_X_y=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKHeto26bBJm",
        "outputId": "a8e35a54-a5dc-4cf1-fdf7-d0abdf8069f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel] Scaling data...done.\n",
            "Original feature count: 10\n",
            "Selected feature count: 6\n",
            "Features discarded: 4\n"
          ]
        }
      ],
      "source": [
        "# Feature Selection\n",
        "fs = FeatureSelector(verbose=1)\n",
        "X_selected = fs.fit_transform(pd.DataFrame(X), pd.Series(y))\n",
        "\n",
        "# Check how many features were discarded\n",
        "print(\"Original feature count:\", X.shape[1])\n",
        "print(\"Selected feature count:\", X_selected.shape[1])\n",
        "discarded_features = X.shape[1] - X_selected.shape[1] # to see which features were removed\n",
        "print(\"Features discarded:\", discarded_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SDulg8cCnRA"
      },
      "source": [
        "**Answer:**\n",
        "AutoFeat's FeatureSelector discards features in hte dataset that it deems are not good predictors of our outcome variable. In our case, out of our 10 features it only chose to keep 6 and discard 4 that were not as valuable for preduction of our outcome y.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eXCL_NyGy2_"
      },
      "source": [
        "## 2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrQQMpW4Gy3I",
        "outputId": "3f7fbf81-da77-4e20-eea1-260ccd1d95d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 score on the training set: 0.5279193863361498\n",
            "R2 score on the test set: 0.45260276297191937\n"
          ]
        }
      ],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "r2_train = model.score(X_train, y_train)\n",
        "r2_test = model.score(X_test, y_test)\n",
        "print(\"R2 score on the training set:\", r2_train)\n",
        "print(\"R2 score on the test set:\", r2_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjo-oTDxCole"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "$R^2$ tells us how much of variablity in our outcome variable (y) is explained by the dependent variables (x).\n",
        "\n",
        "From the above output of $R^2$ values for our training and testing set we can immediatelly notice that training one is quite a bit higher than the test one. $R^2$ score on the training set is about 0.53 and $R^2$ score on the test set is about 0.45. While we would always expect (except in a perfect theoretical case of 0 overfitting) the score for training to be slighly higer but in our case it could indicate that there is overfitting. This means our model is too complex and does not generlize well on new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BuGX6EVG4uI"
      },
      "source": [
        "## 2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "ThCq_RhQG4uJ",
        "outputId": "882ece61-7484-4d65-b320-bf20618700f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
            "  x = um.multiply(x, x, out=x)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[featsel] Scaling data...done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Feature engineering with AutoFeatRegressor\n",
        "afreg = AutoFeatRegressor(verbose=1, feateng_steps=3)\n",
        "X_train_feat = afreg.fit_transform(X_train, y_train)\n",
        "X_test_feat = afreg.transform(X_test)\n",
        "\n",
        "# Fit the model again\n",
        "model.fit(X_train_feat, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2rMIMjf9zdK",
        "outputId": "60930c1a-b94a-4e0c-8caf-941b3cbecb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 score on the training set with AutoFeat: 0.6325539093905881\n",
            "R2 score on the test set with AutoFeat: 0.5191672981051741\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "r2_train_feat = model.score(X_train_feat, y_train)\n",
        "r2_test_feat = model.score(X_test_feat, y_test)\n",
        "print(\"R2 score on the training set with AutoFeat:\", r2_train_feat)\n",
        "print(\"R2 score on the test set with AutoFeat:\", r2_test_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBWftdH992Ba",
        "outputId": "f0678196-69d6-4dd7-a454-085697530432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Five new features generated: ['Abs(x008)/x008', 'exp(x006)*Abs(x001)', 'exp(x002)*exp(x008)', 'exp(x002)*exp(x003)', '1/(x002**3 + x005**3)']\n"
          ]
        }
      ],
      "source": [
        "# Convert X_train and X_train_feat to Pandas DataFrames\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "X_train_feat_df = pd.DataFrame(X_train_feat)\n",
        "\n",
        "# Print new features\n",
        "new_features = list(X_train_feat_df.columns[10:])\n",
        "print(\"Five new features generated:\", new_features[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyQOE5AhHBxX"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "From the above output of AutoFeatRegressor we can see that both of our $R^2$ measures have incresed. This indicates that feature engineering has let to a better model fit. Our $R^2$ on training data is now 0.63 which is pretty high in comparison to our previous one, and our $R^2$ on test data increased to 0.51 from 0.45. While this does indicate out model perfomrns better, the issue of overfitting has not been solved but increased. We can see that the gap between our training and testing results has increased. This is likely due to the fact that the model became more complex by adding new features and thus is not generalizing well to new, unseen data.\n",
        "\n",
        "Above we can see example of 5 new generated features. We can see that all of these features are transformations of 1 or more of the original features. They allowed us to understand nuances of our data better but can also cause overfitting by adapting to noise in the data as well as the general trends.\n",
        "\n",
        "Here are 5 newly generated features:\n",
        "$\\frac{Abs(x008))}{x008}, exp(x006)*Abs(x001), exp(x002)*exp(x008), exp(x002)*exp(x003), \\frac{1}{(x002^3 + x005^3)}$\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
