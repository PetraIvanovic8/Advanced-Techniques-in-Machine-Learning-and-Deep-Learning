{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcDhlfqyBd6m"
      },
      "source": [
        "# Problem 2 - Neural Network Training and Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvTA8iUgYMCh"
      },
      "source": [
        "Lets import and inspect our data as shown in the example code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_DplLtSpoam",
        "outputId": "9353094e-94d4-4041-bfdd-560ee2acb381"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
              " '__version__': '1.0',\n",
              " '__globals__': [],\n",
              " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " 'y': array([[10],\n",
              "        [10],\n",
              "        [10],\n",
              "        ...,\n",
              "        [ 9],\n",
              "        [ 9],\n",
              "        [ 9]], dtype=uint8)}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "%matplotlib inline\n",
        "\n",
        "data = loadmat('ex3data1.mat')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX5pJ2UuqDPe",
        "outputId": "d325075e-dc76-4eb8-c889-cf539f18b18c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((5000, 400), (5000, 1))"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = data['X']\n",
        "y = data['y']\n",
        "\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIRwNYIDqD_4",
        "outputId": "ef9fff44-d617-472d-ccc3-38e1e2028ed7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(5000, 10)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_onehot = encoder.fit_transform(y)\n",
        "y_onehot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWtAeGY2qFHh",
        "outputId": "73e4324d-c392-4fa0-edfb-8b36e0cf7e6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([10], dtype=uint8), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0], y_onehot[0,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-id00ye6CNLB"
      },
      "source": [
        "## 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vTNdohMM9x99"
      },
      "outputs": [],
      "source": [
        "# sigmoid()\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-2*z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T53tv-GoYUbr"
      },
      "source": [
        "Our sigmoid function is defined with -2z instead of z as we are using the scaled sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1D_yfneCWqL"
      },
      "source": [
        "## 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "TaITPmRaf-BK"
      },
      "outputs": [],
      "source": [
        "# forward_propagate()\n",
        "def forward_propagate(X, theta1, theta2, theta3):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    #input layer\n",
        "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
        "    z2 = a1 * theta1.T\n",
        "\n",
        "    #1st hidden layer\n",
        "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
        "    z3 = a2 * theta2.T\n",
        "\n",
        "    #2nd hidden layer\n",
        "    a3 = np.insert(sigmoid(z3), 0, values=np.ones(m), axis=1)\n",
        "    z4 = a3 * theta3.T\n",
        "\n",
        "    h = sigmoid(z4)\n",
        "\n",
        "    return a1, z2, a2, z3, a3, z4, h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIXwECi2YyZE"
      },
      "source": [
        "In the forward_propagate function we added a3 and z4 terms to account for out 2nd hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7R7c6Mv91wP"
      },
      "source": [
        "## 2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "FGpDkL9--AIN"
      },
      "outputs": [],
      "source": [
        "# cost() without regularization\n",
        "def cost_without_regularization(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    end_theta1 = hidden_size * (input_size + 1)\n",
        "    end_theta2 = end_theta1 + hidden_size * (hidden_size + 1)\n",
        "\n",
        "    theta1 = np.matrix(np.reshape(params[:end_theta1], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[end_theta1:end_theta2], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[end_theta2:], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "\n",
        "    return J\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX_hVrlsaJo3"
      },
      "source": [
        "In the cost_without_regularization function we added theta 3 to account for our 2nd hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QVZhqzIaUXx"
      },
      "source": [
        "Lets create sample parameters to test our functions as we define them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzkhpGRw1A5X",
        "outputId": "e72f426f-16b8-4a1f-df12-6273caeb6230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20, 401) (20, 21) (10, 21)\n"
          ]
        }
      ],
      "source": [
        "# initial setup\n",
        "input_size = 400\n",
        "hidden_size = 20\n",
        "num_labels = 10\n",
        "learning_rate = 1\n",
        "\n",
        "# randomly initialize a parameter array of the size of the full network's parameters\n",
        "params = (np.random.random(size=hidden_size * (input_size + 1) +\n",
        "                           hidden_size * (hidden_size + 1) +\n",
        "                           num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
        "\n",
        "m = X.shape[0]\n",
        "X = np.matrix(X)\n",
        "y = np.matrix(y)\n",
        "\n",
        "# unravel the parameter array into parameter matrices for each layer\n",
        "end_theta1 = hidden_size * (input_size + 1)\n",
        "end_theta2 = end_theta1 + hidden_size * (hidden_size + 1)\n",
        "\n",
        "theta1 = np.matrix(np.reshape(params[:end_theta1], (hidden_size, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(params[end_theta1:end_theta2], (hidden_size, (hidden_size + 1))))\n",
        "theta3 = np.matrix(np.reshape(params[end_theta2:], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "print(theta1.shape, theta2.shape, theta3.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRwsv7kZ2wPy",
        "outputId": "4a366afb-ba9c-4119-e556-31de4fb05e19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((5000, 401),\n",
              " (5000, 20),\n",
              " (5000, 21),\n",
              " (5000, 20),\n",
              " (5000, 21),\n",
              " (5000, 10),\n",
              " (5000, 10))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "a1.shape, z2.shape, a2.shape, z3.shape, a3.shape, z4.shape, h.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SEAL1Jw3DMp",
        "outputId": "e9c4b1fa-be63-40e8-c49a-bba314ed783d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.100031060813976"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cost_without_regularization(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "1qvvtJX391wQ"
      },
      "outputs": [],
      "source": [
        "# cost() with regularization\n",
        "def cost_with_regularization(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "  # reshape the parameter array into parameter matrices for each layer\n",
        "    end_theta1 = hidden_size * (input_size + 1)\n",
        "    end_theta2 = end_theta1 + hidden_size * (hidden_size + 1)\n",
        "\n",
        "    theta1 = np.matrix(np.reshape(params[:end_theta1], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[end_theta1:end_theta2], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[end_theta2:], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # compute the cost\n",
        "    J = 0\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "\n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta2[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta3[:,1:], 2)))\n",
        "\n",
        "    return J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEeU_yjzbcbH"
      },
      "source": [
        "In the cost_with_regularization function in addition to theta 3 variable we also altered our cost regulariztion term J to account for the theta 3 (our second hidden layer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc9eUVU-4Byx",
        "outputId": "abeeaf4e-2f84-4f06-c89f-42924e523827"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.104578168789641"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cost_with_regularization(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA9buRFEbq3R"
      },
      "source": [
        "We can see that cost increases with regularization, but this increase is very small (0.004)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrRm_0lM914k"
      },
      "source": [
        "## 2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "WGKHQ89q914l"
      },
      "outputs": [],
      "source": [
        "# sigmoid_gradient()\n",
        "def sigmoid_gradient(z):\n",
        "    return 2 * np.multiply(sigmoid(z), (1 - sigmoid(z)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLsQOL5Wb270"
      },
      "source": [
        "In sigmoid_gradient funciton we multipied our sigmoid gradient with 2 wo account for our scaled sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFAnt75y92BK"
      },
      "source": [
        "## 2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kNhNbo8Y-KLd"
      },
      "outputs": [],
      "source": [
        "# backprop() without regularization\n",
        "def backprop_without_regularization(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    end_theta1 = hidden_size * (input_size + 1)\n",
        "    end_theta2 = end_theta1 + hidden_size * (hidden_size + 1)\n",
        "\n",
        "    theta1 = np.matrix(np.reshape(params[:end_theta1], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[end_theta1:end_theta2], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[end_theta2:], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(theta1.shape)\n",
        "    delta2 = np.zeros(theta2.shape)\n",
        "    delta3 = np.zeros(theta3.shape)\n",
        "\n",
        "    # compute the cost\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "\n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta2[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta3[:,1:], 2)))\n",
        "    # perform backpropagation\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]\n",
        "        z2t = z2[t,:]\n",
        "        a2t = a2[t,:]\n",
        "        z3t = z3[t,:]\n",
        "        a3t = a3[t,:]\n",
        "\n",
        "        ht = h[t,:]\n",
        "        yt = y[t,:]\n",
        "\n",
        "        d4t = ht - yt\n",
        "\n",
        "        z3t = np.insert(z3t, 0, values=np.ones(1))\n",
        "        d3t = np.multiply((theta3.T * d4t.T).T, sigmoid_gradient(z3t))\n",
        "\n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))\n",
        "        d2t = np.multiply((theta2.T * d3t[:,1:].T).T, sigmoid_gradient(z2t))\n",
        "\n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + (d3t[:,1:]).T * a2t\n",
        "        delta3 = delta3 + d4t.T * a3t\n",
        "\n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    delta3 = delta3 / m\n",
        "\n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2), np.ravel(delta3)))\n",
        "\n",
        "    return J, grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHSM-2KfcIBd"
      },
      "source": [
        "In the backprop_without_regularization fuction we needed to alter the backpropagation step by adding z3t and a3t terms as well as d4t and delta 3 to account for our additional 2nd hidden layer. We also added our 2nd hidden layer (delta 3) to the \"grad\" array with unraveled gradient matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfIIP8fn4K77",
        "outputId": "8f0de855-75db-4de6-852e-583aa535a551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7.104578168789641, (8650,))"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "J, grad = backprop_without_regularization(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_LOlywen92BL"
      },
      "outputs": [],
      "source": [
        "# backprop() with regularization\n",
        "def backprop_with_regularization(params, input_size, hidden_size, num_labels, X, y, learning_rate):\n",
        "    m = X.shape[0]\n",
        "    X = np.matrix(X)\n",
        "    y = np.matrix(y)\n",
        "\n",
        "    # reshape the parameter array into parameter matrices for each layer\n",
        "    end_theta1 = hidden_size * (input_size + 1)\n",
        "    end_theta2 = end_theta1 + hidden_size * (hidden_size + 1)\n",
        "\n",
        "    theta1 = np.matrix(np.reshape(params[:end_theta1], (hidden_size, (input_size + 1))))\n",
        "    theta2 = np.matrix(np.reshape(params[end_theta1:end_theta2], (hidden_size, (hidden_size + 1))))\n",
        "    theta3 = np.matrix(np.reshape(params[end_theta2:], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "    # run the feed-forward pass\n",
        "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
        "\n",
        "    # initializations\n",
        "    J = 0\n",
        "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
        "    delta2 = np.zeros(theta2.shape)  # (10, 26)\n",
        "    delta3 = np.zeros(theta3.shape)  # (10, 27)\n",
        "\n",
        "    # compute the cost\n",
        "    for i in range(m):\n",
        "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
        "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
        "        J += np.sum(first_term - second_term)\n",
        "\n",
        "    J = J / m\n",
        "\n",
        "    # add the cost regularization term\n",
        "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta2[:,1:], 2)) +\n",
        "                                             np.sum(np.power(theta3[:,1:], 2)))\n",
        "    # perform backpropagation\n",
        "    for t in range(m):\n",
        "        a1t = a1[t,:]\n",
        "        z2t = z2[t,:]\n",
        "        a2t = a2[t,:]\n",
        "        z3t = z3[t,:]\n",
        "        a3t = a3[t,:]\n",
        "\n",
        "        ht = h[t,:]\n",
        "        yt = y[t,:]\n",
        "\n",
        "        d4t = ht - yt\n",
        "\n",
        "        z3t = np.insert(z3t, 0, values=np.ones(1))\n",
        "        d3t = np.multiply((theta3.T * d4t.T).T, sigmoid_gradient(z3t))\n",
        "\n",
        "        z2t = np.insert(z2t, 0, values=np.ones(1))\n",
        "        d2t = np.multiply((theta2.T * d3t[:,1:].T).T, sigmoid_gradient(z2t))\n",
        "\n",
        "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
        "        delta2 = delta2 + (d3t[:,1:]).T * a2t\n",
        "        delta3 = delta3 + d4t.T * a3t\n",
        "\n",
        "    delta1 = delta1 / m\n",
        "    delta2 = delta2 / m\n",
        "    delta3 = delta3 / m\n",
        "\n",
        "    # add the gradient regularization term\n",
        "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * learning_rate) / m\n",
        "    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * learning_rate) / m\n",
        "    delta3[:,1:] = delta3[:,1:] + (theta3[:,1:] * learning_rate) / m\n",
        "\n",
        "    # unravel the gradient matrices into a single array\n",
        "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2), np.ravel(delta3)))\n",
        "\n",
        "    return J, grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J9IIhyNczl4"
      },
      "source": [
        "In the backprop_with_regularization in addition to the delta 1 and delta 2 regularization terms we also added the delta 3 regularization term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfX7RjhB5YuJ",
        "outputId": "f4c0f06a-ceec-40aa-a093-5e7b716f2117"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7.104578168789641, (8650,))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "J, grad = backprop_with_regularization(params, input_size, hidden_size, num_labels, X, y_onehot, learning_rate)\n",
        "J, grad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpmTkUOL92Hm"
      },
      "source": [
        "## 2.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3HGEqa6c_16"
      },
      "source": [
        "Lets train our 3-layered neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKP1F-jP92Hn",
        "outputId": "ada79b16-5bd7-4053-8c63-c1ea51fbdd06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-53-4310fc965557>:5: OptimizeWarning: Unknown solver options: maxiter\n",
            "  fmin = minimize(fun=backprop_with_regularization, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, learning_rate),\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              " message: Converged (|f_n-f_(n-1)| ~= 0)\n",
              " success: True\n",
              "  status: 1\n",
              "     fun: 0.09706570345205856\n",
              "       x: [ 1.068e+00 -2.191e-07 ...  1.112e+00 -5.328e-01]\n",
              "     nit: 61\n",
              "     jac: [ 1.284e-05 -4.383e-11 ... -1.189e-05 -5.827e-06]\n",
              "    nfev: 1163"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "# minimize the objective function\n",
        "fmin = minimize(fun=backprop_with_regularization, x0=params, args=(input_size, hidden_size, num_labels, X, y_onehot, learning_rate),\n",
        "                method='TNC', jac=True, options={'maxiter': 250})\n",
        "fmin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "5f0gWorC5fzb"
      },
      "outputs": [],
      "source": [
        "X = np.matrix(X)\n",
        "# reshape the parameter array into parameter matrices for each layer\n",
        "end_theta1 = hidden_size * (input_size + 1)\n",
        "end_theta2 = end_theta1 + hidden_size * (hidden_size + 1)\n",
        "\n",
        "theta1 = np.matrix(np.reshape(fmin.x[:end_theta1], (hidden_size, (input_size + 1))))\n",
        "theta2 = np.matrix(np.reshape(fmin.x[end_theta1:end_theta2], (hidden_size, (hidden_size + 1))))\n",
        "theta3 = np.matrix(np.reshape(fmin.x[end_theta2:], (num_labels, (hidden_size + 1))))\n",
        "\n",
        "a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLPj7Jou92OD"
      },
      "source": [
        "## 2.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNdm1aSedl_v"
      },
      "source": [
        "Lets make foward predictions with our model and compute accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59v8hLZT92OD",
        "outputId": "c25f91e8-f545-4cc9-a425-4c768c148904"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[10],\n",
              "       [10],\n",
              "       [10],\n",
              "       ...,\n",
              "       [ 9],\n",
              "       [ 9],\n",
              "       [ 9]])"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5-t9yUE5iIP",
        "outputId": "d468d420-fc38-4f5f-97c9-952c5b76d769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 100.0%\n"
          ]
        }
      ],
      "source": [
        "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
        "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
        "print('accuracy = {0}%'.format(accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdXYjW4l92VA"
      },
      "source": [
        "## 2.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45i7WFxc-Wyu"
      },
      "source": [
        "**Answer:**\n",
        "Looking at our model we can notice that it achieved accuracy of 100% this can mean two things - a) we built an exceptioinal model that captures more informaiton about our data with the 2nd hidden layer or b) our model is overfitting and thus will not generalize well on the new data. Comparing it to the original model with 1 hidden layer (instead of 2) we can notice that the amout the accuracy approved is minimal - from 99.22% to 100%. Solely looking at this we should consider if adding the additional layer is worth it as it leads to very little improvment (since the original model is doing very well) but it does take longer to train (requires more computational resources which can get very expensive very quickly) and it also potentially leads to overfitting. Due to this I do not think that adding the addiotional hidden layer is valuable as it makes the model more complex which could (and maybe did) lead to overfitting and is more computationally expensive which is not worth it as the original model preformed very well."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
