### Assignment 3
This assignment delved into advanced neural network concepts, including softmax activation, backpropagation enhancements, weight initialization effects, dead neurons, and regularization techniques using dropout and batch normalization. Techniques I used involved mathematical derivations for softmax gradients, modifications to neural network architectures for improved learning, experimentation with weight initialization to mitigate vanishing and exploding gradients, and the practical application of Leaky ReLU to prevent dead neurons. The assignment also compared regularization methods on the MNIST dataset, emphasizing hands-on implementation and theoretical understanding of deep learning principles.